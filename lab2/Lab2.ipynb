{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44bc1b80",
   "metadata": {},
   "source": [
    "***Deep Learning Applications 2023** course, held by Professor **Andrew David Bagdanov** - University of Florence, Italy*\n",
    "\n",
    "*Notebook and code created by **Giovanni Colombo** - Mat. 7092745*\n",
    "\n",
    "Check the dedicated [Repository on GitHub](https://github.com/giovancombo/DLA-Labs/tree/main/lab2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #2 - LLMs\n",
    "\n",
    "In this laboratory we will get our hands dirty working with Large Language Models (e.g. GPT and BERT) to do various useful things. I you haven't already, it is highly recommended to:\n",
    "\n",
    "+ Read the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper, which is the basis for all transformer-based LLMs.\n",
    "+ Watch (and potentially *code along*) with this [Andrej Karpathy video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which shows you how to build an autoregressive GPT model from the ground up.\n",
    "\n",
    "## Exercise 1: Warming Up\n",
    "In this first exercise you will train a *small* autoregressive GPT model for character generation (the one used by Karpathy in his video) to generate text in the style of Dante Aligheri. Use [this file](https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt), which contains the entire text of Dante's Inferno (**note**: you will have to delete some introductory text at the top of the file before training). Train the model for a few epochs, monitor the loss, and generate some text at the end of training. Qualitatively evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "bd466d3b-cc41-4de3-9f82-3547569909f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0 🦅\n"
     ]
    }
   ],
   "source": [
    "# Imports and dependencies\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from pipeline import *\n",
    "\n",
    "# Hyperparameters\n",
    "text = 'taylor_swift'\n",
    "\n",
    "train_size = 0.7\n",
    "batch_size = 64             # Batch size = number of independent sequences of text, analyzed in parallel\n",
    "learning_rate = 5e-4\n",
    "block_size = 512            # Dimension of an input seuqence of characters, for next character prediction\n",
    "n_embd = 100                # Embedding dimension for each token\n",
    "n_heads = 4                 # Number of Self-Attention heads in a Multi-Head Attention block\n",
    "n_layers = 4                # Number of Blocks of the Transformer\n",
    "dropout = 0.4\n",
    "eval_iters = 200\n",
    "total_steps = 10\n",
    "log_interval = 100\n",
    "\n",
    "# Creating a configuration dictionary for logging in wandb\n",
    "config = dict(\n",
    "    text = text,\n",
    "    batch_size = batch_size,\n",
    "    block_size = block_size,\n",
    "    n_embd = n_embd,\n",
    "    n_heads = n_heads,\n",
    "    n_layers = n_layers,\n",
    "    learning_rate = learning_rate,\n",
    "    dropout = dropout,\n",
    "    eval_iters = eval_iters,\n",
    "    total_steps = total_steps,\n",
    "    log_interval = log_interval,\n",
    "    train_size = train_size,\n",
    ")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58363a37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Downloading the Dante's Divina Commedia txt file from the internet\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#!wget https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Opening and reading the content of the input text file\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of dataset in characters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(text))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# Downloading the Dante's Divina Commedia txt file from the internet\n",
    "#!wget https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt\n",
    "\n",
    "# Opening and reading the content of the input text file\n",
    "with open(text + '.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"Length of dataset in characters:\", len(text))\n",
    "\n",
    "# Creating a sorted set of all the unique characters present in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)             # Number of unique characters in the text = size of the vocabulary\n",
    "\n",
    "\n",
    "# Creating a dictionary for mapping characters to integers and viceversa\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[i] for i in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# text = list of characters\n",
    "# data = list of integers of all the text --> it's our dataset\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "\n",
    "# Splitting our dataset in train and validation sets\n",
    "n = int(train_size*len(data))\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "01014666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☀️ Initializing Weights & Biases...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\giova\\__UNI\\Deep Learning Applications\\LABORATORIO\\Modulo2_LLMs\\wandb\\run-20240108_231328-neqad6c2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/giovancombo/DLA_Lab2_LLM/runs/neqad6c2' target=\"_blank\">major-sun-62</a></strong> to <a href='https://wandb.ai/giovancombo/DLA_Lab2_LLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/giovancombo/DLA_Lab2_LLM' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab2_LLM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/giovancombo/DLA_Lab2_LLM/runs/neqad6c2' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab2_LLM/runs/neqad6c2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 Model TransformerDecoder instantiated!\n",
      "🫘 Number of parameters: 553289\n",
      "🔥 Optimizer: \n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0005\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "Device: cuda:0 🦅\n",
      "\n",
      "Starting Training...💪🏼\n",
      "📜 Step 1/10:\t🫥 Train Loss = 4.4583; Val Loss = 4.4586\t🎯 Train Accuracy = 2.87%; Val Accuracy = 2.83%\n",
      "📜 Step 10/10:\t🫥 Train Loss = 3.4851; Val Loss = 3.5022\t🎯 Train Accuracy = 17.09%; Val Accuracy = 16.59%\n",
      "\n",
      "Training completed!👌🏼\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁█</td></tr><tr><td>Train Accuracy</td><td>▁█</td></tr><tr><td>Train Loss</td><td>█▁</td></tr><tr><td>Validation Accuracy</td><td>▁█</td></tr><tr><td>Validation Loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>17.08778</td></tr><tr><td>Train Loss</td><td>3.48508</td></tr><tr><td>Validation Accuracy</td><td>16.58801</td></tr><tr><td>Validation Loss</td><td>3.50219</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">major-sun-62</strong> at: <a href='https://wandb.ai/giovancombo/DLA_Lab2_LLM/runs/neqad6c2' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab2_LLM/runs/neqad6c2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240108_231328-neqad6c2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generation configuration\n",
    "generation = False\n",
    "new_tokens = 500           # Number of tokens generated\n",
    "\n",
    "# Saving configuration\n",
    "save_model = False\n",
    "folder = f\"1_transformers/{text}\"\n",
    "model_name = \"model_\" + text + \"_bs\" + str(batch_size) + \"_bl\" + str(block_size) + \"_ne\" + str(n_embd) + \"_nh\" + str(n_heads) + \"_nl\" + str(n_layers) + \"_lr\" + str(learning_rate)\n",
    "\n",
    "wandb.login()\n",
    "print(\"Initializing Weights & Biases run...\")\n",
    "\n",
    "with wandb.init(project = \"DLA_Lab2_LLM\", config = config):\n",
    "    config = wandb.config\n",
    "    \n",
    "    # Building model and optimizer\n",
    "    model, criterion, optimizer = build_model(vocab_size, block_size, n_embd, n_heads, n_layers, learning_rate, device)\n",
    "\n",
    "    # Training the model\n",
    "    train(model, criterion, optimizer)\n",
    "    wandb.unwatch(model)\n",
    "\n",
    "    # Generating new text from the model trained (optional)\n",
    "    if generation:\n",
    "        text_generator(model, new_tokens)\n",
    "\n",
    "    # Saving the model (optional)\n",
    "    if save_model:\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        torch.save(model, f\"{folder}/{model_name}.pt\")\n",
    "        print('\\nModel saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68441a09-dfaf-424a-b640-4fc8cea289b5",
   "metadata": {},
   "source": [
    "## Exercise 2: Working with Real LLMs\n",
    "\n",
    "Our toy GPT can only take us so far. In this exercise we will see how to use the [Hugging Face](https://huggingface.co/) model and dataset ecosystem to access a *huge* variety of pre-trained transformer models.\n",
    "\n",
    "### Exercise 2.1: Installation and text tokenization\n",
    "\n",
    "First things first, we need to install the [Hugging Face transformer library](https://huggingface.co/docs/transformers/index):\n",
    "\n",
    "    conda install -c huggingface -c conda-forge transformers\n",
    "    \n",
    "The key classes that you will work with are `GPT2Tokenizer` to encode text into sub-word tokens, and the `GPT2LMHeadModel`. **Note** the `LMHead` part of the class name -- this is the version of the GPT2 architecture that has the text prediction heads attached to the final hidden layer representations (i.e. what we need to **generate** text). \n",
    "\n",
    "Instantiate the `GPT2Tokenizer` and experiment with encoding text into integer tokens. Compare the length of input with the encoded sequence length.\n",
    "\n",
    "**Tip**: Pass the `return_tensors='pt'` argument to the tokenizer to get Pytorch tensors as output (instead of lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "af199a6d-1f3a-4b2c-a23f-d697b93c5adb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu 🐌\n"
     ]
    }
   ],
   "source": [
    "# Imports and dependencies\n",
    "import os\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e6e4bfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔡 Input text:\t\tHello World\n",
      "📑 Tokenized text:\ttensor([[15496,  2159]])\n"
     ]
    }
   ],
   "source": [
    "# Custom input text to be tokenized\n",
    "input_text = \"Hello World\"\n",
    "\n",
    "# Creating a subword tokenizer from GPT2 pretrained model: new vocab_size = 50,257!\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Converting string inputs to sequences of tokens: let's compare the input length with the encoded sequence length\n",
    "# It's curious to see the difference between calling tokenizer itself, or its encode/tokenize attributes\n",
    "tokenized_text = tokenizer.encode(input_text, return_tensors = 'pt')\n",
    "\n",
    "print(f\"Input string:\\t\\t{input_text}\\nTokenized string:\\t{tokenized_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbfad2c",
   "metadata": {},
   "source": [
    "This GPT2 tokenizer works on a subword level, so what I could notice is that, tipically, inputs are divided into several 2/3/4 words chunks and encoded to a particular integer.\n",
    "As the input string sequences increases their length, the encoded sequence length increases.\n",
    "Input sequences of 2/3/4 characters can be encoded to a single integer.\n",
    "\n",
    "While inputing the sequence *\"Hello World\"*, I could notice that the tokenizer has a single integer for the whole \"Hello\" and \"World\" words, suggesting that many existing english (and not only, maybe) words are encoded to a single integer.\n",
    "\n",
    "Passing the `return_tensors = 'pt'` argument makes the tokenizer output PyTorch tensors instead of lists.\n",
    "\n",
    "From the original [Hugging Face Documentation](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Tokenizer), we can read that:\n",
    "> This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will be encoded differently whether it is at the beginning of the sentence (without space) or not.\n",
    "\n",
    "Trying to see the behaviour of slightly different versions of the sequence *\"Hello World\"*, it's possible to notice that:\n",
    "- *\"Hello World\"* encodes to tensor([[15496,  2159]])\n",
    "- *\"hello World\"* encodes to tensor([[31373,  2159]]) --> Case matters\n",
    "- *\" hello World\"* encodes to tensor([[23748,  2159]]) --> Space matters!\n",
    "- *\"HelloWorld\"* encodes to tensor([[15496, 10603]])\n",
    "- *\"Hello World \"* encodes to tensor([[15496,  2159,   220]]) --> But space character has its own encoding integer when nothing follows it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458b725-63c1-49ae-8011-71a9196387b8",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Generating Text\n",
    "\n",
    "There are a lot of ways we can, given a *prompt* in input, sample text from a GPT2 model. Instantiate a pre-trained `GPT2LMHeadModel` and use the [`generate()`](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate text from a prompt.\n",
    "\n",
    "**Note**: The default inference mode for GPT2 is *greedy* which might not result in satisfying generated text. Look at the `do_sample` and `temperature` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bdad9208-cc9e-4750-baa5-f9367e71362a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel instantiated!👌🏼\n",
      "\n",
      "🔡 Input text:\tThe main goal in life is\n",
      "✨ Waiting for the generation of new text...\n",
      "\n",
      "Text generated!🪄\n",
      "\u001b[3mThe main goal in life is to be as creative as possible in your career – so to see this as fulfilling when you can do so without procrastinating, isn't that amazing?\n",
      "\n",
      "Yes, it is! It is all about taking your imagination but it's all important, and it gives you a boost without necessarily thinking about it in the slightest! You'd never know, but seeing someone take advantage of the creativity of their subconscious without feeling ashamed about doing so is something really awesome to experience. You really give it away\u001b[0m\n",
      "\n",
      "Text generated saved in 'text_generation/generation_log.txt' 💾\n"
     ]
    }
   ],
   "source": [
    "# Saving configuration\n",
    "save_generation = False\n",
    "folder = '2_textgeneration'\n",
    "\n",
    "# We can even ask the user to input a text prompt\n",
    "input_text = \"The main goal in life is\"         # input(\"What do you want to say?\\n\")\n",
    "\n",
    "# Hyperparameters for text generation\n",
    "max_new_tokens = 100\n",
    "do_sample = True\n",
    "temperature = 1.2\n",
    "early_stopping = True\n",
    "no_repeat_ngram_size = 2\n",
    "\n",
    "# Loading the pretrained model: setting the padding token as the end of sequence token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id = tokenizer.eos_token_id)\n",
    "\n",
    "print(f\"{model.__class__.__name__} instantiated!\\n\")\n",
    "print(f\"Input text: {input_text}\\nWaiting for the generation of new text...\")\n",
    "\n",
    "# inputs is a Pytorch tensor\n",
    "tokenized_text = tokenizer(input_text, return_tensors = 'pt')\n",
    "\n",
    "# Let's generate some text from the input sequence: new_text is a Pytorch tensor\n",
    "generated_text = tokenizer.decode(model.generate(tokenized_text['input_ids'],\n",
    "                                                max_new_tokens = max_new_tokens,\n",
    "                                                do_sample = do_sample,\n",
    "                                                temperature = temperature,\n",
    "                                                early_stopping = early_stopping,\n",
    "                                                no_repeat_ngram_size = no_repeat_ngram_size)[0].tolist())\n",
    "print(\"\\nText generated!\")\n",
    "print(f\"{generated_text}\")\n",
    "\n",
    "\n",
    "# Saving the generated text in a txt file\n",
    "if save_generation:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    with open(f\"{folder}/generation_log.txt\", 'a') as f:\n",
    "        f.write(f\"HPs: max_new_tokens = {max_new_tokens}, do_sample = {do_sample}, temperature = {temperature}, early_stopping = {early_stopping}, no_repeat_ngram_size = {no_repeat_ngram_size}\\n\\n\")\n",
    "        f.write(f\"Input text: {input_text}\\nGenerated text: {generated_text}\\n\\n- - - - - - - - - - - - - - - -\\n\")\n",
    "\n",
    "    print(f'\\nText generated saved in \\'{folder}/generation_log.txt\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c1265",
   "metadata": {},
   "source": [
    "In order to qualitatively evaluate the performance of the `generate()` function and the effect of its arguments, I decided to fix the text prompt to be the same at every run: *\"Who knows if God exists, but for sure I\"*\n",
    "\n",
    "- `do_sample = False, temperature = 1.0`: with no argument tuned, the generation is totally greedy and helds no sense. The text generated is a simple sentence of text repeated over and over until the `max_new_tokens` limit of tokens is reached. When `do_sample` is `False`, it's like having a very low `temperature` value, as sampling (= source of noise) is frozen in favour of a deterministic, greedy approach to generation.\n",
    "- Setting `do_sample` to `True` unlocks the generation, by allowing sampling of more diverse original sequences of tokens, instead of giving always the same greedy text. The tuning of `temperature` lets the magic happen!\n",
    "- The higher the `temperature`, the \"noisier\" and unpredictable the generation will be. Very high temperatures lead to, again, non-sense generation, with wrong words and sequences of random symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d00e7-d4db-440f-8702-11118f07b0a4",
   "metadata": {},
   "source": [
    "## Exercise 3: Reusing Pre-trained LLMs (choose one)\n",
    "\n",
    "Choose **one** of the following exercises (well, *at least* one). In each of these you are asked to adapt a pre-trained LLM (`GPT2Model` or `DistilBERT` are two good choices) to a new Natural Language Understanding task. A few comments:\n",
    "\n",
    "+ Since GPT2 is a *autoregressive* model, there is no latent space aggregation at the last transformer layer (you get the same number of tokens out that you give in input). To use a pre-trained model for a classification or retrieval task, you should aggregate these tokens somehow (or opportunistically select *one* to use).\n",
    "\n",
    "+ BERT models (including DistilBERT) have a special [CLS] token prepended to each latent representation in output from a self-attention block. You can directly use this as a representation for classification (or retrieval).\n",
    "\n",
    "+ The first *two* exercises below can probably be done *without* any fine-tuning - that is, just training a shallow MLP to classify or represent with the appropriate loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc2470",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Training a Text Classifier\n",
    "\n",
    "Peruse the [text classification datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=downloads). Choose a *moderately* sized dataset and use a LLM to train a classifier to solve the problem.\n",
    "\n",
    "**Note**: A good first baseline for this problem is certainly to use an LLM *exclusively* as a feature extractor and then train a shallow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "2b5346f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda 🦅\n"
     ]
    }
   ],
   "source": [
    "# Imports and dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from utils import *\n",
    "from shallowclf import *\n",
    "\n",
    "# Generalizing code for handling different datasets and models\n",
    "datasets = ['ag_news', 'dair-ai/emotion']\n",
    "dataset_name = datasets[1]\n",
    "pretrained_model = 'distilbert-base-uncased'\n",
    "\n",
    "# Features Extraction from the dataset (to be done only the first time: following times we can load the saved features)\n",
    "extract_features = False         # If True, extracts the features from the dataset and saves them again\n",
    "batch_size = 64\n",
    "\n",
    "# Saving configuration\n",
    "save_classification = False\n",
    "folder = f'31_textclassification/{dataset_name}'\n",
    "\n",
    "#Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d5e67",
   "metadata": {},
   "source": [
    "So far, I just tried to fine-tune a pretrained DistilBERT model for the Sequence Classification task.\n",
    "\n",
    "But for this specific exercise, fine-tuning can be avoided! One hint is to use DistilBERT *only* as a mere feature extractor, and to use a very shallow model (an MLP, or even a Logistic Regression!) on the final representation for the multi-class classification.\n",
    "\n",
    "Let's try to do so!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a8d188",
   "metadata": {},
   "source": [
    "The choice of the dataset is crucial for what we're trying to achieve: text classification without having to fine-tune DistilBERT.\n",
    "\n",
    "Looking at the Hugging Face Datasets, one of the best datasets to use is the *ag_news*: a moderately sized multi-class dataset, with perfectly balanced classes.\n",
    "But, as always, I cannot feel satisfied with easy things: my attention got captured by the *dair-ai/emotion* dataset too, that, in comparison to the previous one, looks like a real mess! 6 classes, skewed, with not that much data available.\n",
    "\n",
    "I'll try to face the same challenge using the two datasets, in order to check and report any difference encountered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9bf62d",
   "metadata": {},
   "source": [
    "Looking at the DistilBertTokenizer, I can see that it's a word level tokenizer, at least for the english language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c9e9a63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Features and Labels from 'text_classification/dair-ai/emotion'...📂\n",
      "Features and Labels loaded!👌🏼\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the selected dataset using the load_dataset function from Hugging Face datasets\n",
    "data = load_dataset(dataset_name)\n",
    "print(f\"Dataset \\'{dataset_name}\\' loaded!\")\n",
    "\n",
    "# 2) Instantiate the Tokenizer to tokenize the raw data, and the pretrained model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(pretrained_model)\n",
    "model = DistilBertModel.from_pretrained(pretrained_model).to(device)\n",
    "\n",
    "# Loading features and labels from the folder\n",
    "train_features, test_features, train_labels, test_labels = feature_extractor(data, model, tokenizer,\n",
    "                                                                                batch_size,\n",
    "                                                                                folder,\n",
    "                                                                                extract = extract_features,\n",
    "                                                                                save = save_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "cfbd4ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...💪🏼\n",
      "📜 Epoch 1/20:\t🫥 Training Loss = 1.0757   🎯 Test Accuracy = 60.40%\n",
      "📜 Epoch 2/20:\t🫥 Training Loss = 1.0004   🎯 Test Accuracy = 61.95%\n",
      "📜 Epoch 3/20:\t🫥 Training Loss = 0.9655   🎯 Test Accuracy = 62.80%\n",
      "📜 Epoch 4/20:\t🫥 Training Loss = 0.9377   🎯 Test Accuracy = 63.70%\n",
      "📜 Epoch 5/20:\t🫥 Training Loss = 0.9017   🎯 Test Accuracy = 64.40%\n",
      "📜 Epoch 6/20:\t🫥 Training Loss = 0.8737   🎯 Test Accuracy = 65.45%\n",
      "📜 Epoch 7/20:\t🫥 Training Loss = 0.8416   🎯 Test Accuracy = 65.15%\n",
      "📜 Epoch 8/20:\t🫥 Training Loss = 0.8069   🎯 Test Accuracy = 65.20%\n",
      "📜 Epoch 9/20:\t🫥 Training Loss = 0.7749   🎯 Test Accuracy = 65.65%\n",
      "📜 Epoch 10/20:\t🫥 Training Loss = 0.7397   🎯 Test Accuracy = 65.80%\n",
      "📜 Epoch 11/20:\t🫥 Training Loss = 0.7071   🎯 Test Accuracy = 66.00%\n",
      "📜 Epoch 12/20:\t🫥 Training Loss = 0.6701   🎯 Test Accuracy = 66.75%\n",
      "📜 Epoch 13/20:\t🫥 Training Loss = 0.6322   🎯 Test Accuracy = 66.50%\n",
      "📜 Epoch 14/20:\t🫥 Training Loss = 0.5930   🎯 Test Accuracy = 66.65%\n",
      "📜 Epoch 15/20:\t🫥 Training Loss = 0.5613   🎯 Test Accuracy = 66.75%\n",
      "📜 Epoch 16/20:\t🫥 Training Loss = 0.5206   🎯 Test Accuracy = 66.45%\n",
      "📜 Epoch 17/20:\t🫥 Training Loss = 0.4996   🎯 Test Accuracy = 66.65%\n",
      "📜 Epoch 18/20:\t🫥 Training Loss = 0.4617   🎯 Test Accuracy = 66.50%\n",
      "📜 Epoch 19/20:\t🫥 Training Loss = 0.4305   🎯 Test Accuracy = 66.45%\n",
      "📜 Epoch 20/20:\t🫥 Training Loss = 0.4056   🎯 Test Accuracy = 66.15%\n",
      "\n",
      "Training completed!👌🏼\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for the shallow classifier (MLP) instantiated\n",
    "hidden_size = 512\n",
    "epochs = 20\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Weighted Loss for handling class imbalance\n",
    "weighted = False\n",
    "\n",
    "shallowcls = MLP(hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(shallowcls.parameters(), lr = learning_rate)\n",
    "\n",
    "if weighted:\n",
    "    weights = torch.tensor([3.43, 2.98, 12.27, 7.41, 8.26, 27.97]).to(device)\n",
    "else:\n",
    "    weights = None\n",
    "criterion = nn.CrossEntropyLoss(weight = weights)\n",
    "\n",
    "print(\"Starting Training the MLP...\")\n",
    "training_mlp(shallowcls, optimizer, criterion, epochs, batch_size, train_features, train_labels, test_features, test_labels, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "61e5ddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a Logistic Regression model...💪🏼\n",
      "Model trained! Making predictions on new data...🧐\n",
      "\n",
      "Prediction completed!\n",
      "🎯 Test Accuracy = 65.25%\n",
      "🧨 F1 Score = 0.6436\n",
      "🪡 Precision = 0.6416\n",
      "📣 Recall = 0.6525\n"
     ]
    }
   ],
   "source": [
    "print(\"Training a Logistic Regression model...\")\n",
    "logreg = LogisticRegression(max_iter = 1500).fit(train_features, train_labels)\n",
    "\n",
    "print(\"Model trained! Making predictions on new data...\\n\")\n",
    "pred = logreg.predict(test_features)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, pred)*100\n",
    "f1 = f1_score(test_labels, pred, average = 'weighted')\n",
    "precision = precision_score(test_labels, pred, average = 'weighted')\n",
    "recall = recall_score(test_labels, pred, average = 'weighted')\n",
    "\n",
    "print(f\"Prediction completed! Test Accuracy = {accuracy:.2f}%\\nF1 Score = {f1:.4f}\\nPrecision = {precision:.4f}\\nRecall = {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d86cda",
   "metadata": {},
   "source": [
    "- Using the *dair-ai/emotion* dataset, the multi-class classifier, with or without any tweaks on the loss weights to address the class imbalance problem, struggles to reach 67% Test Accuracy (while a fine-tuned DistilBERT is capable to go up to 94% accuracy).\n",
    "- Using the *ag_news* dataset, instead, the classifier makes no effort to provide results with 92% Test Accuracy.\n",
    "\n",
    "Furthermore, results obtained by training a simple MLP and an even simpler Logistic Regression are basically the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e103ec",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Training a Question Answering Model\n",
    "\n",
    "Peruse the [multiple choice question answering datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:multiple-choice&sort=downloads). Chose a *moderately* sized one and train a model to answer contextualized multiple-choice questions. You *might* be able to avoid fine-tuning by training a simple model to *rank* the multiple choices (see margin ranking loss in Pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "e6e10735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda 🦅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 405k/405k [00:00<00:00, 786kB/s]\n",
      "Downloading data: 100%|██████████| 6.97M/6.97M [00:02<00:00, 2.65MB/s]\n",
      "Downloading data: 100%|██████████| 407k/407k [00:00<00:00, 1.39MB/s]\n",
      "Generating test split: 100%|██████████| 1436/1436 [00:00<00:00, 20448.77 examples/s]\n",
      "Generating train split: 100%|██████████| 25421/25421 [00:00<00:00, 515151.11 examples/s]\n",
      "Generating validation split: 100%|██████████| 1436/1436 [00:00<00:00, 287220.82 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'race-middle' loaded!👌🏼\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DistilBertTokenizer and DistilBertModel instantiated!👌🏼\n"
     ]
    }
   ],
   "source": [
    "# Imports and dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Generalizing code for handling different datasets and models\n",
    "pretrained_model = 'distilbert-base-uncased'\n",
    "\n",
    "#Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(emojize(f\"Device: {device} :eagle:\") if torch.cuda.is_available() else emojize(f\"Device: {device} :snail:\")) if emojizer else print(f\"Device: {device}\")\n",
    "\n",
    "# Loading the selected dataset (race-middle), Model and Tokenizer\n",
    "data = load_dataset('race', 'middle')\n",
    "print(emojize(f\"Dataset \\'race-middle\\' loaded!:OK_hand_medium-light_skin_tone:\\n\") if emojizer else f\"Dataset \\'{dataset_name}\\' loaded!\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model).to(device)\n",
    "\n",
    "print(emojize(f\"\\n{tokenizer.__class__.__name__} and {model.__class__.__name__} instantiated!:OK_hand_medium-light_skin_tone:\") if emojizer else f\"\\n{tokenizer.__class__.__name__} and {model.__class__.__name__} instantiated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "35ede8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['example_id', 'article', 'answer', 'question', 'options'],\n",
       "    num_rows: 25421\n",
       "})"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']\n",
    "\n",
    "# input1 = article\n",
    "# input2 = question\n",
    "\n",
    "# output1 = answer predicted from article\n",
    "# output2 = answer predicted from question\n",
    "\n",
    "# criterion = nn.MarginRankingLoss()\n",
    "# loss = criterion(output1, output2, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce054686",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Training a Retrieval Model\n",
    "\n",
    "The Hugging Face dataset repository contains a large number of [\"text retrieval\" problems](https://huggingface.co/datasets?task_categories=task_categories:text-retrieval&p=1&sort=downloads). These tasks generally require that the model measure *similarity* between text in some metric space -- naively, just a cosine similarity between [CLS] tokens can get you pretty far. Find an interesting retrieval problem and train a model (starting from a pre-trained LLM of course) to solve it.\n",
    "\n",
    "**Tip**: Sometimes identifying the *retrieval* problems in these datasets can be half the challenge. [This dataset](https://huggingface.co/datasets/BeIR/scifact) might be a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f22f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
