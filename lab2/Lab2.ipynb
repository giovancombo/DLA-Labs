{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44bc1b80",
   "metadata": {},
   "source": [
    "***Deep Learning Applications 2023** course, held by Professor **Andrew David Bagdanov** - University of Florence, Italy*\n",
    "\n",
    "*Notebook and code created by **Giovanni Colombo** - Mat. 7092745*\n",
    "\n",
    "Check the dedicated [Repository on GitHub](https://github.com/giovancombo/DLA-Labs/tree/main/lab2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #2 - LLMs\n",
    "\n",
    "In this laboratory we will get our hands dirty working with Large Language Models (e.g. GPT and BERT) to do various useful things. I you haven't already, it is highly recommended to:\n",
    "\n",
    "+ Read the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper, which is the basis for all transformer-based LLMs.\n",
    "+ Watch (and potentially *code along*) with this [Andrej Karpathy video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which shows you how to build an autoregressive GPT model from the ground up.\n",
    "\n",
    "## Exercise 1: Warming Up\n",
    "In this first exercise you will train a *small* autoregressive GPT model for character generation (the one used by Karpathy in his video) to generate text in the style of Dante Aligheri. Use [this file](https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt), which contains the entire text of Dante's Inferno (**note**: you will have to delete some introductory text at the top of the file before training). Train the model for a few epochs, monitor the loss, and generate some text at the end of training. Qualitatively evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "bd466d3b-cc41-4de3-9f82-3547569909f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0 🦅\n"
     ]
    }
   ],
   "source": [
    "# Imports and dependencies\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "# %pip install emoji\n",
    "from emoji import emojize\n",
    "emojizer = True             # Bringing some color to the code\n",
    "\n",
    "# Hyperparameters\n",
    "text = 'taylor_swift'\n",
    "\n",
    "train_size = 0.7\n",
    "\n",
    "batch_size = 64             # Batch size = number of independent sequences of text, analyzed in parallel\n",
    "block_size = 512            # Dimension of an input seuqence of characters, for next character prediction\n",
    "n_embd = 100                # Embedding dimension for each token\n",
    "\n",
    "n_heads = 4                 # Number of Self-Attention heads in a Multi-Head Attention block\n",
    "n_layers = 4                # Number of Blocks of the Transformer\n",
    "learning_rate = 5e-4\n",
    "dropout = 0.4\n",
    "\n",
    "eval_iters = 200\n",
    "total_steps = 10\n",
    "log_interval = 100\n",
    "\n",
    "# Creating a configuration dictionary for logging in wandb\n",
    "config = dict(\n",
    "    text = text,\n",
    "    batch_size = batch_size,\n",
    "    block_size = block_size,\n",
    "    n_embd = n_embd,\n",
    "    n_heads = n_heads,\n",
    "    n_layers = n_layers,\n",
    "    learning_rate = learning_rate,\n",
    "    dropout = dropout,\n",
    "    eval_iters = eval_iters,\n",
    "    total_steps = total_steps,\n",
    "    log_interval = log_interval,\n",
    "    train_size = train_size,\n",
    ")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(emojize(f\"Device: {device} :eagle:\") if torch.cuda.is_available() else emojize(f\"Device: {device} :snail:\")) if emojizer else print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d8650755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 298350\n"
     ]
    }
   ],
   "source": [
    "# Downloading the Dante's Divina Commedia txt file from the internet\n",
    "#!wget https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt\n",
    "\n",
    "# Opening and reading the content of the input text file\n",
    "with open(text + '.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"Length of dataset in characters:\", len(text))\n",
    "\n",
    "# Creating a sorted set of all the unique characters present in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)             # Number of unique characters in the text = size of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "58363a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary for mapping characters to integers and viceversa\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[i] for i in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# text = list of characters\n",
    "# data = list of integers of all the text --> it's our dataset\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "\n",
    "# Splitting our dataset in train and validation sets\n",
    "n = int(train_size*len(data))\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "32975468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement a single Self-Attention Head = creating communication between tokens\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)       # Q,K,V = matrices (n_embd, head_size)\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape                       # (B,T,C) = (batch_size, block_size, n_embd)\n",
    "        q = self.query(x)                     # (B,T,n_embd) @ (n_embd,head_size) = (B,T,head_size)\n",
    "        k = self.key(x)                       # (batch_size, block_size, head_size)\n",
    "        v = self.value(x)                     # (batch_size, block_size, head_size)\n",
    "        # Dot-Product and Scaling\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5              # (B,T,head_size) @ (B,head_size,T) = (B,T,T)\n",
    "        # Masking (only for Decoder)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        # Softmax                         \n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        # Applying dropout for randomly inhibiting some communication between tokens\n",
    "        wei = self.dropout(wei)\n",
    "        # Dot-Product with Values\n",
    "        out = wei @ v                                       # (B,T,T) @ (B,T,C) = (B,T,C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0dbe2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement a Multi-Head Attention block = Multiple Self-Attention Heads in parallel and concatenated\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([SelfAttention(head_size) for _ in range(n_heads)])\n",
    "        self.projection = nn.Linear(n_embd, n_embd)             # For handling Residual Connections\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)   # Concatenation of the outputs of the heads\n",
    "        out = self.dropout(self.projection(out))                # Dropout always at the end\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ad41fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement the Feed Forward block = a simple MLP, allowing tokens to do some computation after communication\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),    # The original paper uses 4*n_embd as hidden dimension\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffwd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9b7ae7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement a Decoder block = Multi-Head Attention + Feed Forward, comprehensive of Residual Connections & Layer Normalization\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        head_size = n_embd // n_heads       # As the original paper does\n",
    "\n",
    "        self.attention = MultiHeadAttention(n_heads, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "29581c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement the Transformer Decoder: many Decoder blocks in sequence, with a final Linear layer\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        # Creating Lookup Tables for storing Token and Positional Encodings\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # In the original paper, authors used 6 layers of 8 heads each\n",
    "        self.decoder_blocks = nn.Sequential(*[DecoderBlock(n_embd, n_heads = n_heads) for _ in range(n_layers)])\n",
    "\n",
    "        self.ln = nn.LayerNorm(n_embd)\n",
    "        self.fc = nn.Linear(n_embd, vocab_size)     # Final Linear layer for predicting the next character\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        B,T = idx.shape         # idx and targets are both (B,T) tensors of integers: B = batch_size, T = block_size\n",
    "        # Creating the Embeddings for the specific input tokens\n",
    "        tok_emb = self.tok_embedding(idx)                               # (B,T,C) = (batch_size, block_size, n_embd)\n",
    "        pos_emb = self.pos_embedding(torch.arange(T, device = device))  # (T,C) = (block_size, n_embd)\n",
    "        x = tok_emb + pos_emb                       # (B,T,C) + (T,C) = (B,T,C) for Broadcasting Semantics\n",
    "\n",
    "        x = self.decoder_blocks(x)                  # (B,T,n_embd)\n",
    "        x = self.ln(x)                              # (B,T,n_embd)        \n",
    "        logits = self.fc(x)                         # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "            return logits\n",
    "        else:\n",
    "            B,T,C = logits.shape                    # (B,T,vocab_size)\n",
    "            # Reshaping logits and targets for dimension issues with PyTorch cross_entropy function\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return logits, targets, loss\n",
    "\n",
    "    # Let's create a function for generating new text!\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits = self(idx_cond)                     # Calling the Forward method: no targets provided, we're generating\n",
    "            logits = logits[:, -1, :]                   # Focusing only on the last character                -> (B,vocab_size)\n",
    "            probs = F.softmax(logits, dim = -1)         # Getting probabilities distribution through Softmax -> (B,vocab_size)           \n",
    "            idx_next = torch.multinomial(probs, num_samples = 1)     # Sampling from the distribution -> (B,1)\n",
    "            idx = torch.cat((idx, idx_next), dim = 1)   # Adding the new character to the sequence -> (B,T+1)  \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3a92c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for instantiating Model, Loss and Optimizer\n",
    "def build_model():\n",
    "    model = TransformerDecoder().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(emojize(f\":puzzle_piece: Model {model.__class__.__name__} instantiated!\\n\" +\n",
    "                        f\":beans: Number of parameters: {n_params}\") if emojizer else f\"Model {model.__class__.__name__} instantiated!\\nNumber of parameters: {n_params}\")\n",
    "    print(emojize(f\":fire: Optimizer: {optimizer.__class__.__name__}\") if emojizer else f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "    print(optimizer)\n",
    "    print(emojize(f\"Device: {device} :eagle:\") if torch.cuda.is_available() else emojize(f\"Device: {device} :snail:\")) if emojizer else print(f\"Device: {device}\")\n",
    "\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2a725106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function for getting a new batch of random sequences of characters in the text\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,))      # Drawing a set of batch_size indexes in the text\n",
    "    x = torch.stack([data[i : i+block_size] for i in idx])          # Stacking block_size characters from each index\n",
    "    y = torch.stack([data[i+1 : i+block_size+1] for i in idx])      # Creating the targets (= inputs shifted by 1)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Let's create a function for saving and visualizing train and validation losses\n",
    "@torch.no_grad()                            # Context Manager for disabling gradient calculation: better memory usage\n",
    "def estimate_loss():\n",
    "    outloss = {}\n",
    "    outacc = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        accuracies = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):         # Evaluating the losses eval_iters times on different batches\n",
    "            X, Y = get_batch(split)\n",
    "            logits, targets, loss = model(X, Y)\n",
    "\n",
    "            # Computing accuracy\n",
    "            total, correct = 0, 0\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (pred == targets).sum().item()\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            losses[k] = loss.item()\n",
    "            accuracies[k] = accuracy\n",
    "        outloss[split] = losses.mean()\n",
    "        outacc[split] = accuracies.mean()\n",
    "\n",
    "    return outloss, outacc                  # out = dictionary of train and validation mean losses\n",
    "\n",
    "# Function for log of validation data at the end of an epoch\n",
    "def log_validation(epoch, mean_loss, val_loss, mean_accuracy, val_accuracy, step):\n",
    "    wandb.log({\"Train Loss\": mean_loss, \n",
    "               \"Validation Loss\": val_loss,\n",
    "               \"Epoch\": epoch + 1,\n",
    "               \"Train Accuracy\": mean_accuracy, \n",
    "               \"Validation Accuracy\": val_accuracy,}, step = step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "299d7365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop      \n",
    "def train(model, criterion, optimizer):\n",
    "    # Telling W&B to watch gradients and the model parameters\n",
    "    wandb.watch(model, criterion, log = \"all\", log_freq = log_interval)\n",
    "    example_ct = 0\n",
    "\n",
    "    print(emojize(\"\\nStarting Training...:flexed_biceps_medium-light_skin_tone:\") if emojizer else \"Starting Training...\")\n",
    "    for step in range(total_steps):\n",
    "        model.train()\n",
    "        xb, yb = get_batch('train')                 # Sampling a batch of data\n",
    "        _, _, loss = model(xb, yb)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none = True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        example_ct += batch_size\n",
    "\n",
    "        if step % log_interval == 0 or step == total_steps - 1:\n",
    "            losses, accuracies = estimate_loss()\n",
    "            log_validation(step, losses['train'], losses['val'], accuracies['train'], accuracies['val'], step)\n",
    "            print(emojize(f\":scroll: Step {step+1}/{total_steps}:\\t:dotted_line_face: Train Loss = {losses['train']:.4f}; Val Loss = {losses['val']:.4f}\\t:bullseye: Train Accuracy = {accuracies['train']:.2f}%; Val Accuracy = {accuracies['val']:.2f}%\") if emojizer\n",
    "                  else f\"Step {step+1}/{total_steps}:\\tTrain Loss = {losses['train']:.4f}; Val Loss = {losses['val']:.4f}\\tTrain Accuracy = {accuracies['train']:.2f}%; Val Accuracy = {accuracies['val']:.2f}%\")\n",
    "\n",
    "    print(emojize(\"\\nTraining completed!:OK_hand_medium-light_skin_tone:\") if emojizer else \"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating new text!\n",
    "def text_generator(model, new_tokens):\n",
    "    # context = First character of the generated sequence = (1,1) Tensor of value 0 --> Token embedding for New Line\n",
    "    context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
    "\n",
    "    print(emojize(\"\\n:sparkles: TEXT GENERATION ACTIVATED! Generating new text...\") if emojizer else \"\\nTEXT GENERATION ACTIVATED! Generating new text...\")\n",
    "    generated_text = decode(model.generate(context, max_new_tokens = new_tokens)[0].tolist())\n",
    "    \n",
    "    print(emojize(\"Text generated!:magic_wand:\") if emojizer else \"Text generated!\")\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "01014666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☀️ Initializing Weights & Biases...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\giova\\__UNI\\Deep Learning Applications\\LABORATORIO\\Modulo2_LLMs\\wandb\\run-20240108_231328-neqad6c2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/giovancombo/DLA_Lab2_LLM/runs/neqad6c2' target=\"_blank\">major-sun-62</a></strong> to <a href='https://wandb.ai/giovancombo/DLA_Lab2_LLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/giovancombo/DLA_Lab2_LLM' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab2_LLM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/giovancombo/DLA_Lab2_LLM/runs/neqad6c2' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab2_LLM/runs/neqad6c2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 Model TransformerDecoder instantiated!\n",
      "🫘 Number of parameters: 553289\n",
      "🔥 Optimizer: \n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0005\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "Device: cuda:0 🦅\n",
      "\n",
      "Starting Training...💪🏼\n",
      "📜 Step 1/10:\t🫥 Train Loss = 4.4583; Val Loss = 4.4586\t🎯 Train Accuracy = 2.87%; Val Accuracy = 2.83%\n",
      "📜 Step 10/10:\t🫥 Train Loss = 3.4851; Val Loss = 3.5022\t🎯 Train Accuracy = 17.09%; Val Accuracy = 16.59%\n",
      "\n",
      "Training completed!👌🏼\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁█</td></tr><tr><td>Train Accuracy</td><td>▁█</td></tr><tr><td>Train Loss</td><td>█▁</td></tr><tr><td>Validation Accuracy</td><td>▁█</td></tr><tr><td>Validation Loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>17.08778</td></tr><tr><td>Train Loss</td><td>3.48508</td></tr><tr><td>Validation Accuracy</td><td>16.58801</td></tr><tr><td>Validation Loss</td><td>3.50219</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">major-sun-62</strong> at: <a href='https://wandb.ai/giovancombo/DLA_Lab2_LLM/runs/neqad6c2' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab2_LLM/runs/neqad6c2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240108_231328-neqad6c2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generation configuration\n",
    "generation = False\n",
    "new_tokens = 500           # Number of tokens generated\n",
    "\n",
    "# Saving configuration\n",
    "save_model = False\n",
    "folder = f\"1_transformers/{text}\"\n",
    "model_name = \"model_\" + text + \"_bs\" + str(batch_size) + \"_bl\" + str(block_size) + \"_ne\" + str(n_embd) + \"_nh\" + str(n_heads) + \"_nl\" + str(n_layers) + \"_lr\" + str(learning_rate)\n",
    "\n",
    "\n",
    "wandb.login()\n",
    "print(emojize(\":sun: Initializing Weights & Biases run...\") if emojizer else \"Initializing Weights & Biases run...\")\n",
    "\n",
    "with wandb.init(project = \"DLA_Lab2_LLM\", config = config):\n",
    "    config = wandb.config\n",
    "    \n",
    "    # Building model and optimizer\n",
    "    model, criterion, optimizer = build_model()\n",
    "\n",
    "    # Training the model\n",
    "    train(model, criterion, optimizer)\n",
    "\n",
    "    # Generating new text from the model trained (optional)\n",
    "    if generation:\n",
    "        text_generator(model, new_tokens)\n",
    "\n",
    "    # Saving the model (optional)\n",
    "    if save_model:\n",
    "        print(emojize(f\"\\nSaving Model parameters in \\'{folder}\\'... :writing_hand_medium-light_skin_tone:\") if emojizer else f\"\\nSaving Model parameters in \\'{folder}\\'...\")\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            print(emojize(f\"Folder \\'{folder}\\' created :file_folder:\") if emojizer else f\"Folder \\'{folder}\\' created!\")\n",
    "\n",
    "        torch.save(model.state_dict(), f\"{folder}/{model_name}.pt\")\n",
    "        print(emojize(f\"\\nModel parameters saved! :floppy_disk:\") if emojizer else '\\nModel parameters saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68441a09-dfaf-424a-b640-4fc8cea289b5",
   "metadata": {},
   "source": [
    "## Exercise 2: Working with Real LLMs\n",
    "\n",
    "Our toy GPT can only take us so far. In this exercise we will see how to use the [Hugging Face](https://huggingface.co/) model and dataset ecosystem to access a *huge* variety of pre-trained transformer models.\n",
    "\n",
    "### Exercise 2.1: Installation and text tokenization\n",
    "\n",
    "First things first, we need to install the [Hugging Face transformer library](https://huggingface.co/docs/transformers/index):\n",
    "\n",
    "    conda install -c huggingface -c conda-forge transformers\n",
    "    \n",
    "The key classes that you will work with are `GPT2Tokenizer` to encode text into sub-word tokens, and the `GPT2LMHeadModel`. **Note** the `LMHead` part of the class name -- this is the version of the GPT2 architecture that has the text prediction heads attached to the final hidden layer representations (i.e. what we need to **generate** text). \n",
    "\n",
    "Instantiate the `GPT2Tokenizer` and experiment with encoding text into integer tokens. Compare the length of input with the encoded sequence length.\n",
    "\n",
    "**Tip**: Pass the `return_tensors='pt'` argument to the tokenizer to get Pytorch tensors as output (instead of lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "af199a6d-1f3a-4b2c-a23f-d697b93c5adb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu 🐌\n"
     ]
    }
   ],
   "source": [
    "# Imports and dependencies\n",
    "import os\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cpu')\n",
    "\n",
    "print(emojize(f\"Device: {device} :snail:\")) if emojizer else print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e6e4bfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔡 Input text:\t\tHello World\n",
      "📑 Tokenized text:\ttensor([[15496,  2159]])\n"
     ]
    }
   ],
   "source": [
    "# Custom input text to be tokenized\n",
    "input_text = \"Hello World\"\n",
    "\n",
    "# Creating a subword tokenizer from GPT2 pretrained model: new vocab_size = 50,257!\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Converting string inputs to sequences of tokens: let's compare the input length with the encoded sequence length\n",
    "# It's curious to see the difference between calling tokenizer itself, or its encode/tokenize attributes\n",
    "tokenized_text = tokenizer.encode(input_text, return_tensors = 'pt')\n",
    "\n",
    "print(emojize(f\":input_latin_lowercase: Input text:\\t\\t{input_text}\\n:bookmark_tabs: Tokenized text:\\t{tokenized_text}\") if emojizer else f\"Input string:\\t\\t{input_text}\\nTokenized string:\\t{tokenized_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbfad2c",
   "metadata": {},
   "source": [
    "This GPT2 tokenizer works on a subword level, so what I could notice is that, tipically, inputs are divided into several 2/3/4 words chunks and encoded to a particular integer.\n",
    "As the input string sequences increases their length, the encoded sequence length increases.\n",
    "Input sequences of 2/3/4 characters can be encoded to a single integer.\n",
    "\n",
    "While inputing the sequence *\"Hello World\"*, I could notice that the tokenizer has a single integer for the whole \"Hello\" and \"World\" words, suggesting that many existing english (and not only, maybe) words are encoded to a single integer.\n",
    "\n",
    "Passing the `return_tensors = 'pt'` argument makes the tokenizer output PyTorch tensors instead of lists.\n",
    "\n",
    "From the original [Hugging Face Documentation](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Tokenizer), we can read that:\n",
    "> This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will be encoded differently whether it is at the beginning of the sentence (without space) or not.\n",
    "\n",
    "Trying to see the behaviour of slightly different versions of the sequence *\"Hello World\"*, it's possible to notice that:\n",
    "- *\"Hello World\"* encodes to tensor([[15496,  2159]])\n",
    "- *\"hello World\"* encodes to tensor([[31373,  2159]]) --> Case matters\n",
    "- *\" hello World\"* encodes to tensor([[23748,  2159]]) --> Space matters!\n",
    "- *\"HelloWorld\"* encodes to tensor([[15496, 10603]])\n",
    "- *\"Hello World \"* encodes to tensor([[15496,  2159,   220]]) --> But space character has its own encoding integer when nothing follows it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458b725-63c1-49ae-8011-71a9196387b8",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Generating Text\n",
    "\n",
    "There are a lot of ways we can, given a *prompt* in input, sample text from a GPT2 model. Instantiate a pre-trained `GPT2LMHeadModel` and use the [`generate()`](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate text from a prompt.\n",
    "\n",
    "**Note**: The default inference mode for GPT2 is *greedy* which might not result in satisfying generated text. Look at the `do_sample` and `temperature` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bdad9208-cc9e-4750-baa5-f9367e71362a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel instantiated!👌🏼\n",
      "\n",
      "🔡 Input text:\tThe main goal in life is\n",
      "✨ Waiting for the generation of new text...\n",
      "\n",
      "Text generated!🪄\n",
      "\u001b[3mThe main goal in life is to be as creative as possible in your career – so to see this as fulfilling when you can do so without procrastinating, isn't that amazing?\n",
      "\n",
      "Yes, it is! It is all about taking your imagination but it's all important, and it gives you a boost without necessarily thinking about it in the slightest! You'd never know, but seeing someone take advantage of the creativity of their subconscious without feeling ashamed about doing so is something really awesome to experience. You really give it away\u001b[0m\n",
      "\n",
      "Text generated saved in 'text_generation/generation_log.txt' 💾\n"
     ]
    }
   ],
   "source": [
    "# Saving configuration\n",
    "save_generation = False\n",
    "folder = '2_text_generation'\n",
    "\n",
    "# We can even ask the user to input a text prompt\n",
    "input_text = \"The main goal in life is\"         # input(\"What do you want to say?\\n\")\n",
    "\n",
    "# Hyperparameters for text generation\n",
    "max_new_tokens = 100\n",
    "do_sample = True\n",
    "temperature = 1.2\n",
    "early_stopping = True\n",
    "no_repeat_ngram_size = 2\n",
    "\n",
    "# Loading the pretrained model: setting the padding token as the end of sequence token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id = tokenizer.eos_token_id)\n",
    "\n",
    "print(emojize(f\"{model.__class__.__name__} instantiated!:OK_hand_medium-light_skin_tone:\\n\") if emojizer else f\"{model.__class__.__name__} instantiated!\\n\")\n",
    "print(emojize(f\":input_latin_lowercase: Input text:\\t{input_text}\\n:sparkles: Waiting for the generation of new text...\") if emojizer else f\"Input text: {input_text}\\nWaiting for the generation of new text...\")\n",
    "\n",
    "# inputs is a Pytorch tensor\n",
    "tokenized_text = tokenizer(input_text, return_tensors = 'pt')\n",
    "\n",
    "# Let's generate some text from the input sequence: new_text is a Pytorch tensor\n",
    "generated_text = tokenizer.decode(model.generate(tokenized_text['input_ids'],\n",
    "                                                 max_new_tokens = max_new_tokens,\n",
    "                                                 do_sample = do_sample,\n",
    "                                                 temperature = temperature,\n",
    "                                                 early_stopping = early_stopping,\n",
    "                                                 no_repeat_ngram_size = no_repeat_ngram_size)[0].tolist())\n",
    "\n",
    "print(emojize(\"\\nText generated!:magic_wand:\") if emojizer else \"\\nText generated!\")\n",
    "print(f\"{generated_text}\")\n",
    "\n",
    "\n",
    "# Saving the generated text in a txt file\n",
    "if save_generation:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        print(emojize(f\"Folder \\'{folder}\\' created!:file_folder:\") if emojizer else f\"Folder \\'{folder}\\' created!\")\n",
    "\n",
    "    with open(f\"{folder}/generation_log.txt\", 'a') as f:\n",
    "        f.write(f\"HPs: max_new_tokens = {max_new_tokens}, do_sample = {do_sample}, temperature = {temperature}, early_stopping = {early_stopping}, no_repeat_ngram_size = {no_repeat_ngram_size}\\n\\n\")\n",
    "        f.write(f\"Input text: {input_text}\\nGenerated text: {generated_text}\\n\\n- - - - - - - - - - - - - - - -\\n\")\n",
    "\n",
    "    print(emojize(f\"\\nText generated saved in \\'{folder}/generation_log.txt\\' :floppy_disk:\") if emojizer else '\\nText generated saved in \\'{folder}/generation_log.txt\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c1265",
   "metadata": {},
   "source": [
    "In order to qualitatively evaluate the performance of the `generate()` function and the effect of its arguments, I decided to fix the text prompt to be the same at every run: *\"Who knows if God exists, but for sure I\"*\n",
    "\n",
    "- `do_sample = False, temperature = 1.0`: with no argument tuned, the generation is totally greedy and helds no sense. The text generated is a simple sentence of text repeated over and over until the `max_new_tokens` limit of tokens is reached. When `do_sample` is `False`, it's like having a very low `temperature` value, as sampling (= source of noise) is frozen in favour of a deterministic, greedy approach to generation.\n",
    "- Setting `do_sample` to `True` unlocks the generation, by allowing sampling of more diverse original sequences of tokens, instead of giving always the same greedy text. The tuning of `temperature` lets the magic happen!\n",
    "- The higher the `temperature`, the \"noisier\" and unpredictable the generation will be. Very high temperatures lead to, again, non-sense generation, with wrong words and sequences of random symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "583104d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛤️ Generating through the \u001b[3mpipeline\u001b[0m\n",
      "\n",
      "🔡 Input text:\tThe main goal in life is\n",
      "✨ Waiting for the generation of new text...\n",
      "\n",
      "Text generated!🪄\n",
      " The main goal in life is always the pursuit of happiness, and that involves a profound level of compassion with every human. Even if others make a mockery of your moral high conduct to stop you from pursuing your dreams - don't even try to get involved.\n",
      "\n",
      "Being an adult, you do not need to ask anyone \"How'd you know I was attractive if a celebrity made a video.\" Rather, it makes sense for your mind to become an organ with a great deal of attention given towards each individual's unique and fascinating personal\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# An alternative way for generating text is to use a Hugging Face pipeline\n",
    "generator = pipeline('2_text-generation', model = model, tokenizer = tokenizer)\n",
    "\n",
    "print(emojize(f\":railway_track: Generating through the \\x1B[3mpipeline\\x1B[0m\\n\\n:input_latin_lowercase: Input text:\\t{input_text}\\n:sparkles: Waiting for the generation of new text...\\n\") if emojizer else f\"Generating through the pipeline\\n\\nInput text:\\t{input_text}\\nWaiting for the generation of new text...\\n\")\n",
    "print(emojize(\"Text generated!:magic_wand:\\n\") if emojizer else \"Text generated!\\n\",\n",
    "      generator(input_text,\n",
    "                max_new_tokens = max_new_tokens,\n",
    "                do_sample = do_sample,\n",
    "                temperature = temperature,\n",
    "                early_stopping = early_stopping,\n",
    "                no_repeat_ngram_size = no_repeat_ngram_size)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d00e7-d4db-440f-8702-11118f07b0a4",
   "metadata": {},
   "source": [
    "## Exercise 3: Reusing Pre-trained LLMs (choose one)\n",
    "\n",
    "Choose **one** of the following exercises (well, *at least* one). In each of these you are asked to adapt a pre-trained LLM (`GPT2Model` or `DistilBERT` are two good choices) to a new Natural Language Understanding task. A few comments:\n",
    "\n",
    "+ Since GPT2 is a *autoregressive* model, there is no latent space aggregation at the last transformer layer (you get the same number of tokens out that you give in input). To use a pre-trained model for a classification or retrieval task, you should aggregate these tokens somehow (or opportunistically select *one* to use).\n",
    "\n",
    "+ BERT models (including DistilBERT) have a special [CLS] token prepended to each latent representation in output from a self-attention block. You can directly use this as a representation for classification (or retrieval).\n",
    "\n",
    "+ The first *two* exercises below can probably be done *without* any fine-tuning - that is, just training a shallow MLP to classify or represent with the appropriate loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc2470",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Training a Text Classifier\n",
    "\n",
    "Peruse the [text classification datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=downloads). Choose a *moderately* sized dataset and use a LLM to train a classifier to solve the problem.\n",
    "\n",
    "**Note**: A good first baseline for this problem is certainly to use an LLM *exclusively* as a feature extractor and then train a shallow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "2b5346f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda 🦅\n"
     ]
    }
   ],
   "source": [
    "# Imports and dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Generalizing code for handling different datasets and models\n",
    "datasets = ['ag_news', 'dair-ai/emotion']\n",
    "dataset_name = datasets[1]\n",
    "pretrained_model = 'distilbert-base-uncased'\n",
    "\n",
    "#Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(emojize(f\"Device: {device} :eagle:\") if torch.cuda.is_available() else emojize(f\"Device: {device} :snail:\")) if emojizer else print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d5e67",
   "metadata": {},
   "source": [
    "So far, I just tried to fine-tune a pretrained DistilBERT model for the Sequence Classification task.\n",
    "\n",
    "But for this specific exercise, fine-tuning can be avoided! One hint is to use DistilBERT *only* as a mere feature extractor, and to use a very shallow model (an MLP, or even a Logistic Regression!) on the final representation for the multi-class classification.\n",
    "\n",
    "Let's try to do so!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a8d188",
   "metadata": {},
   "source": [
    "The choice of the dataset is crucial for what we're trying to achieve: text classification without having to fine-tune DistilBERT.\n",
    "\n",
    "Looking at the Hugging Face Datasets, one of the best datasets to use is the *ag_news*: a moderately sized multi-class dataset, with perfectly balanced classes.\n",
    "But, as always, I cannot feel satisfied with easy things: my attention got captured by the *dair-ai/emotion* dataset too, that, in comparison to the previous one, looks like a real mess! 6 classes, skewed, with not that much data available.\n",
    "\n",
    "I'll try to face the same challenge using the two datasets, in order to check and report any difference encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "72a0dfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giova\\anaconda3\\envs\\DLA\\lib\\site-packages\\datasets\\load.py:1429: FutureWarning: The repository for dair-ai/emotion contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/dair-ai/emotion\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'dair-ai/emotion' loaded!👌🏼\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DistilBertTokenizer and DistilBertModel instantiated!👌🏼\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the selected dataset using the load_dataset function from Hugging Face datasets\n",
    "data = load_dataset(dataset_name)\n",
    "print(emojize(f\"Dataset \\'{dataset_name}\\' loaded!:OK_hand_medium-light_skin_tone:\\n\") if emojizer else f\"Dataset \\'{dataset_name}\\' loaded!\")\n",
    "\n",
    "# 2) Instantiate the Tokenizer to tokenize the raw data\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "# 3) Instantiate the pre-trained Model\n",
    "model = DistilBertModel.from_pretrained(pretrained_model).to(device)\n",
    "\n",
    "print(emojize(f\"\\n{tokenizer.__class__.__name__} and {model.__class__.__name__} instantiated!:OK_hand_medium-light_skin_tone:\") if emojizer else f\"\\n{tokenizer.__class__.__name__} and {model.__class__.__name__} instantiated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9bf62d",
   "metadata": {},
   "source": [
    "Looking at the DistilBertTokenizer, I can see that it's a word level tokenizer, at least for the english language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c9e9a63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Features and Labels from 'text_classification/dair-ai/emotion'...📂\n",
      "Features and Labels loaded!👌🏼\n"
     ]
    }
   ],
   "source": [
    "# Features Extraction from the dataset (to be done only the first time: following times we can load the saved features)\n",
    "extract_features = False         # If True, extracts the features from the dataset and saves them again\n",
    "batch_size = 64\n",
    "\n",
    "# Saving configuration\n",
    "save_classification = False\n",
    "folder = f'3_1_text_classification/{dataset_name}'\n",
    "\n",
    "if extract_features:\n",
    "    \n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "\n",
    "    # Extracting features from the Train dataset\n",
    "    for i in tqdm(range(0, len(data['train']), batch_size), desc = emojize('Extracting Train Features :hammer_and_wrench:') if emojizer else 'Extracting Train Features'):\n",
    "        \n",
    "        encoded_inputs = tokenizer(data['train']['text'][i : i + batch_size], padding = True, truncation = True, return_tensors = 'pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_inputs)\n",
    "        encoded_features = outputs.last_hidden_state.mean(dim = 1).detach().cpu().numpy()\n",
    "\n",
    "        train_features.append(encoded_features)\n",
    "        train_labels.append(data['train']['label'][i : i + batch_size])\n",
    "\n",
    "    # Extracting features from the Test dataset\n",
    "    for i in tqdm(range(0, len(data['test']), batch_size), desc = emojize('Extracting Test Features :hammer_and_wrench:' if emojizer else 'Extracting Test Features')):\n",
    "        \n",
    "        encoded_inputs = tokenizer(data['test']['text'][i : i + batch_size], padding = True, truncation = True, return_tensors = 'pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_inputs)\n",
    "        encoded_features = outputs.last_hidden_state.mean(dim = 1).detach().cpu().numpy()\n",
    "        \n",
    "        test_features.append(encoded_features)\n",
    "        test_labels.append(data['test']['label'][i : i + batch_size])\n",
    "\n",
    "    # Concatenating the lists of features and labels into a single NumPy array\n",
    "    # train_features is now a (16000, 768) array: 768 is the embedding dimension\n",
    "    train_features = np.concatenate(train_features, axis = 0)\n",
    "    train_labels = np.concatenate(train_labels, axis = 0)\n",
    "\n",
    "    test_features = np.concatenate(test_features, axis = 0)\n",
    "    test_labels = np.concatenate(test_labels, axis = 0)\n",
    "\n",
    "    print(emojize(\"Done!:OK_hand_medium-light_skin_tone:\") if emojizer else 'Done!')\n",
    "\n",
    "    if save_classification:\n",
    "        print(emojize(f\"\\nSaving Features and Labels in \\'{folder}\\'... :writing_hand_medium-light_skin_tone:\") if emojizer else f\"\\nSaving features and labels in \\'{folder}\\'...\")\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            print(emojize(f\"Folder \\'{folder}\\' created :file_folder:\") if emojizer else f\"Folder \\'{folder}\\' created!\")\n",
    "\n",
    "        np.save(f'{folder}/train_features.npy', train_features)\n",
    "        np.save(f'{folder}/test_features.npy', test_features)\n",
    "        np.save(f'{folder}/train_labels.npy', train_labels)\n",
    "        np.save(f'{folder}/test_labels.npy', test_labels)\n",
    "\n",
    "        print(emojize(f\"Features and Labels saved! :floppy_disk:\") if emojizer else 'Features and Labels saved!')\n",
    "else:\n",
    "    print(emojize(f\"Loading Features and Labels from \\'{folder}\\'...:open_file_folder:\") if emojizer else f\"\\nLoading features and labels from \\'{folder}\\'...\")\n",
    "\n",
    "    train_features = np.load(f'{folder}/train_features.npy')\n",
    "    test_features = np.load(f'{folder}/test_features.npy')\n",
    "    train_labels = np.load(f'{folder}/train_labels.npy')\n",
    "    test_labels = np.load(f'{folder}/test_labels.npy')\n",
    "    \n",
    "    print(emojize(f\"Features and Labels loaded!:OK_hand_medium-light_skin_tone:\") if emojizer else 'Features and Labels loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "cfbd4ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...💪🏼\n",
      "📜 Epoch 1/20:\t🫥 Training Loss = 1.0757   🎯 Test Accuracy = 60.40%\n",
      "📜 Epoch 2/20:\t🫥 Training Loss = 1.0004   🎯 Test Accuracy = 61.95%\n",
      "📜 Epoch 3/20:\t🫥 Training Loss = 0.9655   🎯 Test Accuracy = 62.80%\n",
      "📜 Epoch 4/20:\t🫥 Training Loss = 0.9377   🎯 Test Accuracy = 63.70%\n",
      "📜 Epoch 5/20:\t🫥 Training Loss = 0.9017   🎯 Test Accuracy = 64.40%\n",
      "📜 Epoch 6/20:\t🫥 Training Loss = 0.8737   🎯 Test Accuracy = 65.45%\n",
      "📜 Epoch 7/20:\t🫥 Training Loss = 0.8416   🎯 Test Accuracy = 65.15%\n",
      "📜 Epoch 8/20:\t🫥 Training Loss = 0.8069   🎯 Test Accuracy = 65.20%\n",
      "📜 Epoch 9/20:\t🫥 Training Loss = 0.7749   🎯 Test Accuracy = 65.65%\n",
      "📜 Epoch 10/20:\t🫥 Training Loss = 0.7397   🎯 Test Accuracy = 65.80%\n",
      "📜 Epoch 11/20:\t🫥 Training Loss = 0.7071   🎯 Test Accuracy = 66.00%\n",
      "📜 Epoch 12/20:\t🫥 Training Loss = 0.6701   🎯 Test Accuracy = 66.75%\n",
      "📜 Epoch 13/20:\t🫥 Training Loss = 0.6322   🎯 Test Accuracy = 66.50%\n",
      "📜 Epoch 14/20:\t🫥 Training Loss = 0.5930   🎯 Test Accuracy = 66.65%\n",
      "📜 Epoch 15/20:\t🫥 Training Loss = 0.5613   🎯 Test Accuracy = 66.75%\n",
      "📜 Epoch 16/20:\t🫥 Training Loss = 0.5206   🎯 Test Accuracy = 66.45%\n",
      "📜 Epoch 17/20:\t🫥 Training Loss = 0.4996   🎯 Test Accuracy = 66.65%\n",
      "📜 Epoch 18/20:\t🫥 Training Loss = 0.4617   🎯 Test Accuracy = 66.50%\n",
      "📜 Epoch 19/20:\t🫥 Training Loss = 0.4305   🎯 Test Accuracy = 66.45%\n",
      "📜 Epoch 20/20:\t🫥 Training Loss = 0.4056   🎯 Test Accuracy = 66.15%\n",
      "\n",
      "Training completed!👌🏼\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for the shallow classifier (MLP) instantiated\n",
    "hidden_size = 512\n",
    "epochs = 20\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Weighted Loss for handling class imbalance\n",
    "weighted = False\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(768, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Instantiating Model, Optimizer and Loss\n",
    "shallowcls = MLP(hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(shallowcls.parameters(), lr = learning_rate)\n",
    "\n",
    "if weighted:\n",
    "    weights = torch.tensor([3.43, 2.98, 12.27, 7.41, 8.26, 27.97]).to(device)\n",
    "else:\n",
    "    weights = None\n",
    "criterion = nn.CrossEntropyLoss(weight = weights)\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "print(emojize(\"Starting Training...:flexed_biceps_medium-light_skin_tone:\") if emojizer else \"Starting Training...\")\n",
    "for epoch in range(epochs):\n",
    "    shallowcls.train()\n",
    "    for i in range(0, len(train_features), batch_size):\n",
    "        X = torch.tensor(train_features[i : i + batch_size], dtype = torch.float).to(device)\n",
    "        Y = torch.tensor(train_labels[i : i + batch_size], dtype = torch.long).to(device)\n",
    "\n",
    "        outputs = shallowcls(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none = True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Time for Testing\n",
    "    total, correct = 0, 0\n",
    "    shallowcls.eval()\n",
    "    for i in range(0, len(test_features), batch_size):\n",
    "        Xval = torch.tensor(test_features[i : i + batch_size], dtype = torch.float).to(device)\n",
    "        Yval = torch.tensor(test_labels[i : i + batch_size], dtype = torch.long).to(device)\n",
    "\n",
    "        outputs = shallowcls(Xval)   \n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        total += Yval.size(0)\n",
    "        correct += (pred == Yval).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "\n",
    "    print(emojize(f\":scroll: Epoch {epoch+1}/{epochs}:\\t:dotted_line_face: Training Loss = {loss.item():.4f}   :bullseye: Test Accuracy = {test_accuracy:.2f}%\") if emojizer else f\"Epoch {epoch+1}/{epochs}:\\tTraining Loss = {loss.item():.4f}   Test Accuracy = {test_accuracy:.2f}%\")\n",
    "\n",
    "print(emojize(\"\\nTraining completed!:OK_hand_medium-light_skin_tone:\") if emojizer else \"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "61e5ddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a Logistic Regression model...💪🏼\n",
      "Model trained! Making predictions on new data...🧐\n",
      "\n",
      "Prediction completed!\n",
      "🎯 Test Accuracy = 65.25%\n",
      "🧨 F1 Score = 0.6436\n",
      "🪡 Precision = 0.6416\n",
      "📣 Recall = 0.6525\n"
     ]
    }
   ],
   "source": [
    "print(emojize(\"Training a Logistic Regression model...:flexed_biceps_medium-light_skin_tone:\") if emojizer else \"Training a Logistic Regression model...\")\n",
    "logreg = LogisticRegression(max_iter = 1500).fit(train_features, train_labels)\n",
    "\n",
    "print(emojize(\"Model trained! Making predictions on new data...:face_with_monocle:\\n\") if emojizer else \"Model trained! Making predictions on new data...\\n\")\n",
    "pred = logreg.predict(test_features)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, pred)*100\n",
    "f1 = f1_score(test_labels, pred, average = 'weighted')\n",
    "precision = precision_score(test_labels, pred, average = 'weighted')\n",
    "recall = recall_score(test_labels, pred, average = 'weighted')\n",
    "\n",
    "print(emojize(f\"Prediction completed!\\n:bullseye: Test Accuracy = {accuracy:.2f}%\\n\" +\n",
    "                    f\":firecracker: F1 Score = {f1:.4f}\\n\" +\n",
    "                    f\":sewing_needle: Precision = {precision:.4f}\\n\" +\n",
    "                    f\":megaphone: Recall = {recall:.4f}\") if emojizer else f\"Prediction completed! Test Accuracy = {accuracy:.2f}%\\nF1 Score = {f1:.4f}\\nPrecision = {precision:.4f}\\nRecall = {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d86cda",
   "metadata": {},
   "source": [
    "- Using the *dair-ai/emotion* dataset, the multi-class classifier, with or without any tweaks on the loss weights to address the class imbalance problem, struggles to reach 67% Test Accuracy (while a fine-tuned DistilBERT is capable to go up to 94% accuracy).\n",
    "- Using the *ag_news* dataset, instead, the classifier makes no effort to provide results with 92% Test Accuracy.\n",
    "\n",
    "Furthermore, results obtained by training a simple MLP and an even simpler Logistic Regression are basically the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e103ec",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Training a Question Answering Model\n",
    "\n",
    "Peruse the [multiple choice question answering datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:multiple-choice&sort=downloads). Chose a *moderately* sized one and train a model to answer contextualized multiple-choice questions. You *might* be able to avoid fine-tuning by training a simple model to *rank* the multiple choices (see margin ranking loss in Pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "e6e10735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda 🦅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 405k/405k [00:00<00:00, 786kB/s]\n",
      "Downloading data: 100%|██████████| 6.97M/6.97M [00:02<00:00, 2.65MB/s]\n",
      "Downloading data: 100%|██████████| 407k/407k [00:00<00:00, 1.39MB/s]\n",
      "Generating test split: 100%|██████████| 1436/1436 [00:00<00:00, 20448.77 examples/s]\n",
      "Generating train split: 100%|██████████| 25421/25421 [00:00<00:00, 515151.11 examples/s]\n",
      "Generating validation split: 100%|██████████| 1436/1436 [00:00<00:00, 287220.82 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'race-middle' loaded!👌🏼\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DistilBertTokenizer and DistilBertModel instantiated!👌🏼\n"
     ]
    }
   ],
   "source": [
    "# Imports and dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Generalizing code for handling different datasets and models\n",
    "pretrained_model = 'distilbert-base-uncased'\n",
    "\n",
    "#Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(emojize(f\"Device: {device} :eagle:\") if torch.cuda.is_available() else emojize(f\"Device: {device} :snail:\")) if emojizer else print(f\"Device: {device}\")\n",
    "\n",
    "# Loading the selected dataset (race-middle), Model and Tokenizer\n",
    "data = load_dataset('race', 'middle')\n",
    "print(emojize(f\"Dataset \\'race-middle\\' loaded!:OK_hand_medium-light_skin_tone:\\n\") if emojizer else f\"Dataset \\'{dataset_name}\\' loaded!\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model).to(device)\n",
    "\n",
    "print(emojize(f\"\\n{tokenizer.__class__.__name__} and {model.__class__.__name__} instantiated!:OK_hand_medium-light_skin_tone:\") if emojizer else f\"\\n{tokenizer.__class__.__name__} and {model.__class__.__name__} instantiated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "35ede8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['example_id', 'article', 'answer', 'question', 'options'],\n",
       "    num_rows: 25421\n",
       "})"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']\n",
    "\n",
    "# input1 = article\n",
    "# input2 = question\n",
    "\n",
    "# output1 = answer predicted from article\n",
    "# output2 = answer predicted from question\n",
    "\n",
    "# criterion = nn.MarginRankingLoss()\n",
    "# loss = criterion(output1, output2, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce054686",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Training a Retrieval Model\n",
    "\n",
    "The Hugging Face dataset repository contains a large number of [\"text retrieval\" problems](https://huggingface.co/datasets?task_categories=task_categories:text-retrieval&p=1&sort=downloads). These tasks generally require that the model measure *similarity* between text in some metric space -- naively, just a cosine similarity between [CLS] tokens can get you pretty far. Find an interesting retrieval problem and train a model (starting from a pre-trained LLM of course) to solve it.\n",
    "\n",
    "**Tip**: Sometimes identifying the *retrieval* problems in these datasets can be half the challenge. [This dataset](https://huggingface.co/datasets/BeIR/scifact) might be a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f22f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
