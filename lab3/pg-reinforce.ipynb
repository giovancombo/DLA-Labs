{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a18c83b-448a-4679-8f19-0fa8d2c95508",
   "metadata": {},
   "source": [
    "# Getting up to speed with DRL\n",
    "\n",
    "In this notebook I provide a simple example of implementing a policy gradient Deep Reinforcement Learning algorithm to solve a control problem with continuous state space and discrete action space -- the venerable [CartPole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/). You should study the implementation in this notebook in preparation for the laboratory next Wednesday.\n",
    "\n",
    "This notebook should run in an environment with at least the following packages installed (the gpu version of PyTorch is not mandatory):\n",
    "\n",
    "     conda create -n DRL -c conda-forge gymnasium pytorch-gpu matplotlib pygame jupyterlab\n",
    "     \n",
    "Some background reading to get you started:\n",
    "\n",
    "1. We will be using the [Gymnasium](https://gymnasium.farama.org/) framework for all of our experiments. This framework provides a consistent interface to a broad range of reinforcement learning environments (including CartPole). You should familiarize yourself with how it works, how environments are specified, how to instantiate them, and how to interact with them.\n",
    "\n",
    "2. [This excellent blog post](http://karpathy.github.io/2016/05/31/rl/) is a great introduction to policy gradients, where they come from and how they work. Give it a read and I am sure it will help understand better what is going on in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a976f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Plus one non standard one -- we need this to sample from policies.\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218112b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an environment, observation, and policy, sample from pi(a | obs). Returns the\n",
    "# selected action and the log probability of that action (needed for policy gradient).\n",
    "def select_action(env, obs, policy):\n",
    "    dist = Categorical(policy(obs))\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "    return (action.item(), log_prob.reshape(1))\n",
    "\n",
    "# Utility to compute the discounted total reward. Torch doesn't like flipped arrays, so we need to\n",
    "# .copy() the final numpy array. There's probably a better way to do this.\n",
    "def compute_returns(rewards, gamma):\n",
    "    return np.flip(np.cumsum([gamma**(i+1)*r for (i, r) in enumerate(rewards)][::-1]), 0).copy()\n",
    "\n",
    "# Given an environment and a policy, run it up to the maximum number of steps.\n",
    "def run_episode(env, policy, maxlen=500, device='cpu'):\n",
    "    # Collect just about everything.\n",
    "    observations = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    \n",
    "    # Reset the environment and start the episode.\n",
    "    (obs, info) = env.reset()\n",
    "    for i in range(maxlen):\n",
    "        # Get the current observation, run the policy and select an action.\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "        (action, log_prob) = select_action(env, obs, policy)\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        # Advance the episode by executing the selected action.\n",
    "        (obs, reward, term, trunc, info) = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if term or trunc:\n",
    "            break\n",
    "    return (observations, actions, torch.cat(log_probs), rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dea044-14e1-44f1-a851-ef6e19452692",
   "metadata": {},
   "source": [
    "## The Policy network\n",
    "\n",
    "Here I provide a simple policy network which should work with any environment with continuous observations and discrete action spaces. Note how it uses the *specification* of the environment to configure its input and output spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6adbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple, but generic, policy network with one hidden layer.\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, env.action_space.n)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = F.relu(self.fc2(s))\n",
    "        s = F.softmax(self.fc3(s), dim=-1)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e2af5-62fd-422e-8fb7-06295c49ddc7",
   "metadata": {},
   "source": [
    "## The `REINFORCE` Algorithm\n",
    "\n",
    "This is a very simple implementation of the most basic policy gradient DRL algorithm: `REINFORCE`. It is a very direct implementation of the policy gradient update (although I use Adam instead of SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1fb727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A direct, inefficient, and probably buggy of the REINFORCE policy gradient algorithm.\n",
    "def reinforce(policy, env, env_render=None, gamma=0.99, lr=1e-4, num_episodes=10, device='cpu'):\n",
    "    # The only non-vanilla part: we use Adam instead of SGD.\n",
    "    opt = torch.optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    # Track episode rewards in a list.\n",
    "    running_rewards = [0.0]\n",
    "    \n",
    "    # The main training loop.\n",
    "    policy.train()\n",
    "    for episode in range(num_episodes):\n",
    "        # Run an episode of the environment, collect everything needed for policy update.\n",
    "        (observations, actions, log_probs, rewards) = run_episode(env, policy, device=device)\n",
    "        \n",
    "        # Compute the discounted reward for every step of the episode. \n",
    "        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)\n",
    "        \n",
    "        # Keep a running average of total discounted rewards for the whole episode.\n",
    "        running_rewards.append(0.005 * returns[0].item() + 0.995 * running_rewards[-1])\n",
    "        \n",
    "        # Standardize returns.\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-6)\n",
    "        \n",
    "        # Make an optimization step\n",
    "        log_probs = log_probs.to(device)\n",
    "        returns = returns.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = (-log_probs * returns).sum()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # Render an episode after every 100 policy updates.\n",
    "        if not episode % 500:\n",
    "            policy.eval()\n",
    "            (obs, _, _, _) = run_episode(env_render, policy, device=device)\n",
    "            policy.train()\n",
    "            print(f'Running reward: {running_rewards[-1]}')\n",
    "    \n",
    "    # Return the running rewards.\n",
    "    policy.eval()\n",
    "    return running_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7086f041-a315-4f4e-9db6-01cb446fffa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Running reward: 0.07352840423583984\n",
      "Running reward: 24.216458833174674\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m policy \u001b[38;5;241m=\u001b[39m PolicyNet(env)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Train the agent.\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(reinforce(policy, env, env_render, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Close up everything\u001b[39;00m\n\u001b[0;32m     23\u001b[0m env_render\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\giova\\__UNI\\Deep Learning Applications\\DLA-Labs\\lab3\\reinforce.py:82\u001b[0m, in \u001b[0;36mreinforce\u001b[1;34m(policy, env, env_render, gamma, lr, num_episodes, device)\u001b[0m\n\u001b[0;32m     79\u001b[0m policy\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# Run an episode of the environment, collect everything needed for policy update.\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m     (observations, actions, log_probs, rewards) \u001b[38;5;241m=\u001b[39m \u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# Compute the discounted reward for every step of the episode. \u001b[39;00m\n\u001b[0;32m     85\u001b[0m     returns \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(compute_returns(rewards, gamma), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mc:\\Users\\giova\\__UNI\\Deep Learning Applications\\DLA-Labs\\lab3\\reinforce.py:41\u001b[0m, in \u001b[0;36mrun_episode\u001b[1;34m(env, policy, maxlen, device)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(maxlen):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Get the current observation, run the policy and select an action.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(obs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 41\u001b[0m     (action, log_prob) \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     observations\u001b[38;5;241m.\u001b[39mappend(obs)\n\u001b[0;32m     43\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(action)\n",
      "File \u001b[1;32mc:\\Users\\giova\\__UNI\\Deep Learning Applications\\DLA-Labs\\lab3\\reinforce.py:18\u001b[0m, in \u001b[0;36mselect_action\u001b[1;34m(env, obs, policy)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(env, obs, policy):\n\u001b[1;32m---> 18\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     19\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     20\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\DRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\giova\\__UNI\\Deep Learning Applications\\DLA-Labs\\lab3\\reinforce.py:64\u001b[0m, in \u001b[0;36mPolicyNet.forward\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, s):\n\u001b[1;32m---> 64\u001b[0m     s \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     65\u001b[0m     s \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(s))\n\u001b[0;32m     66\u001b[0m     s \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(s), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\DRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\DRL\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gymnasium\n",
    "import torch\n",
    "\n",
    "from reinforce import *\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# In the new version of Gymnasium you need different environments for rendering and no rendering.\n",
    "# Here we instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "# down), and another that does not animate.\n",
    "env = gymnasium.make('CartPole-v1')\n",
    "env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# Make a policy network.\n",
    "policy = PolicyNet(env)\n",
    "\n",
    "# Train the agent.\n",
    "plt.plot(reinforce(policy, env, env_render, num_episodes=2000))\n",
    "\n",
    "# Close up everything\n",
    "env_render.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40868e-7f32-4409-9f07-1fe781de45da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# And run the final agent for a few episodes.\n",
    "env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "for _ in range(10):\n",
    "    run_episode(env_render, policy)\n",
    "env_render.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bc02a",
   "metadata": {},
   "source": [
    "What about running the model on the Navigation Problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the new version of Gymnasium you need different environments for rendering and no rendering.\n",
    "# Here we instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "# down), and another that does not animate.\n",
    "\n",
    "env = gymnasium.make('gym_navigation:NavigationGoal-v0', render_mode=None, track_id=1)\n",
    "env_render = gymnasium.make('gym_navigation:NavigationGoal-v0', render_mode='human')\n",
    "\n",
    "# Make a policy network.\n",
    "policy = PolicyNet(env).to(device)\n",
    "\n",
    "# Train the agent.\n",
    "running  = reinforce(policy, env, env_render, device=device, lr=1e-4, num_episodes=1000)\n",
    "running += reinforce(policy, env, env_render, device=device, lr=1e-5, num_episodes=1000)\n",
    "plt.plot(running)\n",
    "\n",
    "# Close up everything\n",
    "env_render.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc78ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And run the final agent for a few episodes.\n",
    "env_render = gymnasium.make('gym_navigation:NavigationGoal-v0', render_mode='human')\n",
    "for _ in range(10):\n",
    "    (_, _, _, r) = run_episode(env_render, policy, device=device)\n",
    "    returns = compute_returns(r, 0.99)\n",
    "    print(returns[0])\n",
    "env_render.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202cea9-aef2-46da-ace9-ddf25bca77be",
   "metadata": {},
   "source": [
    "## For your consideration\n",
    "\n",
    "There are many things that can be improved in this example. Some things you can think about:\n",
    "\n",
    "1. **Replay**. In the current implementation we execute an episode, and then immediately run an optimization step on all of the steps of the episode. Not only are we using *correlated* samples from a single episode, we are decidedly *not* taking advantage of parallelism via batch gradient descent. Note that `REINFORCE` does **not** require entire trajectories, all we need are the discounted rewards and log probabilities for *individual transitions*.\n",
    "\n",
    "2. **Exploration**. The model is probably overfitting (or perhaps remaining too *plastic*, which can explain the unstable convergence). Our policy is *always* stochastic in that we sample from the output distribution. It would be interesting to add a temperature parameter to the policy so that we can control this behavior, or even implement a deterministic policy sampler that always selects the action with max probability to evaluate the quality of the learned policy network.\n",
    "\n",
    "3. **Discount Factor**: The discount factor (default $\\gamma = 0.99$) is an important hyperparameter that has an effect on the stability of training. Try different values for $\\gamma$ and see how it affects training. Can you think of other ways to stabilize training?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
