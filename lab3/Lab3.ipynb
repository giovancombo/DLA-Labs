{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdf4d12",
   "metadata": {},
   "source": [
    "***Deep Learning Applications 2023** course, held by Professor **Andrew David Bagdanov** - University of Florence, Italy*\n",
    "\n",
    "*Notebook and code created by **Giovanni Colombo** - Mat. 7092745*\n",
    "\n",
    "Check the dedicated [Repository on GitHub](https://github.com/giovancombo/DLA-Labs/tree/main/lab3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #3 - DRL\n",
    "\n",
    "In this laboratory session we will hack one of your colleague's (Francesco Fantechi, from Ingegneria Informatica) implementation of a navigation environment for Deep Reinforcement Learning. The setup is fairly simple:\n",
    "\n",
    "+ A simple 2D environment with a (limited) number of *obstacles* and a single *goal* is presented to the agent, which must learn how to navigate to the goal without hitting any obstacles.\n",
    "+ The agent *observes* the environment via a set of 16 rays cast uniformly which return the distance to the first obstacle encountered, as well as the distance and direction to the goal.\n",
    "+ The agent has three possible actions: `ROTATE LEFT`, `ROTATE RIGHT`, or `MOVE FORWARD`.\n",
    "\n",
    "For each step of an episode, the agent receives a reward of:\n",
    "+ -100 if hitting an obstacle (episode ends).\n",
    "+ -100 if one hundred steps are reached without hitting the goal.\n",
    "+ +100 if hitting the goal (episode ends)\n",
    "+ A small *positive* reward if the distance to the goal is *reduced*.\n",
    "+ A small *negative* reward if the distance to the goal is *increased*.\n",
    "\n",
    "In the file `main.py` you will find an implementation of **Deep Q-Learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5adad7-759b-4000-925b-701f41fe6e97",
   "metadata": {},
   "source": [
    "## Exercise 1: Testing the Environment\n",
    "\n",
    "The first thing to do is verify that the environment is working in your Anaconda virtual environment. I had a weird problem with Tensorboard and had to downgrade it using:\n",
    "\n",
    "    conda install -c conda-forge tensorboard=2.11.2\n",
    "    \n",
    "In any case, you should be able to run:\n",
    "\n",
    "    python main.py\n",
    "    \n",
    "from the repository root and it will run episodes using a pretrained agent. To train an agent from scratch, you must modify `main.py` setting `TRAIN = True` at the top. Then running `main.py` again will train an agent for 2000 episodes of training. To run the trained agent you will again have to modify `main.py` on line 225 to load the last saved checkpoint:\n",
    "\n",
    "    PATH = './checkpoints/last.pth'\n",
    "    \n",
    "and then run the script again (after setting `TRAIN = False` !).\n",
    "\n",
    "Make sure you can at run the demo agent and train one from scratch. If you don't have a GPU you can set the number of training episodes to a smaller number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c95cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set TRAIN = True for training and then False for testing\n",
    "!python Navigation_Goal_Deep_Q_Learning/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac9092",
   "metadata": {},
   "source": [
    "Well, I guess I did it. The main script works. I let it train for 1000 episodes.\n",
    "\n",
    "Qualitatively, it's possible to see that the agents looks like it has not learned so well to find the goal. Many times, the agent hits the walls or obstables without even trying to change direction, even just after the spawn. Some other times, the agent finds its way to the goal, until it stops right in front of it and changes direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed9279-3a2f-4fcd-bf29-b105b2da8433",
   "metadata": {},
   "source": [
    "## Exercise 2: Stabilizing Q-Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feafbec",
   "metadata": {},
   "source": [
    "## Exercise 3: Going Deeper\n",
    "\n",
    "As usual, pick **AT LEAST ONE** of the following exercises to complete.\n",
    "\n",
    "### Exercise 3.1: Solving the environment with `REINFORCE`\n",
    "\n",
    "Use my (or even better, improve on my) implementation of `REINFORCE` to solve the environment.\n",
    "\n",
    "**Note**: There is a *design flaw* in the environment implementation that will lead to strange (by explainable) behavior in agents trained with `REINFORCE`. See if you can figure it out and fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12742de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward: -0.7287060056686402\n",
      "Running reward: -1.1493442062921524\n",
      "Running reward: -1.5693263594672346\n",
      "Running reward: -1.9958526295864023\n",
      "Running reward: -2.4143626318803646\n",
      "Running reward: -2.8590494323073887\n",
      "Running reward: -3.307590755885598\n",
      "Running reward: -3.523532870312957\n",
      "Running reward: -3.976472876554654\n",
      "Running reward: -4.341953289881841\n",
      "Running reward: -4.709449150873838\n",
      "Running reward: -5.131456409514\n",
      "Running reward: -5.3675004252264396\n",
      "Running reward: -3.3445638325241354\n",
      "Running reward: -3.7173847755685463\n",
      "Running reward: -4.085489708074981\n",
      "Running reward: -4.524758380750426\n",
      "Running reward: -4.925999792704096\n",
      "Running reward: -5.3047203567898915\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m policy \u001b[38;5;241m=\u001b[39m PolicyNet(env)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train the agent.\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m running  \u001b[38;5;241m=\u001b[39m reinforce(policy, env, env_render, device\u001b[38;5;241m=\u001b[39mdevice, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     18\u001b[0m running \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reinforce(policy, env, env_render, device\u001b[38;5;241m=\u001b[39mdevice, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     19\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(running)\n",
      "File \u001b[1;32mc:\\Users\\giova\\__UNI\\Deep Learning Applications\\DLA-Labs\\lab3\\reinforce.py:85\u001b[0m, in \u001b[0;36mreinforce\u001b[1;34m(policy, env, env_render, gamma, lr, num_episodes, device)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m:\n\u001b[0;32m     84\u001b[0m     policy\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 85\u001b[0m     (obs, _, _, _) \u001b[38;5;241m=\u001b[39m \u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_render\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     policy\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_rewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\giova\\__UNI\\Deep Learning Applications\\DLA-Labs\\lab3\\reinforce.py:38\u001b[0m, in \u001b[0;36mrun_episode\u001b[1;34m(env, policy, maxlen, device)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(maxlen):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Get the current observation, run the policy and select an action.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(obs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 38\u001b[0m     (action, log_prob) \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     observations\u001b[38;5;241m.\u001b[39mappend(obs)\n\u001b[0;32m     40\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(action)\n",
      "File \u001b[1;32mc:\\Users\\giova\\__UNI\\Deep Learning Applications\\DLA-Labs\\lab3\\reinforce.py:15\u001b[0m, in \u001b[0;36mselect_action\u001b[1;34m(env, obs, policy)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(env, obs, policy):\n\u001b[1;32m---> 15\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     16\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     17\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\DRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\giova\\__UNI\\Deep Learning Applications\\DLA-Labs\\lab3\\policy.py:15\u001b[0m, in \u001b[0;36mPolicyNet.forward\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, s):\n\u001b[0;32m     14\u001b[0m     s \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(s))\n\u001b[1;32m---> 15\u001b[0m     s \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(s)) \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m     16\u001b[0m     s \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(s), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\DRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\DRL\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gymnasium\n",
    "\n",
    "from reinforce import *\n",
    "from policy import *\n",
    "\n",
    "# In the new version of Gymnasium you need different environments for rendering and no rendering.\n",
    "# Here we instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "# down), and another that does not animate.\n",
    "env = gymnasium.make('gym_navigation:NavigationGoal-v0', render_mode=None, track_id=1)\n",
    "env_render = gymnasium.make('gym_navigation:NavigationGoal-v0', render_mode='human')\n",
    "\n",
    "# Make a policy network.\n",
    "policy = PolicyNet(env).to(device)\n",
    "\n",
    "# Train the agent.\n",
    "running  = reinforce(policy, env, env_render, device=device, lr=1e-4, num_episodes=100)\n",
    "running += reinforce(policy, env, env_render, device=device, lr=1e-5, num_episodes=100)\n",
    "plt.plot(running)\n",
    "\n",
    "# Close up everything\n",
    "env_render.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad32251",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Solving another environment\n",
    "\n",
    "The [Gymnasium](https://gymnasium.farama.org/) framework has a ton of interesting and fun environments to work with. Pick one and try to solve it using any technique you like. The [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) environment is a fun one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b6c801",
   "metadata": {},
   "source": [
    "Ok, so, let's build a new LunarLander environment, with render mode so that we can graphically see progress of our lander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8edaed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from reinforce import *\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "env_name = \"LunarLander-v2\"\n",
    "episodes = 20\n",
    "\n",
    "env = gym.make(env_name, render_mode='human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b45c9d",
   "metadata": {},
   "source": [
    "To set things, I start implementing a lander that takes totally random actions at each time tick. The total reward will be very bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75049b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1,  score: -62.1084151496713\n",
      "Episode 2,  score: -194.84647116135045\n",
      "Episode 3,  score: -76.65653307325597\n",
      "Episode 4,  score: -68.78041603709626\n",
      "Episode 5,  score: -306.6852013935006\n",
      "Episode 6,  score: -114.56088488358084\n",
      "Episode 7,  score: -2.5777693246189415\n",
      "Episode 8,  score: -123.80920637421295\n",
      "Episode 9,  score: -167.7293375767921\n",
      "Episode 10,  score: -523.5022029599596\n",
      "Episode 11,  score: -260.32935786394\n",
      "Episode 12,  score: -137.38976611142658\n",
      "Episode 13,  score: -99.43182030164184\n",
      "Episode 14,  score: -163.50386089447278\n",
      "Episode 15,  score: -97.87862314242409\n",
      "Episode 16,  score: -83.57409230094022\n",
      "Episode 17,  score: -329.40643678070023\n",
      "Episode 18,  score: -146.48907410685518\n",
      "Episode 19,  score: -103.10951506921988\n",
      "Episode 20,  score: -148.4399761696688\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    score = 0\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()              # Scelta random tra i valori [0,1,2,3] delle azioni\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode {},  score: {}'.format(episode, score))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9997e40f",
   "metadata": {},
   "source": [
    "...As expected. Now I will try to use the REINFORCE algorithm already implemented in Francesco's work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54251a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the new version of Gymnasium you need different environments for rendering and no rendering.\n",
    "# Here we instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "# down), and another that does not animate.\n",
    "env = gymnasium.make(env_name, render_mode='human')\n",
    "\n",
    "# Make a policy network.\n",
    "policy = PolicyNet(env).to(device)\n",
    "\n",
    "# Train the agent.\n",
    "running  = reinforce(policy, env, env_render, device=device, lr=1e-4, num_episodes=1000)\n",
    "running += reinforce(policy, env, env_render, device=device, lr=1e-5, num_episodes=1000)\n",
    "plt.plot(running)\n",
    "\n",
    "# Close up everything\n",
    "env_render.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf1447-d222-4b24-a357-5b7f9824390c",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Advanced techniques \n",
    "\n",
    "The `REINFORCE` and Q-Learning approaches, though venerable, are not even close to the state-of-the-art. Try using an off-the-shelf implementation of [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347) to solve one (or more) of these environments. Compare your results with those of Q-Learning and/or REINFORCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b8a52-d97b-4c32-bfcb-9069e7527ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
