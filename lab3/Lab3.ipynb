{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdf4d12",
   "metadata": {},
   "source": [
    "***Deep Learning Applications 2023** course, held by Professor **Andrew David Bagdanov** - University of Florence, Italy*\n",
    "\n",
    "*Notebook and code created by **Giovanni Colombo** - Mat. 7092745*\n",
    "\n",
    "Check the dedicated [Repository on GitHub](https://github.com/giovancombo/DLA-Labs/tree/main/lab3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #3 - DRL\n",
    "\n",
    "In this laboratory session we will hack one of your colleague's (Francesco Fantechi, from Ingegneria Informatica) implementation of a navigation environment for Deep Reinforcement Learning. The setup is fairly simple:\n",
    "\n",
    "+ A simple 2D environment with a (limited) number of *obstacles* and a single *goal* is presented to the agent, which must learn how to navigate to the goal without hitting any obstacles.\n",
    "+ The agent *observes* the environment via a set of 16 rays cast uniformly which return the distance to the first obstacle encountered, as well as the distance and direction to the goal.\n",
    "+ The agent has three possible actions: `ROTATE LEFT`, `ROTATE RIGHT`, or `MOVE FORWARD`.\n",
    "\n",
    "For each step of an episode, the agent receives a reward of:\n",
    "+ -100 if hitting an obstacle (episode ends).\n",
    "+ -100 if one hundred steps are reached without hitting the goal.\n",
    "+ +500 if hitting the goal (episode ends)\n",
    "+ A small *positive* reward if the distance to the goal is *reduced*.\n",
    "+ A small *negative* reward if the distance to the goal is *increased*.\n",
    "\n",
    "In the file `main.py` you will find an implementation of **Deep Q-Learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5adad7-759b-4000-925b-701f41fe6e97",
   "metadata": {},
   "source": [
    "## Exercise 1: Testing the Environment\n",
    "\n",
    "The first thing to do is verify that the environment is working in your Anaconda virtual environment. I had a weird problem with Tensorboard and had to downgrade it using:\n",
    "\n",
    "    conda install -c conda-forge tensorboard=2.11.2\n",
    "    \n",
    "In any case, you should be able to run:\n",
    "\n",
    "    python main.py\n",
    "    \n",
    "from the repository root and it will run episodes using a pretrained agent. To train an agent from scratch, you must modify `main.py` setting `TRAIN = True` at the top. Then running `main.py` again will train an agent for 2000 episodes of training. To run the trained agent you will again have to modify `main.py` on line 225 to load the last saved checkpoint:\n",
    "\n",
    "    PATH = './checkpoints/last.pth'\n",
    "    \n",
    "and then run the script again (after setting `TRAIN = False` !).\n",
    "\n",
    "Make sure you can at run the demo agent and train one from scratch. If you don't have a GPU you can set the number of training episodes to a smaller number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c95cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set TRAIN = True for training and then False for testing\n",
    "!python main_dqn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac9092",
   "metadata": {},
   "source": [
    "Well, I guess I did it. The main script works. I let it train for 1000 episodes.\n",
    "\n",
    "Qualitatively, it's possible to see that the agents looks like it has not learned so well to find the goal. Many times, the agent hits the walls or obstables without even trying to change direction, even just after the spawn. Some other times, the agent finds its way to the goal, until it stops right in front of it and changes direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed9279-3a2f-4fcd-bf29-b105b2da8433",
   "metadata": {},
   "source": [
    "## Exercise 2: Stabilizing Q-Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d92f64f",
   "metadata": {},
   "source": [
    "Ok, so, now that I verified that the environment works, it's now time to stabilize Q-Learning via tweaking the hyperparameters and the architecture. This will just be an ablation study, in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d947b300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1feafbec",
   "metadata": {},
   "source": [
    "## Exercise 3: Going Deeper\n",
    "\n",
    "As usual, pick **AT LEAST ONE** of the following exercises to complete.\n",
    "\n",
    "### Exercise 3.1: Solving the environment with `REINFORCE`\n",
    "\n",
    "Use my (or even better, improve on my) implementation of `REINFORCE` to solve the environment.\n",
    "\n",
    "**Note**: There is a *design flaw* in the environment implementation that will lead to strange (by explainable) behavior in agents trained with `REINFORCE`. See if you can figure it out and fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a28155",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Coming up with a good reward structure is the main challenge of reinforcement learning. Your problem could be perfectly within the capabilities of the model, but if the reward structure is not set up correctly it may never learn.\n",
    "\n",
    "The goal of the rewards is to encourage specific behavior. In our case we want to guide the agent towards the goal cell, defined by -1.\n",
    "\n",
    "Similar to the layers and neurons in the network, and epsilon and its associated values, there can be many right (and many wrong) ways to define the reward structure.\n",
    "\n",
    "The two main types of reward structures:\n",
    "\n",
    "Sparse: When rewards are only given in a handful of states.\n",
    "Dense: When rewards are common throughout the state-space.\n",
    "With sparse rewards the agent has very little feedback to lead it. This would be like simply giving a set penalty for each step, and if the agent reaches the goal you provide one large reward.\n",
    "\n",
    "The agent can certainly learn to reach the goal, but depending on the size of the state-space it can take much longer and may get stuck on a suboptimal strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ad8f2",
   "metadata": {},
   "source": [
    "What would be a good way to reward the agent to move towards the goal more incrementally?\n",
    "\n",
    "The first way is to return the negative of the Manhattan distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1ac1f8",
   "metadata": {},
   "source": [
    "There are many things that can be improved in this implementation. Some things you can think about:\n",
    "\n",
    "1. **Replay**. In the current implementation we execute an episode, and then immediately run an optimization step on all of the steps of the episode. Not only are we using *correlated* samples from a single episode, we are decidedly *not* taking advantage of parallelism via batch gradient descent. Note that `REINFORCE` does **not** require entire trajectories, all we need are the discounted rewards and log probabilities for *individual transitions*.\n",
    "NB: In realtà in REINFORCE non è possibile utilizzare un Replay Buffer! Perché è un metodo on-policy, quindi non può supportare cambio di policy transition-wise.\n",
    "\n",
    "2. **Exploration**. The model is probably overfitting (or perhaps remaining too *plastic*, which can explain the unstable convergence). Our policy is *always* stochastic in that we sample from the output distribution. It would be interesting to add a temperature parameter to the policy so that we can control this behavior, or even implement a deterministic policy sampler that always selects the action with max probability to evaluate the quality of the learned policy network.\n",
    "\n",
    "3. **Discount Factor**: The discount factor (default $\\gamma = 0.99$) is an important hyperparameter that has an effect on the stability of training. Try different values for $\\gamma$ and see how it affects training. Can you think of other ways to stabilize training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be39017d",
   "metadata": {},
   "source": [
    "4. **Baseline Function**: funzione indipendente dall'azione stabilizza molto l'algoritmo REINFORCE, che di norma invece è instabile.\n",
    "La funzione è arbitraria, ma di norma viene scelta una *stima della state-value function V*, un po' come poi succederà in PPO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd6a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "from models import PolicyNet\n",
    "from reinforce import reinforce\n",
    "from combo_reinforce import combo_reinforce\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec789a",
   "metadata": {},
   "source": [
    "Here, I try to solve the Navigation Goal environment using the provided REINFORCE implementation.\n",
    "\n",
    "In the new version of Gymnasium you need different environments for rendering and no rendering.\n",
    "Here we instaintiate two versions of cartpole, one that animates the episodes (which slows everything down), and another that does not animate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b77433",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    env_name = \"gym_navigation:NavigationGoal-v0\",\n",
    "    hidden_size = 64,\n",
    "    lr = 1e-4,\n",
    "    gamma = 0.99,\n",
    "    episodes = 1500,\n",
    "    wandb_log = True,\n",
    "    capture_video = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab1f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(config['env_name'], render_mode=None)\n",
    "env_render = gym.make(config['env_name'], render_mode = 'human')\n",
    "policy = PolicyNet(env, config['hidden_size']).to(device)\n",
    "\n",
    "with wandb.init(project=\"DLA_Lab3_DRL\", config = config, monitor_gym=True, save_code=True):\n",
    "    config = wandb.config\n",
    "\n",
    "    running  = reinforce(policy, env, env_render, device=device, lr=config['lr'], num_episodes=config['episodes'], wandb_log=config['wandb_log'])\n",
    "\n",
    "plt.plot(running)\n",
    "torch.save(policy, f\"models/andyreinforce_newgymnavigation_lr{config['lr']}_gamma{config['gamma']}.pt\")\n",
    "env_render.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a9a01",
   "metadata": {},
   "source": [
    "I'll now try to implement my own version of the REINFORCE algorithm. Let's first test the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e92edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(config['env_name'], render_mode = 'human')\n",
    "policy = PolicyNet(env, config['hidden_size']).to(device)\n",
    "\n",
    "with wandb.init(project=\"DLA_Lab3_DRL\", config=config, monitor_gym=True, save_code=True):\n",
    "    config = wandb.config\n",
    "\n",
    "    running = combo_reinforce(env, policy, lr = config['lr'], gamma = config['gamma'], episodes = config['episodes'], device = device, wandb_log = config['wandb_log'])\n",
    "\n",
    "plt.plot(running)\n",
    "torch.save(policy, f\"models/comboreinforce_gymnavigation_lr{config['lr']}_gamma{config['gamma']}.pt\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad32251",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Solving another environment\n",
    "\n",
    "The [Gymnasium](https://gymnasium.farama.org/) framework has a ton of interesting and fun environments to work with. Pick one and try to solve it using any technique you like. The [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) environment is a fun one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b6c801",
   "metadata": {},
   "source": [
    "Ok, so, let's build a new LunarLander environment, with render mode so that we can graphically see progress of our lander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    env_name = \"MountainCar-v0\",\n",
    "    hidden_size = 64,\n",
    "    lr = 1e-3,\n",
    "    gamma = 0.99,\n",
    "    episodes = 1500,\n",
    "    wandb_log = True,\n",
    "    capture_video = True)\n",
    "\n",
    "env = gym.make(config['env_name'], render_mode='rgb_array' if config['capture_video'] else None)\n",
    "if config['capture_video']:\n",
    "    env = gym.wrappers.RecordVideo(env, f\"videos/{config['env_name']}/comboreinforce-lr{config['lr']}-g{config['gamma']}\",\n",
    "                                   episode_trigger=lambda t: t % 25 == 0)\n",
    "policy = PolicyNet(env, config['hidden_size']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36a21e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(config['env_name'], render_mode=None)\n",
    "env_render = gym.make(config['env_name'], render_mode = 'human')\n",
    "policy = PolicyNet(env, config['hidden_size']).to(device)\n",
    "\n",
    "with wandb.init(project=\"DLA_Lab3_DRL\", config = config, monitor_gym=True, save_code=True):\n",
    "    config = wandb.config\n",
    "\n",
    "    running  = reinforce(policy, env, env_render, device=device, lr=config['lr'], num_episodes=config['episodes'], wandb_log=config['wandb_log'])\n",
    "\n",
    "plt.plot(running)\n",
    "torch.save(policy, f\"models/andyreinforce_CartPole-v1_lr{config['lr']}_gamma{config['gamma']}.pt\")\n",
    "env_render.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b45c9d",
   "metadata": {},
   "source": [
    "To set things, I start implementing a lander that takes totally random actions at each time tick. The total reward will be very bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75049b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(10):\n",
    "    (state, _) = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    score = 0\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            print(f'Episode {episode+1}\\tScore: {score:.2f}')\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9997e40f",
   "metadata": {},
   "source": [
    "...As expected. Now I will try to use the REINFORCE algorithm: running the two versions of it made me find out that the episode-wise REINFORCE is the only one that \"works\" in this task, as opposite to the CartPole environment, which showed better results with the interaction-wise REINFORCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54251a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=\"DLA_Lab3_DRL\", config=config, monitor_gym=True, save_code=True):\n",
    "    config = wandb.config\n",
    "\n",
    "    running = combo_reinforce(env, policy, lr = config['lr'], gamma = config['gamma'], episodes = config['episodes'], device = device, wandb_log = config['wandb_log'])\n",
    "\n",
    "plt.plot(running)\n",
    "torch.save(policy, f\"models/comboreinforce_{config['env_name']}_lr{config['lr']}_gamma{config['gamma']}.pt\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb3c63",
   "metadata": {},
   "source": [
    "And finally, the Deep Q-Learning technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c7b5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b964056d",
   "metadata": {},
   "source": [
    "To complete the experience, let's try to solve also the CartPole environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf1447-d222-4b24-a357-5b7f9824390c",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Advanced techniques \n",
    "\n",
    "The `REINFORCE` and Q-Learning approaches, though venerable, are not even close to the state-of-the-art. Try using an off-the-shelf implementation of [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347) to solve one (or more) of these environments. Compare your results with those of Q-Learning and/or REINFORCE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4aeb0c",
   "metadata": {},
   "source": [
    "PPO uses the Actor-Critic approach for the agent. This means that it uses two models, one called the Actor and the other called Critic.\n",
    "\n",
    "The Actor model performs the task of learning what action to take under a particular observed state of the environment. In LunarLander-v2 case, it takes eight values list of the game as input which represents the current state of our rocket and gives a particular action what engine to fire as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517b8a52-d97b-4c32-bfcb-9069e7527ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgiovancombo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\giova\\__UNI\\Deep Learning Applications\\DLA-Labs\\lab3\\wandb\\run-20240215_191924-lpujhz9l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/giovancombo/DLA_Lab3_DRL/runs/lpujhz9l' target=\"_blank\">blazing-caress-167</a></strong> to <a href='https://wandb.ai/giovancombo/DLA_Lab3_DRL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/giovancombo/DLA_Lab3_DRL' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab3_DRL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/giovancombo/DLA_Lab3_DRL/runs/lpujhz9l' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab3_DRL/runs/lpujhz9l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giova\\anaconda3\\envs\\DRL\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 | Avg Reward 11.8\n",
      "Episode 20 | Avg Reward 10.5\n",
      "Episode 30 | Avg Reward 9.7\n",
      "Episode 40 | Avg Reward 10.0\n",
      "Episode 50 | Avg Reward 10.7\n",
      "Episode 60 | Avg Reward 10.7\n",
      "Episode 70 | Avg Reward 11.3\n",
      "Episode 80 | Avg Reward 10.6\n",
      "Episode 90 | Avg Reward 9.5\n",
      "Episode 100 | Avg Reward 10.2\n",
      "Episode 110 | Avg Reward 10.4\n",
      "Episode 120 | Avg Reward 10.3\n",
      "Episode 130 | Avg Reward 10.6\n",
      "Episode 140 | Avg Reward 11.0\n",
      "Episode 150 | Avg Reward 10.5\n",
      "Episode 160 | Avg Reward 12.2\n",
      "Episode 170 | Avg Reward 11.4\n",
      "Episode 180 | Avg Reward 10.9\n",
      "Episode 190 | Avg Reward 13.2\n",
      "Episode 200 | Avg Reward 16.8\n",
      "Episode 210 | Avg Reward 37.1\n",
      "Episode 220 | Avg Reward 99.9\n",
      "Episode 230 | Avg Reward 156.7\n",
      "Episode 240 | Avg Reward 69.0\n",
      "Episode 250 | Avg Reward 71.8\n",
      "Episode 260 | Avg Reward 109.3\n",
      "Episode 270 | Avg Reward 157.3\n",
      "Episode 280 | Avg Reward 106.7\n",
      "Episode 290 | Avg Reward 64.2\n",
      "Episode 300 | Avg Reward 59.8\n",
      "Episode 310 | Avg Reward 68.3\n",
      "Episode 320 | Avg Reward 55.5\n",
      "Episode 330 | Avg Reward 98.7\n",
      "Episode 340 | Avg Reward 178.0\n",
      "Episode 350 | Avg Reward 159.9\n",
      "Episode 360 | Avg Reward 159.7\n",
      "Episode 370 | Avg Reward 171.8\n",
      "Episode 380 | Avg Reward 118.5\n",
      "Episode 390 | Avg Reward 91.4\n",
      "Episode 400 | Avg Reward 80.8\n",
      "Episode 410 | Avg Reward 79.1\n",
      "Episode 420 | Avg Reward 73.6\n",
      "Episode 430 | Avg Reward 73.7\n",
      "Episode 440 | Avg Reward 84.2\n",
      "Episode 450 | Avg Reward 83.1\n",
      "Episode 460 | Avg Reward 70.8\n",
      "Episode 470 | Avg Reward 79.8\n",
      "Episode 480 | Avg Reward 64.7\n",
      "Episode 490 | Avg Reward 87.0\n",
      "Episode 500 | Avg Reward 74.8\n",
      "Episode 510 | Avg Reward 70.9\n",
      "Episode 520 | Avg Reward 58.1\n",
      "Episode 530 | Avg Reward 56.7\n",
      "Episode 540 | Avg Reward 62.9\n",
      "Episode 550 | Avg Reward 68.1\n",
      "Episode 560 | Avg Reward 115.6\n",
      "Episode 570 | Avg Reward 123.3\n",
      "Episode 580 | Avg Reward 115.3\n",
      "Episode 590 | Avg Reward 110.4\n",
      "Episode 600 | Avg Reward 96.0\n",
      "Episode 610 | Avg Reward 101.1\n",
      "Episode 620 | Avg Reward 123.3\n",
      "Episode 630 | Avg Reward 84.7\n",
      "Episode 640 | Avg Reward 279.2\n",
      "Episode 650 | Avg Reward 500.0\n",
      "Episode 660 | Avg Reward 318.3\n",
      "Episode 670 | Avg Reward 232.1\n",
      "Episode 680 | Avg Reward 214.2\n",
      "Episode 690 | Avg Reward 207.5\n",
      "Episode 700 | Avg Reward 233.7\n",
      "Episode 710 | Avg Reward 255.4\n",
      "Episode 720 | Avg Reward 275.5\n",
      "Episode 730 | Avg Reward 355.4\n",
      "Episode 740 | Avg Reward 360.9\n",
      "Episode 750 | Avg Reward 175.6\n",
      "Episode 760 | Avg Reward 9.3\n",
      "Episode 770 | Avg Reward 9.0\n",
      "Episode 780 | Avg Reward 9.6\n",
      "Episode 790 | Avg Reward 9.5\n",
      "Episode 800 | Avg Reward 9.7\n",
      "Episode 810 | Avg Reward 9.8\n",
      "Episode 820 | Avg Reward 9.8\n",
      "Episode 830 | Avg Reward 9.2\n",
      "Episode 840 | Avg Reward 9.2\n",
      "Episode 850 | Avg Reward 9.7\n",
      "Episode 860 | Avg Reward 9.3\n",
      "Episode 870 | Avg Reward 9.3\n",
      "Episode 880 | Avg Reward 9.1\n",
      "Episode 890 | Avg Reward 9.3\n",
      "Episode 900 | Avg Reward 9.4\n",
      "Episode 910 | Avg Reward 9.3\n",
      "Episode 920 | Avg Reward 9.9\n",
      "Episode 930 | Avg Reward 8.9\n",
      "Episode 940 | Avg Reward 9.5\n",
      "Episode 950 | Avg Reward 10.6\n",
      "Episode 960 | Avg Reward 12.6\n",
      "Episode 970 | Avg Reward 20.5\n",
      "Episode 980 | Avg Reward 22.8\n",
      "Episode 990 | Avg Reward 22.1\n",
      "Episode 1000 | Avg Reward 30.1\n",
      "Episode 1010 | Avg Reward 38.4\n",
      "Episode 1020 | Avg Reward 42.1\n",
      "Episode 1030 | Avg Reward 57.8\n",
      "Episode 1040 | Avg Reward 55.7\n",
      "Episode 1050 | Avg Reward 64.5\n",
      "Episode 1060 | Avg Reward 79.5\n",
      "Episode 1070 | Avg Reward 86.3\n",
      "Episode 1080 | Avg Reward 105.7\n",
      "Episode 1090 | Avg Reward 124.1\n",
      "Episode 1100 | Avg Reward 132.0\n",
      "Episode 1110 | Avg Reward 132.2\n",
      "Episode 1120 | Avg Reward 134.6\n",
      "Episode 1130 | Avg Reward 130.4\n",
      "Episode 1140 | Avg Reward 124.2\n",
      "Episode 1150 | Avg Reward 120.8\n",
      "Episode 1160 | Avg Reward 112.4\n",
      "Episode 1170 | Avg Reward 115.8\n",
      "Episode 1180 | Avg Reward 121.8\n",
      "Episode 1190 | Avg Reward 132.6\n",
      "Episode 1200 | Avg Reward 126.0\n",
      "Episode 1210 | Avg Reward 152.4\n",
      "Episode 1220 | Avg Reward 180.5\n",
      "Episode 1230 | Avg Reward 372.7\n",
      "Episode 1240 | Avg Reward 445.0\n",
      "Episode 1250 | Avg Reward 471.9\n",
      "Episode 1260 | Avg Reward 500.0\n",
      "Episode 1270 | Avg Reward 500.0\n",
      "Episode 1280 | Avg Reward 500.0\n",
      "Episode 1290 | Avg Reward 425.6\n",
      "Episode 1300 | Avg Reward 493.8\n",
      "Episode 1310 | Avg Reward 500.0\n",
      "Episode 1320 | Avg Reward 478.4\n",
      "Episode 1330 | Avg Reward 287.0\n",
      "Episode 1340 | Avg Reward 107.9\n",
      "Episode 1350 | Avg Reward 98.9\n",
      "Episode 1360 | Avg Reward 117.4\n",
      "Episode 1370 | Avg Reward 144.6\n",
      "Episode 1380 | Avg Reward 156.2\n",
      "Episode 1390 | Avg Reward 154.0\n",
      "Episode 1400 | Avg Reward 120.9\n",
      "Episode 1410 | Avg Reward 172.8\n",
      "Episode 1420 | Avg Reward 175.6\n",
      "Episode 1430 | Avg Reward 170.7\n",
      "Episode 1440 | Avg Reward 164.5\n",
      "Episode 1450 | Avg Reward 165.4\n",
      "Episode 1460 | Avg Reward 288.2\n",
      "Episode 1470 | Avg Reward 228.4\n",
      "Episode 1480 | Avg Reward 9.4\n",
      "Episode 1490 | Avg Reward 9.4\n",
      "Episode 1500 | Avg Reward 9.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>score</td><td>▁▁▁▁▁▁▂▂▂▄▂▁▂▂▂▃▃▅▃▄▁▁▁▁▁▁▁▂▂▃▃▃▃███▃▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>score</td><td>10.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blazing-caress-167</strong> at: <a href='https://wandb.ai/giovancombo/DLA_Lab3_DRL/runs/lpujhz9l' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab3_DRL/runs/lpujhz9l</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240215_191924-lpujhz9l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "from combo_ppo import PPOAgent, PPOTrainer, train_ppo\n",
    "\n",
    "config = dict(\n",
    "    env_name = \"CartPole-v1\",\n",
    "    hidden_size = 64,\n",
    "    policy_lr = 3e-4,\n",
    "    value_lr = 1e-3,\n",
    "    target_kl_div = 0.02,\n",
    "    max_policy_train_iters = 40,\n",
    "    value_train_iters = 40,\n",
    "    episodes = 1500,\n",
    "    max_steps = 1000,\n",
    "    log_frequency = 10,\n",
    "    wandb_log = True,\n",
    "    capture_video = False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode = 'rgb_array' if config['capture_video'] else None)\n",
    "if config['capture_video']:\n",
    "    env = gym.wrappers.RecordVideo(env, f\"videos/{config['env_name']}/ppo-plr{config['policy_lr']}-vlr{config['value_lr']}\",\n",
    "                                   episode_trigger=lambda t: t % 25 == 0)\n",
    "model = PPOAgent(env.observation_space.shape[0], env.action_space.n, config['hidden_size']).to(device)\n",
    "ppo = PPOTrainer(model,\n",
    "                 policy_lr = config['policy_lr'],\n",
    "                 value_lr = config['value_lr'],\n",
    "                 target_kl_div = config['target_kl_div'],\n",
    "                 max_policy_train_iters = config['max_policy_train_iters'],\n",
    "                 value_train_iters = config['value_train_iters'],)\n",
    "\n",
    "\n",
    "train_ppo(env, model, ppo, config['episodes'], config['max_steps'], config['log_frequency'], device,\n",
    "          config['wandb_log'], config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
