{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a18c83b-448a-4679-8f19-0fa8d2c95508",
   "metadata": {},
   "source": [
    "# Getting up to speed with DRL\n",
    "\n",
    "In this notebook I provide a simple example of implementing a policy gradient Deep Reinforcement Learning algorithm to solve a control problem with continuous state space and discrete action space -- the venerable [CartPole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/). You should study the implementation in this notebook in preparation for the laboratory next Wednesday.\n",
    "\n",
    "This notebook should run in an environment with at least the following packages installed (the gpu version of PyTorch is not mandatory):\n",
    "\n",
    "     conda create -n DRL -c conda-forge gymnasium pytorch-gpu matplotlib pygame jupyterlab\n",
    "     \n",
    "Some background reading to get you started:\n",
    "\n",
    "1. We will be using the [Gymnasium](https://gymnasium.farama.org/) framework for all of our experiments. This framework provides a consistent interface to a broad range of reinforcement learning environments (including CartPole). You should familiarize yourself with how it works, how environments are specified, how to instantiate them, and how to interact with them.\n",
    "\n",
    "2. [This excellent blog post](http://karpathy.github.io/2016/05/31/rl/) is a great introduction to policy gradients, where they come from and how they work. Give it a read and I am sure it will help understand better what is going on in this notebook.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "We start with our standard imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b2a363-e3cd-4b22-8741-f3545e150281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Plus one non standard one -- we need this to sample from policies.\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd56607-0c95-449c-8724-8db4e54788fd",
   "metadata": {},
   "source": [
    "And also some utility functions useful for what comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c56fab-8522-40fa-b5f2-895b042c1c48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Given an environment, observation, and policy, sample from pi(a | obs). Returns the\n",
    "# selected action and the log probability of that action (needed for policy gradient).\n",
    "def select_action(env, obs, policy):\n",
    "    dist = Categorical(policy(obs))\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "    return (action.item(), log_prob.reshape(1))\n",
    "\n",
    "# Utility to compute the discounted total reward. Torch doesn't like flipped arrays, so we need to\n",
    "# .copy() the final numpy array. There's probably a better way to do this.\n",
    "def compute_returns(rewards, gamma):\n",
    "    return np.flip(np.cumsum([gamma**(i+1)*r for (i, r) in enumerate(rewards)][::-1]), 0).copy()\n",
    "\n",
    "# Given an environment and a policy, run it up to the maximum number of steps.\n",
    "def run_episode(env, policy, maxlen=500, device='cpu'):\n",
    "    # Collect just about everything.\n",
    "    observations = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    \n",
    "    # Reset the environment and start the episode.\n",
    "    (obs, info) = env.reset()\n",
    "    for i in range(maxlen):\n",
    "        # Get the current observation, run the policy and select an action.\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "        (action, log_prob) = select_action(env, obs, policy)\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        # Advance the episode by executing the selected action.\n",
    "        (obs, reward, term, trunc, info) = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if term or trunc:\n",
    "            break\n",
    "    return (observations, actions, torch.cat(log_probs), rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dea044-14e1-44f1-a851-ef6e19452692",
   "metadata": {},
   "source": [
    "## The Policy network\n",
    "\n",
    "Here I provide a simple policy network which should work with any environment with continuous observations and discrete action spaces. Note how it uses the *specification* of the environment to configure its input and output spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a8c3360-3671-4075-8ebe-9dfd7f1d1a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A simple, but generic, policy network with one hidden layer.\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, env.action_space.n)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = F.relu(self.fc2(s))\n",
    "        s = F.softmax(self.fc3(s), dim=-1)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e2af5-62fd-422e-8fb7-06295c49ddc7",
   "metadata": {},
   "source": [
    "## The `REINFORCE` Algorithm\n",
    "\n",
    "This is a very simple implementation of the most basic policy gradient DRL algorithm: `REINFORCE`. It is a very direct implementation of the policy gradient update (although I use Adam instead of SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09393c40-def8-47ca-9a3c-8c7689d5f87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A direct, inefficient, and probably buggy implementation of the REINFORCE policy gradient algorithm.\n",
    "def reinforce(policy, env, env_render=None, gamma=0.99, lr=1e-4, num_episodes=10, device='cpu'):\n",
    "    # The only non-vanilla part: we use Adam instead of SGD.\n",
    "    opt = torch.optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    # Track episode rewards in a list.\n",
    "    running_rewards = [0.0]\n",
    "    \n",
    "    # The main training loop.\n",
    "    policy.train()\n",
    "    for episode in range(num_episodes):\n",
    "        # Run an episode of the environment, collect everything needed for policy update.\n",
    "        (observations, actions, log_probs, rewards) = run_episode(env, policy, device=device)\n",
    "        \n",
    "        # Compute the discounted reward for every step of the episode. \n",
    "        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)\n",
    "        \n",
    "        # Keep a running average of total discounted rewards for the whole episode.\n",
    "        running_rewards.append(0.005 * returns[0].item() + 0.995 * running_rewards[-1])\n",
    "        \n",
    "        # Standardize returns.\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-6)\n",
    "        \n",
    "        # Make an optimization step\n",
    "        log_probs = log_probs.to(device)\n",
    "        returns = returns.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = (-log_probs * returns).sum()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # Render an episode after every 100 policy updates.\n",
    "        if not episode % 500:\n",
    "            policy.eval()\n",
    "            (obs, _, _, _) = run_episode(env_render, policy, device=device)\n",
    "            policy.train()\n",
    "            print(f'Running reward: {running_rewards[-1]}')\n",
    "    \n",
    "    # Return the running rewards.\n",
    "    policy.eval()\n",
    "    return running_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7086f041-a315-4f4e-9db6-01cb446fffa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a (rendering) CartPole environment.\n",
    "env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# Make a policy network and run a few episodes to see how well random initialization works.\n",
    "policy = PolicyNet(env_render)\n",
    "for _ in range(10):\n",
    "    run_episode(env_render, policy)\n",
    "    \n",
    "# If you don't close the environment, the PyGame window stays visible.\n",
    "env_render.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adeffa1e-d23d-46ea-a3e4-e65251592529",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward: 1.8944137573242188\n",
      "Running reward: 71.66281281491712\n",
      "Running reward: 94.2494010752924\n",
      "Running reward: 92.51053924227531\n",
      "Running reward: 98.09951680761739\n",
      "Running reward: 61.94329475551012\n",
      "Running reward: 52.20149737450006\n",
      "Running reward: 73.01279925211064\n",
      "Running reward: 56.36627565516559\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m policy \u001b[38;5;241m=\u001b[39m PolicyNet(env)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Train the agent.\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(reinforce(policy, env, env_render, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m))\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Close up everything\u001b[39;00m\n\u001b[0;32m     14\u001b[0m env_render\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn [4], line 13\u001b[0m, in \u001b[0;36mreinforce\u001b[1;34m(policy, env, env_render, gamma, num_episodes)\u001b[0m\n\u001b[0;32m     10\u001b[0m policy\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Run an episode of the environment, collect everything needed for policy update.\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     (observations, actions, log_probs, rewards) \u001b[38;5;241m=\u001b[39m \u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Compute the discounted reward for every step of the episode. \u001b[39;00m\n\u001b[0;32m     16\u001b[0m     returns \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(compute_returns(rewards, gamma), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[1;32mIn [2], line 27\u001b[0m, in \u001b[0;36mrun_episode\u001b[1;34m(env, policy, maxlen)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(maxlen):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Get the current observation, run the policy and select an action.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(obs)\n\u001b[1;32m---> 27\u001b[0m     (action, log_prob) \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     observations\u001b[38;5;241m.\u001b[39mappend(obs)\n\u001b[0;32m     29\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(action)\n",
      "Cell \u001b[1;32mIn [2], line 4\u001b[0m, in \u001b[0;36mselect_action\u001b[1;34m(env, obs, policy)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(env, obs, policy):\n\u001b[1;32m----> 4\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m      6\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\DRL\\lib\\site-packages\\torch\\distributions\\categorical.py:66\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     65\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\DRL\\lib\\site-packages\\torch\\distributions\\distribution.py:61\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     59\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[0;32m     60\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m---> 61\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m     62\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     63\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m             )\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# In the new version of Gymnasium you need different environments for rendering and no rendering.\n",
    "# Here we instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "# down), and another that does not animate.\n",
    "env = gymnasium.make('CartPole-v1')\n",
    "env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# Make a policy network.\n",
    "policy = PolicyNet(env)\n",
    "\n",
    "# Train the agent.\n",
    "plt.plot(reinforce(policy, env, env_render, num_episodes=2000))\n",
    "\n",
    "# Close up everything\n",
    "env_render.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40868e-7f32-4409-9f07-1fe781de45da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# And run the final agent for a few episodes.\n",
    "env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "for _ in range(10):\n",
    "    run_episode(env_render, policy)\n",
    "env_render.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bc02a",
   "metadata": {},
   "source": [
    "What about running the model on the Navigation Problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the new version of Gymnasium you need different environments for rendering and no rendering.\n",
    "# Here we instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "# down), and another that does not animate.\n",
    "\n",
    "env = gymnasium.make('gym_navigation:NavigationGoal-v0', render_mode=None, track_id=1)\n",
    "env_render = gymnasium.make('gym_navigation:NavigationGoal-v0', render_mode='human')\n",
    "\n",
    "# Make a policy network.\n",
    "policy = PolicyNet(env).to(device)\n",
    "\n",
    "# Train the agent.\n",
    "running  = reinforce(policy, env, env_render, device=device, lr=1e-4, num_episodes=5000)\n",
    "running += reinforce(policy, env, env_render, device=device, lr=1e-5, num_episodes=5000)\n",
    "plt.plot(running)\n",
    "\n",
    "# Close up everything\n",
    "env_render.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc78ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And run the final agent for a few episodes.\n",
    "env_render = gymnasium.make('gym_navigation:NavigationGoal-v0', render_mode='human')\n",
    "for _ in range(10):\n",
    "    (_, _, _, r) = run_episode(env_render, policy, device=device)\n",
    "    returns = compute_returns(r, 0.99)\n",
    "    print(returns[0])\n",
    "env_render.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202cea9-aef2-46da-ace9-ddf25bca77be",
   "metadata": {},
   "source": [
    "## For your consideration\n",
    "\n",
    "There are many things that can be improved in this example. Some things you can think about:\n",
    "\n",
    "1. **Replay**. In the current implementation we execute an episode, and then immediately run an optimization step on all of the steps of the episode. Not only are we using *correlated* samples from a single episode, we are decidedly *not* taking advantage of parallelism via batch gradient descent. Note that `REINFORCE` does **not** require entire trajectories, all we need are the discounted rewards and log probabilities for *individual transitions*.\n",
    "\n",
    "2. **Exploration**. The model is probably overfitting (or perhaps remaining too *plastic*, which can explain the unstable convergence). Our policy is *always* stochastic in that we sample from the output distribution. It would be interesting to add a temperature parameter to the policy so that we can control this behavior, or even implement a deterministic policy sampler that always selects the action with max probability to evaluate the quality of the learned policy network.\n",
    "\n",
    "3. **Discount Factor**: The discount factor (default $\\gamma = 0.99$) is an important hyperparameter that has an effect on the stability of training. Try different values for $\\gamma$ and see how it affects training. Can you think of other ways to stabilize training?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
