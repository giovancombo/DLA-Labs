{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d550d73",
   "metadata": {},
   "source": [
    "***Deep Learning Applications 2023** course, held by Professor **Andrew David Bagdanov** - University of Florence, Italy*\n",
    "\n",
    "*Notebook and code created by **Giovanni Colombo** - Mat. 7092745*\n",
    "\n",
    "Check the [Repository on GitHub](https://github.com/giovancombo/DLA-Labs/tree/main/lab1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f7c5d-46f3-4cbd-80ad-f1e50cd65096",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #1\n",
    "\n",
    "In this first laboratory we will work relatively simple architectures to get a feel for working with Deep Models. This notebook is designed to work with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed8906-bd19-4b4f-8b79-4feae355ffd6",
   "metadata": {},
   "source": [
    "## Exercise 1: Warming Up\n",
    "In this series of exercises I will duplicate (on a small scale) the results of the ResNet paper:\n",
    "\n",
    "> [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385), Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR 2016.\n",
    "\n",
    "I will do this in steps, firstly using a Multilayer Perceptron on MNIST.\n",
    "\n",
    "What's important to recall is that the main message of the ResNet paper is that **deeper networks do not guarantee** more reduction in training loss (or in validation accuracy).\n",
    "Below, I will incrementally build a sequence of experiments to verify this for different architectures, starting with an *MLP*.\n",
    "\n",
    "The Laboratory requires me to compare multiple training runs, so I took this as a great opportunity to learn to use [Weights and Biases](https://wandb.ai/site) for performance monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2cad13-ee2c-4e43-b5c7-31760da8c2df",
   "metadata": {},
   "source": [
    "### Exercise 1.1: A baseline MLP\n",
    "\n",
    "I will now implement a *simple Multilayer Perceptron* to classify the 10 digits of MNIST, and (hopefully) train it to convergence, monitoring Training and Validation losses and accuraces with W&B.\n",
    "\n",
    "The exercise wants me to think in an *abstract* way: I'll have to instantiate multiple models, with different hyperparameters configurations each, and train them on different datasets.\n",
    "It could be a good idea to try to generalize the most possible the instantiation of every object of the training workflow. That's why I decided to try to build a single file `config.yaml`, where I put almost every variable that can help me building any model I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giova\\anaconda3\\envs\\DLA\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports and dependencies\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import yaml\n",
    "# Importing Weights and Biases for tracking and comparing different runs\n",
    "import wandb\n",
    "\n",
    "import aux_functions, models\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors\n",
    "                                      if not mirror.startswith(\"http://yann.lecun.com\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821224b8",
   "metadata": {},
   "source": [
    "I define a `load` function, that passes the dictionary `config` (obtained from my `.yaml` file) as an argument, in order to load the dataset we want (between MNIST and CIFAR10), transformed accordingly, and splitted into *Train*, *Validation* and *Test* sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(config):\n",
    "    # Transformations applied to the dataset\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.1307,), (0.3081,)) if config.dataset == \"MNIST\" else transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "    train_data = getattr(torchvision.datasets, config.dataset)(\"./data\", train = True, download = True, transform = transform)\n",
    "    test_set = getattr(torchvision.datasets, config.dataset)(\"./data\", train = False, download = True, transform = transform)\n",
    "    \n",
    "    # Splitting the Training Set into Training and Validation Sets\n",
    "    train_set, val_set = random_split(train_data, [len(train_data) - config.val_size, config.val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_set, config.batch_size, shuffle = True, num_workers = config.num_workers)\n",
    "    val_loader = DataLoader(val_set, config.batch_size, num_workers = config.num_workers)\n",
    "    test_loader = DataLoader(test_set, config.batch_size, num_workers = config.num_workers)\n",
    "\n",
    "    print(f\"Dataset {config.dataset} loaded with {len(train_set)} Train samples, {len(val_set)} Validation samples, {len(test_set)} Test samples.\\n\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91032c6",
   "metadata": {},
   "source": [
    "The script file `models.py` contains all the model classes used for this Laboratory:\n",
    "+ **MLP**, for instantiating a *Multilayer Perceptron*\n",
    "+ **ResidualMLP**, for instantiating an MLP that implements *Residual Connections*\n",
    "+ **CNN**, for instantiating *Convolutional Network*, with the possibility of tuning almost every possible parameter\n",
    "+ **ResidualCNN**, for instantiating a ConvNet that implements *Residual Connections*\n",
    "+ **ResNet**, for instantiating an actual *ResNet* as defined in the [Paper](https://arxiv.org/abs/1512.03385), in its *[9, 18, 34, 50, 101, 152]* versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8165608",
   "metadata": {},
   "source": [
    "The `build_model` function instantiates Model, Loss Function and Optimizer chosen with the `config` file, and sends it to `device`, that can be `cuda` (in my case, a *Nvidia GeForce RTX 3060 Laptop*) or `cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config):\n",
    "    # Building the model\n",
    "    if config.convnet:\n",
    "        if config.residual:\n",
    "            if config.resnet:\n",
    "                m = models.ResNet(config.resnet_name, config.input_shape, config.resnet_hidden_size, config.classes, config.activation, config.use_bn, config.dropout)\n",
    "            else:\n",
    "                m = models.ResidualCNN(config.input_shape, config.CNN_hidden_size, config.classes, config.depth, config.activation, config.use_bn)\n",
    "        else:\n",
    "            m = models.CNN(config.input_shape, config.CNN_hidden_size, config.classes, config.depth, config.kernel_size, config.stride, config.padding, config.activation, config.dropout, config.pool, config.pool_size, config.use_bn)\n",
    "    elif config.residual:\n",
    "        m = models.ResidualMLP(config.input_size, config.MLP_hidden_size, config.classes, config.activation, config.dropout)\n",
    "    else:\n",
    "        m = models.MLP(config.input_size, config.MLP_hidden_size, config.classes, config.activation, config.dropout)\n",
    "    \n",
    "    model = m.to(device)\n",
    "\n",
    "    # Defining the loss function and the optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if config.optimizer == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                     lr = config.learning_rate,\n",
    "                                     weight_decay = config.weight_decay)\n",
    "    elif config.optimizer == \"SGD\" or config.optimizer == \"RMSprop\":\n",
    "        optimizer = getattr(torch.optim, config.optimizer)(model.parameters(),\n",
    "                                                           lr = config.learning_rate,\n",
    "                                                           momentum = config.momentum,\n",
    "                                                           weight_decay = config.weight_decay)\n",
    "\n",
    "    print(f\"Model instantiated: {model.__class__.__name__}\")\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\\n\")\n",
    "    print(model)\n",
    "    print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "    print(optimizer)\n",
    "    print(f\"Loss: {criterion}\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd52404",
   "metadata": {},
   "source": [
    "Functions for periodical log of Loss and Accuracy from Training and Evaluation phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for periodical log of training data\n",
    "def log_train(epoch, loss, accuracy, mean_loss, mean_acc, example_ct, config):\n",
    "    print(f'Epoch {epoch + 1}/{config.epochs} | Train Loss = {mean_loss:.4f}; Train Accuracy = {mean_acc:.2f}%')\n",
    "    wandb.log({\"Training/Training Loss\": loss,\n",
    "                \"Training/Training Accuracy\": accuracy,\n",
    "                \"Training/Training Epochs\": epoch + 1}, step = example_ct)\n",
    "\n",
    "# Function for log of validation data at the end of an epoch\n",
    "def log_validation(epoch, mean_loss, val_loss, mean_accuracy, val_accuracy, example_ct):\n",
    "    print(f'\\nEnd of epoch {epoch + 1} | Validation Loss: {val_loss:.4f}; Validation Accuracy: {val_accuracy}%\\n')\n",
    "\n",
    "    wandb.log({\"Train Loss\": mean_loss, \n",
    "               \"Validation Loss\": val_loss,\n",
    "               \"Epoch\": epoch + 1,\n",
    "               \"Train Accuracy\": mean_accuracy,\n",
    "               \"Validation Accuracy\": val_accuracy}, step = example_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a89546",
   "metadata": {},
   "source": [
    "The training loop lies in the `train` function, that takes all the objects instantiated in the previous steps and uses them to train the model.\n",
    "\n",
    "The *forward* and *backward* passes are performed batch-wise through the `train_batch` function, that implements a tweak to reshape the input images' sizes accordingly to the model used. Same thing is done in the `validation` and `test` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, config):\n",
    "\n",
    "    # Telling W&B to watch gradients and the model parameters\n",
    "    wandb.watch(model, criterion, log = \"all\", log_freq = config.log_interval)\n",
    "    example_ct = 0\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in tqdm(range(config.epochs), desc = \"Training Epochs\", ncols = 100):\n",
    "        model.train()\n",
    "        losses, accuracies = [], []\n",
    "        for batch, (images, labels) in enumerate(train_loader):\n",
    "            loss, accuracy = train_batch(model, images, labels, criterion, optimizer, config)\n",
    "\n",
    "            example_ct += len(images)\n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(accuracy)\n",
    "            mean_loss, mean_accuracy = np.mean(losses[-config.log_interval:]), np.mean(accuracies[-config.log_interval:])\n",
    "\n",
    "            if ((batch + 1) % config.log_interval) == 0:\n",
    "                log_train(epoch, loss, accuracy, mean_loss, mean_accuracy, example_ct, config)\n",
    "\n",
    "        # Validation at the end of the epoch\n",
    "        val_loss, val_accuracy = test(model, val_loader, config)\n",
    "\n",
    "        # Logging losses and accuracies at the end of the epoch\n",
    "        log_validation(epoch, mean_loss, val_loss, mean_accuracy, val_accuracy, example_ct)\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "\n",
    "\n",
    "# Function for training a single batch\n",
    "def train_batch(model, images, labels, criterion, optimizer, config):\n",
    "    if not config.convnet:\n",
    "        images = images.reshape(-1, config.input_size).to(device)\n",
    "    else:\n",
    "        images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculating training accuracy\n",
    "    correct, total = 0, 0\n",
    "    _, pred = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (pred == labels).sum().item()\n",
    "        \n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "# Evaluation Loop (for Validation and Test)\n",
    "@torch.no_grad()\n",
    "def test(model, test_loader, config):\n",
    "    test_loss = 0\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    for images, labels in test_loader:\n",
    "        if not config.convnet:\n",
    "            images = images.reshape(-1, config.input_size).to(device)\n",
    "        else:\n",
    "            images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        test_loss += F.cross_entropy(outputs, labels, reduction = 'sum')\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / total\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991571f",
   "metadata": {},
   "source": [
    "The `load`, `build_model`, `train` and `test` functions are all contained in a single function, `model_pipeline`, that allows me to wrap all my workflow into a *Weights & Biases* run more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(project_name):\n",
    "    # Loading the yaml file containing the hyperparameter configuration\n",
    "    with open(\"config.yaml\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    wandb.login()\n",
    "    print(\"Initializing Weights & Biases run...\")\n",
    "\n",
    "    # Initializing a wandb run for logging losses, accuracies and gradients\n",
    "    with wandb.init(project = project_name, config = config):\n",
    "        config = wandb.config\n",
    "\n",
    "        # 1. Load the data\n",
    "        train_loader, val_loader, test_loader = load(config)\n",
    "\n",
    "        # 2. Build the model\n",
    "        model, criterion, optimizer = build_model(config)\n",
    "\n",
    "        # 3. Train the model\n",
    "        train(model, train_loader, val_loader, criterion, optimizer, config)\n",
    "\n",
    "        # 4. Evaluate the model on the test set\n",
    "        test_loss, test_accuracy = test(model, test_loader, config)\n",
    "\n",
    "        print(f\"Testing completed! | Test Loss: {test_loss:.4f}; Test Accuracy = {test_accuracy:.2f}%\")\n",
    "        wandb.log({\"Test Loss\": test_loss,\n",
    "                \"Test Accuracy\": test_accuracy})\n",
    "\n",
    "    # 5. Saving the model, assigning it a name based on the hyperparameters used\n",
    "    if config['save_model']:\n",
    "        folder = f\"models/{config['dataset']}/{model.__class__.__name__}\"\n",
    "        model_name = aux_functions.model_path(config, model.__class__.__name__)\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        torch.save(model.state_dict(), folder + model_name + \".pt\")\n",
    "        print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f38cf7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgiovancombo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Weights & Biases run...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\giova\\__UNI\\Deep Learning Applications\\LABORATORIO\\lab1_CNNs\\wandb\\run-20240111_021448-8psftzzu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/giovancombo/DLA_Lab1_CNN/runs/8psftzzu' target=\"_blank\">likely-flower-368</a></strong> to <a href='https://wandb.ai/giovancombo/DLA_Lab1_CNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/giovancombo/DLA_Lab1_CNN' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab1_CNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/giovancombo/DLA_Lab1_CNN/runs/8psftzzu' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab1_CNN/runs/8psftzzu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10 loaded with 40000 Train samples, 10000 Validation samples, 10000 Test samples.\n",
      "\n",
      "Model instantiated: CNN\n",
      "Number of parameters: 657290\n",
      "\n",
      "CNN(\n",
      "  (act): ReLU()\n",
      "  (convlayers): Sequential(\n",
      "    (0): ConvBlock(\n",
      "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Dropout(p=0.3, inplace=False)\n",
      "    (1): Linear(in_features=65536, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Optimizer: Adam\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0005\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "Loss: CrossEntropyLoss()\n",
      "Device: cuda:0\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|                                                       | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss = 7.1316; Train Accuracy = 18.69%\n",
      "Epoch 1/20 | Train Loss = 4.2960; Train Accuracy = 27.87%\n",
      "Epoch 1/20 | Train Loss = 2.7594; Train Accuracy = 34.28%\n",
      "Epoch 1/20 | Train Loss = 1.9859; Train Accuracy = 39.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   5%|██▎                                            | 1/20 [01:08<21:36, 68.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 1 | Validation Loss: 1.7783; Validation Accuracy: 41.96%\n",
      "\n",
      "Epoch 2/20 | Train Loss = 1.7108; Train Accuracy = 44.56%\n",
      "Epoch 2/20 | Train Loss = 1.5636; Train Accuracy = 46.97%\n",
      "Epoch 2/20 | Train Loss = 1.4298; Train Accuracy = 50.86%\n",
      "Epoch 2/20 | Train Loss = 1.4389; Train Accuracy = 50.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  10%|████▋                                          | 2/20 [02:15<20:14, 67.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 2 | Validation Loss: 1.5307; Validation Accuracy: 47.57%\n",
      "\n",
      "Epoch 3/20 | Train Loss = 1.3855; Train Accuracy = 52.15%\n",
      "Epoch 3/20 | Train Loss = 1.3447; Train Accuracy = 53.68%\n",
      "Epoch 3/20 | Train Loss = 1.3222; Train Accuracy = 54.25%\n",
      "Epoch 3/20 | Train Loss = 1.2920; Train Accuracy = 56.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  15%|███████                                        | 3/20 [03:16<18:20, 64.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 3 | Validation Loss: 1.4879; Validation Accuracy: 48.22%\n",
      "\n",
      "Epoch 4/20 | Train Loss = 1.3117; Train Accuracy = 54.20%\n",
      "Epoch 4/20 | Train Loss = 1.2746; Train Accuracy = 55.98%\n",
      "Epoch 4/20 | Train Loss = 1.2712; Train Accuracy = 56.31%\n",
      "Epoch 4/20 | Train Loss = 1.2580; Train Accuracy = 57.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  20%|█████████▍                                     | 4/20 [04:17<16:48, 63.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 4 | Validation Loss: 1.4986; Validation Accuracy: 50.06%\n",
      "\n",
      "Epoch 5/20 | Train Loss = 1.2135; Train Accuracy = 58.12%\n",
      "Epoch 5/20 | Train Loss = 1.1677; Train Accuracy = 59.50%\n",
      "Epoch 5/20 | Train Loss = 1.1456; Train Accuracy = 60.70%\n",
      "Epoch 5/20 | Train Loss = 1.1371; Train Accuracy = 60.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  25%|███████████▊                                   | 5/20 [05:17<15:31, 62.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 5 | Validation Loss: 1.3129; Validation Accuracy: 54.27%\n",
      "\n",
      "Epoch 6/20 | Train Loss = 1.1173; Train Accuracy = 60.83%\n",
      "Epoch 6/20 | Train Loss = 1.1033; Train Accuracy = 61.75%\n",
      "Epoch 6/20 | Train Loss = 1.1019; Train Accuracy = 61.12%\n",
      "Epoch 6/20 | Train Loss = 1.0732; Train Accuracy = 63.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  30%|██████████████                                 | 6/20 [06:18<14:23, 61.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 6 | Validation Loss: 1.2919; Validation Accuracy: 54.75%\n",
      "\n",
      "Epoch 7/20 | Train Loss = 1.0388; Train Accuracy = 64.22%\n",
      "Epoch 7/20 | Train Loss = 1.0404; Train Accuracy = 63.96%\n",
      "Epoch 7/20 | Train Loss = 1.0366; Train Accuracy = 64.35%\n",
      "Epoch 7/20 | Train Loss = 1.0531; Train Accuracy = 63.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  35%|████████████████▍                              | 7/20 [07:18<13:16, 61.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 7 | Validation Loss: 1.4996; Validation Accuracy: 50.27%\n",
      "\n",
      "Epoch 8/20 | Train Loss = 1.1258; Train Accuracy = 60.89%\n",
      "Epoch 8/20 | Train Loss = 1.0522; Train Accuracy = 63.81%\n",
      "Epoch 8/20 | Train Loss = 1.0085; Train Accuracy = 65.10%\n",
      "Epoch 8/20 | Train Loss = 1.0370; Train Accuracy = 63.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  40%|██████████████████▊                            | 8/20 [08:30<12:53, 64.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 8 | Validation Loss: 1.4727; Validation Accuracy: 51.1%\n",
      "\n",
      "Epoch 9/20 | Train Loss = 1.0411; Train Accuracy = 63.76%\n",
      "Epoch 9/20 | Train Loss = 0.9577; Train Accuracy = 66.90%\n",
      "Epoch 9/20 | Train Loss = 0.9660; Train Accuracy = 67.12%\n",
      "Epoch 9/20 | Train Loss = 1.0319; Train Accuracy = 64.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  45%|█████████████████████▏                         | 9/20 [09:29<11:32, 62.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 9 | Validation Loss: 1.3681; Validation Accuracy: 54.77%\n",
      "\n",
      "Epoch 10/20 | Train Loss = 0.9442; Train Accuracy = 67.62%\n",
      "Epoch 10/20 | Train Loss = 0.9016; Train Accuracy = 68.90%\n",
      "Epoch 10/20 | Train Loss = 0.9338; Train Accuracy = 68.36%\n",
      "Epoch 10/20 | Train Loss = 0.9067; Train Accuracy = 68.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  50%|███████████████████████                       | 10/20 [10:39<10:50, 65.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 10 | Validation Loss: 1.4039; Validation Accuracy: 54.92%\n",
      "\n",
      "Epoch 11/20 | Train Loss = 0.9321; Train Accuracy = 68.13%\n",
      "Epoch 11/20 | Train Loss = 0.9071; Train Accuracy = 68.62%\n",
      "Epoch 11/20 | Train Loss = 0.9023; Train Accuracy = 68.34%\n",
      "Epoch 11/20 | Train Loss = 0.9124; Train Accuracy = 67.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  55%|█████████████████████████▎                    | 11/20 [11:53<10:10, 67.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 11 | Validation Loss: 1.3885; Validation Accuracy: 55.39%\n",
      "\n",
      "Epoch 12/20 | Train Loss = 0.8832; Train Accuracy = 69.35%\n",
      "Epoch 12/20 | Train Loss = 0.8457; Train Accuracy = 70.79%\n",
      "Epoch 12/20 | Train Loss = 0.8328; Train Accuracy = 71.49%\n",
      "Epoch 12/20 | Train Loss = 0.8546; Train Accuracy = 70.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  60%|███████████████████████████▌                  | 12/20 [13:14<09:35, 71.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 12 | Validation Loss: 1.2670; Validation Accuracy: 57.36%\n",
      "\n",
      "Epoch 13/20 | Train Loss = 0.7989; Train Accuracy = 72.56%\n",
      "Epoch 13/20 | Train Loss = 0.8110; Train Accuracy = 72.42%\n",
      "Epoch 13/20 | Train Loss = 0.8087; Train Accuracy = 72.15%\n",
      "Epoch 13/20 | Train Loss = 0.8302; Train Accuracy = 70.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  65%|█████████████████████████████▉                | 13/20 [14:24<08:17, 71.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 13 | Validation Loss: 1.2117; Validation Accuracy: 59.58%\n",
      "\n",
      "Epoch 14/20 | Train Loss = 0.7609; Train Accuracy = 74.15%\n",
      "Epoch 14/20 | Train Loss = 0.7738; Train Accuracy = 73.40%\n",
      "Epoch 14/20 | Train Loss = 0.7846; Train Accuracy = 72.72%\n",
      "Epoch 14/20 | Train Loss = 0.8160; Train Accuracy = 71.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  70%|████████████████████████████████▏             | 14/20 [15:43<07:20, 73.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 14 | Validation Loss: 1.3631; Validation Accuracy: 57.61%\n",
      "\n",
      "Epoch 15/20 | Train Loss = 0.7980; Train Accuracy = 72.47%\n",
      "Epoch 15/20 | Train Loss = 0.7817; Train Accuracy = 73.08%\n",
      "Epoch 15/20 | Train Loss = 0.7605; Train Accuracy = 73.69%\n",
      "Epoch 15/20 | Train Loss = 0.7815; Train Accuracy = 72.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  75%|██████████████████████████████████▌           | 15/20 [17:00<06:12, 74.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 15 | Validation Loss: 1.3877; Validation Accuracy: 56.31%\n",
      "\n",
      "Epoch 16/20 | Train Loss = 0.7768; Train Accuracy = 72.77%\n",
      "Epoch 16/20 | Train Loss = 0.7235; Train Accuracy = 75.31%\n",
      "Epoch 16/20 | Train Loss = 0.7447; Train Accuracy = 73.85%\n",
      "Epoch 16/20 | Train Loss = 0.7664; Train Accuracy = 73.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  80%|████████████████████████████████████▊         | 16/20 [17:59<04:40, 70.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 16 | Validation Loss: 1.2505; Validation Accuracy: 59.67%\n",
      "\n",
      "Epoch 17/20 | Train Loss = 0.7262; Train Accuracy = 75.25%\n",
      "Epoch 17/20 | Train Loss = 0.7018; Train Accuracy = 76.20%\n",
      "Epoch 17/20 | Train Loss = 0.7138; Train Accuracy = 75.29%\n",
      "Epoch 17/20 | Train Loss = 0.7214; Train Accuracy = 74.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  85%|███████████████████████████████████████       | 17/20 [19:03<03:24, 68.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 17 | Validation Loss: 1.2408; Validation Accuracy: 59.5%\n",
      "\n",
      "Epoch 18/20 | Train Loss = 0.6914; Train Accuracy = 76.01%\n",
      "Epoch 18/20 | Train Loss = 0.6958; Train Accuracy = 75.71%\n",
      "Epoch 18/20 | Train Loss = 0.6888; Train Accuracy = 76.30%\n",
      "Epoch 18/20 | Train Loss = 0.7045; Train Accuracy = 75.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  90%|█████████████████████████████████████████▍    | 18/20 [20:23<02:23, 71.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 18 | Validation Loss: 1.2881; Validation Accuracy: 59.63%\n",
      "\n",
      "Epoch 19/20 | Train Loss = 0.6806; Train Accuracy = 76.41%\n",
      "Epoch 19/20 | Train Loss = 0.6670; Train Accuracy = 76.31%\n",
      "Epoch 19/20 | Train Loss = 0.6932; Train Accuracy = 76.20%\n",
      "Epoch 19/20 | Train Loss = 0.7129; Train Accuracy = 76.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  95%|███████████████████████████████████████████▋  | 19/20 [21:23<01:08, 68.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 19 | Validation Loss: 1.3621; Validation Accuracy: 56.84%\n",
      "\n",
      "Epoch 20/20 | Train Loss = 0.7027; Train Accuracy = 75.25%\n",
      "Epoch 20/20 | Train Loss = 0.6842; Train Accuracy = 76.00%\n",
      "Epoch 20/20 | Train Loss = 0.6806; Train Accuracy = 76.11%\n",
      "Epoch 20/20 | Train Loss = 0.6914; Train Accuracy = 75.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████████████████████████████████████████| 20/20 [22:31<00:00, 67.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End of epoch 20 | Validation Loss: 1.3112; Validation Accuracy: 58.73%\n",
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing completed! | Test Loss: 1.3529; Test Accuracy = 57.97%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>Test Accuracy</td><td>▁</td></tr><tr><td>Test Loss</td><td>▁</td></tr><tr><td>Train Accuracy</td><td>▁▃▄▄▅▆▆▆▆▇▆▇▇▇▇▇████</td></tr><tr><td>Train Loss</td><td>█▅▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>Training/Training Accuracy</td><td>▁▃▅▅▅▅▅▅▆▆▆▆▆▇▆▆▆▇▇▇▇▇▇▇▇▇█▇███▇███████▇</td></tr><tr><td>Training/Training Epochs</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>Training/Training Loss</td><td>█▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▃▃▄▆▆▄▅▆▆▆▇█▇▇███▇█</td></tr><tr><td>Validation Loss</td><td>█▅▄▅▂▂▅▄▃▃▃▂▁▃▃▁▁▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>20</td></tr><tr><td>Test Accuracy</td><td>57.97</td></tr><tr><td>Test Loss</td><td>1.35286</td></tr><tr><td>Train Accuracy</td><td>75.3418</td></tr><tr><td>Train Loss</td><td>0.69137</td></tr><tr><td>Training/Training Accuracy</td><td>70.3125</td></tr><tr><td>Training/Training Epochs</td><td>20</td></tr><tr><td>Training/Training Loss</td><td>0.63798</td></tr><tr><td>Validation Accuracy</td><td>58.73</td></tr><tr><td>Validation Loss</td><td>1.3112</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">likely-flower-368</strong> at: <a href='https://wandb.ai/giovancombo/DLA_Lab1_CNN/runs/8psftzzu' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab1_CNN/runs/8psftzzu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240111_021448-8psftzzu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "model_pipeline(project_name = \"DLA_Lab1_CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Rinse and Repeat\n",
    "\n",
    "I will now repeat the verification I did above, but with **Convolutional** Neural Networks.\n",
    "This specific part of the exercise focuses on revealing that **deeper** CNNs *without* residual connections do not always work better, and **even deeper** ones *with* residual connections.\n",
    "\n",
    "**Note**: MNIST is *very* easy to work on (at least up to about 99% accuracy), so I will work on **CIFAR10** from now on.\n",
    "\n",
    "Launching the `model_pipeline` function with its proper configuration allows me to observe the performance of multiple kinds of Convolutional architectures.\n",
    "\n",
    "The focus, here, is on playing with the total **depth** (i.e. the number of layers) of the network, while maintaining the general architecture untouched, in order to show that a **deeper** ConvNet provides better performances, **up to a certain depth (!)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d39e9b",
   "metadata": {},
   "source": [
    "All logs and trackings of my runs are available on Weights & Biases, at [this link](https://wandb.ai/giovancombo/DLA_Lab1_CNN?workspace=user-giovancombo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69b4c9",
   "metadata": {},
   "source": [
    "...Well, as previously said, reaching a very high Validation Accuracy on **MNIST** is *very* easy.\n",
    "Let's try then to train some ConvNets on the **CIFAR10** dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4de2f2-abc5-4f98-9eaf-3497f734a022",
   "metadata": {},
   "source": [
    "-----\n",
    "## Exercise 2: Choose at Least One\n",
    "\n",
    "Let's now deepen our understanding of Deep Networks for visual recognition.\n",
    "\n",
    "+ Firstly, I will find a quantitative answer about *how* and *why* Redidual Networks learn more efficiently than their Convolutional counterparts.\n",
    "+ Secondly, I will become a *network surgeon*, trying to fully-convolutionalize a network by acting on its final layers.\n",
    "+ Thirdly, I will try to implement *Class Activation Maps*, in order to see which parts of an image were the most decisive for its classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07978e8e-9f2e-4949-9699-495af6cb6349",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Explain why Residual Connections are so effective\n",
    "\n",
    "The question *\"Why Residual Networks learn more efficiently than Convolutional Networks?\"* can find an answer by looking at the gradient magnitudes passing through the networks, during backpropagation.\n",
    "\n",
    "`wandb.watch(log = \"all\")` tells *Weights & Biases* to log *gradients* and *parameters*' evolution in all the layers of the network. This functionality is useful to graphically visualize the concept of **Vanishing Gradients**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9edb64",
   "metadata": {},
   "source": [
    "For this exercise, I firstly tried to run a basic *MLP*, and then an *MLP with Residual Connections*. Honestly, at the time, I didn't think that this could be a very clever idea, since I've always seen Residuals been added only on Convolutional Networks, but... I decided to give it a try anyway.\n",
    "\n",
    "As mentioned before, I compared these two architectures by challenging them on their performance over their **depth** (i.t. their number of layers).\n",
    "\n",
    "A basic **10-layer MLP** is seen suffering from Vanishing Gradients, with its accuracy dropping all the way down to 10%, that means picking a class **by chance**.\n",
    "\n",
    "As mentioned in the original [ResNet paper](https://arxiv.org/abs/1512.03385), a higher number of layers leads to not only higher validation loss, but also a *higher training loss*: this means that we are not facing overfitting, but in the \"weird\" behavior that a deeper model shows itself.\n",
    "\n",
    "On the contrary, the **10-layer Residual MLP** performed well, confirming the explanation of ResNet authors: Residual Connections allow a network to go **a lot** deeper (with the only limitation of reaching overfitting).\n",
    "\n",
    "The results can be quantitatively checked by observing the *W&B* logs about gradient magnitudes. The basic **MLP** shows gradients that are very close to zero, meaning that the model is not making any real progress.\n",
    "\n",
    "Conversely, the **Residual MLP** showed gradients that did not vanish nor explode, and progressively diminishing their magnitude during training, meaning that the model is proceeding towards convergence on a (local, hopefully global) optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8b143aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\giova\\__UNI\\Deep Learning Applications\\LABORATORIO\\lab1_CNNs\\wandb\\run-20240106_183457-hg7jr4bz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/giovancombo/DLA_Lab1_CNN/runs/hg7jr4bz' target=\"_blank\">fine-sound-355</a></strong> to <a href='https://wandb.ai/giovancombo/DLA_Lab1_CNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/giovancombo/DLA_Lab1_CNN' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab1_CNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/giovancombo/DLA_Lab1_CNN/runs/hg7jr4bz' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab1_CNN/runs/hg7jr4bz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: ./model/CNN/cnn-ep1-lr0.0005-bs2048-depth1\n",
      "CNN(\n",
      "  (act): ReLU()\n",
      "  (convlayers): Sequential(\n",
      "    (0): ConvBlock(\n",
      "      (conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Dropout(p=0.4, inplace=False)\n",
      "    (1): Linear(in_features=50176, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 502538\n",
      "\n",
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0005\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Training Loss after 20480 examples: 0.9132\n",
      "Epoch 1/1 | Training Loss after 40960 examples: 0.4682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:40<00:00, 40.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1 | Validation Accuracy: 89.55%; Validation Loss: 0.4045\n",
      "\n",
      "End of TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of TESTING | Accuracy on the 10000 test images: 89.98%; Test Loss: 0.3920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁</td></tr><tr><td>Test Accuracy</td><td>▁</td></tr><tr><td>Test Loss</td><td>▁</td></tr><tr><td>Train Accuracy</td><td>▁</td></tr><tr><td>Train Loss</td><td>▁</td></tr><tr><td>Training/Training Examples</td><td>▁█</td></tr><tr><td>Training/Training Loss</td><td>█▁</td></tr><tr><td>Validation Accuracy</td><td>▁</td></tr><tr><td>Validation Loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1</td></tr><tr><td>Test Accuracy</td><td>89.98</td></tr><tr><td>Test Loss</td><td>0.39204</td></tr><tr><td>Train Accuracy</td><td>74.79411</td></tr><tr><td>Train Loss</td><td>1.02313</td></tr><tr><td>Training/Training Examples</td><td>40960</td></tr><tr><td>Training/Training Loss</td><td>0.46818</td></tr><tr><td>Validation Accuracy</td><td>89.55</td></tr><tr><td>Validation Loss</td><td>0.40451</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fine-sound-355</strong> at: <a href='https://wandb.ai/giovancombo/DLA_Lab1_CNN/runs/hg7jr4bz' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab1_CNN/runs/hg7jr4bz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_183457-hg7jr4bz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_model = model_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a4d0fb",
   "metadata": {},
   "source": [
    "The same behaviour can be detected while working on ConvNets and their Residual versions (check gradients on *W&B*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a3a7b-2ed6-4f58-a1b7-5ab1fc432893",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Fully-convolutionalize a network.\n",
    "\n",
    "I decided to save the best model trained so far, the **ResidualCNN** with (..config), and **fully-convolutionalize** it. That is, turn it into a network that can predict classification outputs at *all* pixels in an input image.\n",
    "\n",
    "One goal of this eercise is trying to turn this into a **detector** of handwritten digits.\n",
    "\n",
    "**Hint**: To test my fully-convolutionalized network, I might need to write some functions to take random MNIST samples and embed them into a larger image (i.e. in a regular grid or at random positions), in order to create examples on which train the network at *detecting* digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.FullyCNN(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83455667",
   "metadata": {},
   "source": [
    "(Mostrare Plots di 3/4 immagini con detection effettuata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7955b",
   "metadata": {},
   "source": [
    "The ConvNets built in the previous exercise have a global Average Pooling layer and a Fully Connected Layer at the end, in order to merge all infro from the convolutions in a single prediction for all the image, on the 10 MNIST/CIFAR10 classes.\n",
    "\n",
    "In a Fully Convolutional Network, we need instead to produce a prediction for every single one of the 28x28 (32x32) pixels of an image. I then proceed to do a \"network surgery\", removing the two layers mentioned above and rearranging the net to have the dimension of the input image as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8b81a82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('convlayers.0.conv.weight', tensor([[[[-0.1532,  0.0462,  0.2222],\n",
      "          [ 0.2187,  0.0905,  0.2887],\n",
      "          [ 0.0404, -0.1038,  0.0551]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0987,  0.1163,  0.2752],\n",
      "          [-0.0669, -0.1039, -0.0536],\n",
      "          [-0.1642, -0.0849,  0.0847]]],\n",
      "\n",
      "\n",
      "        [[[-0.0128,  0.1220, -0.0878],\n",
      "          [-0.0406, -0.1903,  0.1247],\n",
      "          [ 0.2825,  0.0905,  0.3247]]],\n",
      "\n",
      "\n",
      "        [[[-0.0213, -0.3222, -0.0479],\n",
      "          [-0.0455,  0.2389,  0.1231],\n",
      "          [-0.1706, -0.3178, -0.0145]]],\n",
      "\n",
      "\n",
      "        [[[-0.1670, -0.2437, -0.1241],\n",
      "          [ 0.1900, -0.1588,  0.2728],\n",
      "          [ 0.1291,  0.2112, -0.0372]]],\n",
      "\n",
      "\n",
      "        [[[-0.0977,  0.3187,  0.2365],\n",
      "          [-0.0891,  0.0213, -0.0519],\n",
      "          [ 0.2691,  0.1165, -0.0896]]],\n",
      "\n",
      "\n",
      "        [[[-0.1329, -0.3168,  0.1288],\n",
      "          [-0.2491,  0.2422,  0.2785],\n",
      "          [ 0.2875, -0.2085, -0.2786]]],\n",
      "\n",
      "\n",
      "        [[[-0.2725, -0.1047, -0.0910],\n",
      "          [-0.2720,  0.0342, -0.1131],\n",
      "          [ 0.0675, -0.2655,  0.0281]]],\n",
      "\n",
      "\n",
      "        [[[-0.0784,  0.1257, -0.2624],\n",
      "          [-0.1630, -0.0197, -0.0526],\n",
      "          [ 0.2167,  0.0897, -0.1223]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1699, -0.1433,  0.0732],\n",
      "          [-0.2923, -0.0433,  0.0059],\n",
      "          [-0.0505, -0.3120,  0.1238]]],\n",
      "\n",
      "\n",
      "        [[[-0.2694,  0.2272, -0.1046],\n",
      "          [-0.0314,  0.3317,  0.0315],\n",
      "          [-0.1957, -0.0867,  0.0765]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0365, -0.0133, -0.3103],\n",
      "          [-0.1376,  0.1208,  0.3058],\n",
      "          [-0.3198, -0.1807,  0.3068]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2023, -0.1953, -0.0185],\n",
      "          [-0.0200,  0.1804,  0.1272],\n",
      "          [-0.1707,  0.2959, -0.1468]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0984, -0.1831,  0.1101],\n",
      "          [ 0.2178,  0.1076, -0.0837],\n",
      "          [-0.1546, -0.1047,  0.0038]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2615,  0.0459, -0.1432],\n",
      "          [ 0.3190, -0.0678,  0.1983],\n",
      "          [-0.3102,  0.0506,  0.0183]]],\n",
      "\n",
      "\n",
      "        [[[-0.0759,  0.3038, -0.2057],\n",
      "          [-0.2345, -0.0023, -0.2655],\n",
      "          [ 0.1789,  0.2717, -0.1484]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2137,  0.1572,  0.2373],\n",
      "          [-0.2245,  0.1178, -0.0389],\n",
      "          [ 0.0169,  0.2533, -0.1456]]],\n",
      "\n",
      "\n",
      "        [[[-0.0802,  0.0920, -0.0158],\n",
      "          [ 0.0928, -0.0297, -0.1018],\n",
      "          [-0.1762, -0.2452,  0.0378]]],\n",
      "\n",
      "\n",
      "        [[[-0.2812, -0.0525,  0.0346],\n",
      "          [-0.0109,  0.0630,  0.2839],\n",
      "          [-0.1371, -0.2854, -0.0737]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1527,  0.1241,  0.1512],\n",
      "          [ 0.2650, -0.0772, -0.0136],\n",
      "          [ 0.2008,  0.1829, -0.1261]]],\n",
      "\n",
      "\n",
      "        [[[-0.0108,  0.0172, -0.2415],\n",
      "          [-0.3149,  0.2853, -0.1539],\n",
      "          [-0.0293,  0.0946, -0.3220]]],\n",
      "\n",
      "\n",
      "        [[[-0.0911, -0.0112,  0.2303],\n",
      "          [-0.2947,  0.2248,  0.0260],\n",
      "          [ 0.1806, -0.1761, -0.1666]]],\n",
      "\n",
      "\n",
      "        [[[-0.1817, -0.2252,  0.1918],\n",
      "          [ 0.1141, -0.0811, -0.1587],\n",
      "          [-0.1837,  0.1009,  0.2490]]],\n",
      "\n",
      "\n",
      "        [[[-0.2066,  0.0018,  0.0900],\n",
      "          [-0.0101,  0.1485, -0.2880],\n",
      "          [ 0.2447, -0.1271,  0.2697]]],\n",
      "\n",
      "\n",
      "        [[[-0.1272,  0.0284, -0.3138],\n",
      "          [-0.3331,  0.3105, -0.0007],\n",
      "          [-0.1650, -0.2630, -0.0839]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2761, -0.2759,  0.1330],\n",
      "          [ 0.0910, -0.3094, -0.2081],\n",
      "          [-0.2986,  0.0739, -0.0823]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0872,  0.0746, -0.0048],\n",
      "          [-0.1448, -0.2029, -0.1401],\n",
      "          [-0.0283, -0.0317, -0.1588]]],\n",
      "\n",
      "\n",
      "        [[[-0.1052,  0.2494, -0.1077],\n",
      "          [-0.1194,  0.1901, -0.1870],\n",
      "          [ 0.0087,  0.2962, -0.2118]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0174, -0.1576,  0.1684],\n",
      "          [ 0.1692, -0.0744, -0.0758],\n",
      "          [ 0.0193,  0.1484,  0.1106]]],\n",
      "\n",
      "\n",
      "        [[[-0.0170, -0.0628, -0.0165],\n",
      "          [ 0.2107,  0.1630, -0.3148],\n",
      "          [ 0.1348, -0.1438,  0.1467]]],\n",
      "\n",
      "\n",
      "        [[[-0.0381, -0.2420, -0.2938],\n",
      "          [ 0.0139, -0.2229, -0.0236],\n",
      "          [-0.0164,  0.1552,  0.2645]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1864, -0.2715, -0.1714],\n",
      "          [-0.0791,  0.2576,  0.0965],\n",
      "          [-0.0925, -0.1024, -0.0953]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2223, -0.2900,  0.2050],\n",
      "          [ 0.1454,  0.3217, -0.0232],\n",
      "          [-0.1520,  0.1969,  0.0938]]],\n",
      "\n",
      "\n",
      "        [[[-0.2123,  0.0560,  0.1354],\n",
      "          [ 0.2976,  0.0842, -0.0596],\n",
      "          [-0.3151, -0.0169,  0.0450]]],\n",
      "\n",
      "\n",
      "        [[[-0.1277,  0.0032,  0.1504],\n",
      "          [-0.3266,  0.0826,  0.2564],\n",
      "          [ 0.0060,  0.1735,  0.2566]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1234, -0.0891, -0.2081],\n",
      "          [ 0.0887, -0.0053, -0.1864],\n",
      "          [-0.0322, -0.3168, -0.2227]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3293,  0.2695, -0.0248],\n",
      "          [ 0.0490,  0.0531,  0.0435],\n",
      "          [ 0.3174, -0.0654, -0.2453]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1343,  0.0342,  0.2095],\n",
      "          [ 0.2833,  0.0988, -0.3233],\n",
      "          [-0.1128,  0.3211, -0.1830]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0638,  0.1890, -0.1041],\n",
      "          [-0.1821, -0.0230,  0.0435],\n",
      "          [ 0.1338, -0.1008,  0.1624]]],\n",
      "\n",
      "\n",
      "        [[[-0.1902,  0.2053, -0.0464],\n",
      "          [ 0.1939,  0.1205,  0.1909],\n",
      "          [-0.1214,  0.2828, -0.0078]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1679,  0.0751, -0.0440],\n",
      "          [-0.0886,  0.0130,  0.0753],\n",
      "          [ 0.1822, -0.0950, -0.2669]]],\n",
      "\n",
      "\n",
      "        [[[-0.1651,  0.1422, -0.0347],\n",
      "          [-0.2493, -0.1249, -0.0206],\n",
      "          [ 0.0955,  0.2063,  0.2600]]],\n",
      "\n",
      "\n",
      "        [[[-0.2746, -0.2977, -0.2223],\n",
      "          [ 0.2576, -0.1516,  0.2436],\n",
      "          [ 0.0605, -0.2555, -0.1537]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0369, -0.2552, -0.1512],\n",
      "          [-0.2715,  0.1766,  0.0186],\n",
      "          [ 0.0241, -0.3136, -0.2106]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3260, -0.0047, -0.0178],\n",
      "          [-0.1412, -0.2245, -0.0078],\n",
      "          [-0.0572,  0.1425, -0.0388]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0554, -0.1901, -0.2918],\n",
      "          [-0.1617, -0.1765, -0.2327],\n",
      "          [-0.2132, -0.0677,  0.2562]]],\n",
      "\n",
      "\n",
      "        [[[-0.1052,  0.1828,  0.2439],\n",
      "          [-0.0861, -0.1735, -0.2358],\n",
      "          [-0.0487, -0.1950, -0.2896]]],\n",
      "\n",
      "\n",
      "        [[[-0.0595,  0.2504, -0.2573],\n",
      "          [-0.1249,  0.1815, -0.1128],\n",
      "          [-0.0469,  0.1615,  0.0515]]],\n",
      "\n",
      "\n",
      "        [[[-0.2258,  0.1329, -0.1029],\n",
      "          [-0.2985, -0.2660, -0.2886],\n",
      "          [-0.2542,  0.1153,  0.0996]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1488,  0.2251,  0.3083],\n",
      "          [-0.2509,  0.2653, -0.1428],\n",
      "          [ 0.1028, -0.2850, -0.0318]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2765, -0.2993,  0.0708],\n",
      "          [ 0.2016,  0.2958,  0.1448],\n",
      "          [ 0.1659,  0.2232,  0.0222]]],\n",
      "\n",
      "\n",
      "        [[[-0.1793, -0.3142, -0.1617],\n",
      "          [-0.1014,  0.0742,  0.2746],\n",
      "          [-0.2986,  0.1626,  0.1555]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2994,  0.0896,  0.0029],\n",
      "          [ 0.2979,  0.1591, -0.2860],\n",
      "          [-0.2759, -0.0601, -0.2581]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1235, -0.0173,  0.2403],\n",
      "          [-0.2605,  0.0365,  0.0924],\n",
      "          [ 0.2550,  0.2854,  0.3265]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1736,  0.3110, -0.3008],\n",
      "          [-0.1194, -0.2117, -0.1493],\n",
      "          [ 0.2586,  0.0294,  0.1330]]],\n",
      "\n",
      "\n",
      "        [[[-0.0303,  0.1393, -0.3219],\n",
      "          [ 0.1343,  0.0159, -0.2718],\n",
      "          [-0.0825,  0.3160,  0.1256]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1489, -0.3373,  0.2083],\n",
      "          [ 0.1795, -0.1694, -0.1809],\n",
      "          [ 0.2416,  0.1628, -0.2717]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0927,  0.0682, -0.0010],\n",
      "          [ 0.0760,  0.3182,  0.2676],\n",
      "          [ 0.2848,  0.0874, -0.0943]]],\n",
      "\n",
      "\n",
      "        [[[-0.2481, -0.2858,  0.1089],\n",
      "          [-0.1361,  0.1876, -0.2585],\n",
      "          [-0.1428, -0.0978, -0.1624]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0529, -0.2371,  0.0134],\n",
      "          [ 0.3024, -0.2567, -0.2311],\n",
      "          [ 0.1308,  0.2260,  0.0990]]],\n",
      "\n",
      "\n",
      "        [[[-0.2217,  0.2805, -0.3096],\n",
      "          [-0.3213, -0.2148, -0.1947],\n",
      "          [-0.3090,  0.2619,  0.3069]]],\n",
      "\n",
      "\n",
      "        [[[-0.2559, -0.0536, -0.1761],\n",
      "          [-0.2224,  0.1918, -0.0852],\n",
      "          [ 0.3015,  0.0452,  0.2590]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0367, -0.3374,  0.2520],\n",
      "          [ 0.1297, -0.0707, -0.2458],\n",
      "          [ 0.2732, -0.1000,  0.2788]]],\n",
      "\n",
      "\n",
      "        [[[-0.1941,  0.2066,  0.0692],\n",
      "          [ 0.0328,  0.3368, -0.2814],\n",
      "          [-0.2733, -0.1597, -0.1821]]]], device='cuda:0')), ('convlayers.0.conv.bias', tensor([-2.4026e-01, -3.2033e-01,  2.1025e-01,  6.4438e-02,  1.8214e-02,\n",
      "        -1.3674e-01, -1.4899e-01, -2.3119e-01,  2.9582e-01, -1.0196e-05,\n",
      "         2.5698e-01, -2.8068e-01, -1.9352e-01, -2.8159e-01,  2.3189e-01,\n",
      "         7.5961e-02,  2.0176e-01,  1.3375e-01, -2.6266e-01,  1.9154e-01,\n",
      "        -2.4955e-01, -1.9473e-01, -2.6159e-01,  5.7933e-02,  6.2724e-02,\n",
      "        -5.8584e-02, -2.5407e-01, -2.9544e-01, -2.5651e-01, -1.3582e-01,\n",
      "         2.1792e-01,  2.6642e-01, -1.5244e-01, -1.1293e-01,  2.5383e-02,\n",
      "         9.4476e-02, -1.9276e-01,  2.4116e-01, -4.1354e-02, -2.2264e-02,\n",
      "        -1.6965e-01,  2.4241e-01,  1.7635e-01, -1.8370e-01, -6.7058e-02,\n",
      "        -2.8052e-02,  1.0630e-05, -2.9858e-01, -1.2954e-01,  2.1339e-02,\n",
      "         9.7985e-02, -2.5127e-01,  1.6055e-01,  1.2408e-01, -2.1194e-01,\n",
      "         2.2804e-01, -2.8993e-01,  1.7882e-01, -3.8736e-04, -5.6883e-02,\n",
      "        -1.2159e-01,  1.2123e-03,  2.8100e-02,  2.4990e-01], device='cuda:0')), ('convlayers.0.bn.weight', tensor([0.9920, 0.9977, 0.9957, 0.9938, 0.9963, 0.9928, 0.9938, 0.9926, 0.9933,\n",
      "        0.9930, 0.9922, 0.9919, 0.9925, 0.9921, 0.9936, 0.9927, 0.9913, 0.9919,\n",
      "        0.9937, 0.9921, 0.9926, 0.9919, 0.9916, 0.9934, 0.9929, 0.9919, 0.9938,\n",
      "        0.9985, 0.9941, 0.9917, 0.9934, 0.9939, 0.9960, 0.9915, 0.9955, 0.9943,\n",
      "        0.9942, 0.9937, 0.9934, 0.9915, 0.9917, 0.9939, 0.9939, 0.9925, 0.9918,\n",
      "        0.9938, 0.9945, 0.9948, 0.9940, 0.9955, 0.9958, 0.9924, 0.9907, 0.9946,\n",
      "        0.9931, 0.9949, 0.9962, 0.9921, 0.9921, 0.9958, 0.9941, 0.9953, 0.9929,\n",
      "        0.9934], device='cuda:0')), ('convlayers.0.bn.bias', tensor([-0.0081, -0.0087, -0.0066, -0.0073, -0.0082, -0.0068, -0.0063, -0.0074,\n",
      "        -0.0072, -0.0069, -0.0070, -0.0081, -0.0075, -0.0059, -0.0070, -0.0061,\n",
      "        -0.0081, -0.0080, -0.0064, -0.0077, -0.0072, -0.0080, -0.0077, -0.0078,\n",
      "        -0.0071, -0.0081, -0.0062, -0.0062, -0.0056, -0.0093, -0.0060, -0.0060,\n",
      "        -0.0040, -0.0064, -0.0068, -0.0057, -0.0070, -0.0061, -0.0069, -0.0082,\n",
      "        -0.0069, -0.0079, -0.0059, -0.0076, -0.0061, -0.0062, -0.0059, -0.0069,\n",
      "        -0.0059, -0.0052, -0.0051, -0.0072, -0.0080, -0.0055, -0.0079, -0.0086,\n",
      "        -0.0078, -0.0072, -0.0078, -0.0072, -0.0061, -0.0069, -0.0087, -0.0067],\n",
      "       device='cuda:0')), ('convlayers.0.bn.running_mean', tensor([-0.2162, -0.2939,  0.2166,  0.0489,  0.0213, -0.1180, -0.1489, -0.2357,\n",
      "         0.2714, -0.0018,  0.2305, -0.2726, -0.1840, -0.2658,  0.2228,  0.0682,\n",
      "         0.2017,  0.1193, -0.2609,  0.1993, -0.2566, -0.1867, -0.2469,  0.0627,\n",
      "         0.0355, -0.0615, -0.2458, -0.2860, -0.2321, -0.1270,  0.2023,  0.2413,\n",
      "        -0.1350, -0.1141,  0.0365,  0.0752, -0.1684,  0.2333, -0.0355, -0.0223,\n",
      "        -0.1607,  0.2343,  0.1508, -0.1934, -0.0604, -0.0436, -0.0106, -0.2866,\n",
      "        -0.1415,  0.0316,  0.1125, -0.2499,  0.1467,  0.1457, -0.1919,  0.2116,\n",
      "        -0.2660,  0.1838, -0.0205, -0.0472, -0.1307,  0.0029,  0.0452,  0.2168],\n",
      "       device='cuda:0')), ('convlayers.0.bn.running_var', tensor([0.4670, 0.1675, 0.3480, 0.3021, 0.2148, 0.3955, 0.1945, 0.6971, 0.1681,\n",
      "        0.3093, 0.1424, 0.2629, 0.1519, 0.1031, 0.2405, 0.1807, 0.3528, 0.2205,\n",
      "        0.2795, 0.5985, 0.3395, 0.1374, 0.1871, 0.1338, 0.5798, 0.4385, 0.3313,\n",
      "        0.1400, 0.1454, 0.1457, 0.3954, 0.1389, 0.4237, 0.1166, 0.5023, 0.6903,\n",
      "        0.6278, 0.3482, 0.1028, 0.4178, 0.1472, 0.2186, 0.5158, 0.6089, 0.1211,\n",
      "        0.8991, 0.5495, 0.1166, 0.9167, 0.3021, 0.8909, 0.4280, 0.4234, 0.8576,\n",
      "        0.1836, 0.1860, 0.2112, 0.9258, 0.6872, 0.2044, 0.6192, 0.2778, 0.1591,\n",
      "        0.2807], device='cuda:0')), ('convlayers.0.bn.num_batches_tracked', tensor(25, device='cuda:0')), ('fc.1.weight', tensor([[ 1.0288e-06,  1.8520e-04, -2.2291e-04,  ..., -1.1322e-04,\n",
      "         -1.6999e-04,  1.0416e-04],\n",
      "        [-4.4365e-04,  4.2344e-05, -5.5653e-04,  ...,  4.4458e-04,\n",
      "         -2.7837e-04,  2.9045e-04],\n",
      "        [ 5.5586e-04,  4.7588e-04,  2.1488e-04,  ..., -4.7719e-04,\n",
      "         -2.1941e-04, -3.3551e-05],\n",
      "        ...,\n",
      "        [ 4.3331e-04,  4.9243e-04, -8.0520e-06,  ...,  2.1834e-04,\n",
      "         -6.4458e-05, -1.8358e-04],\n",
      "        [ 9.3185e-06,  5.9485e-05,  1.9908e-04,  ..., -1.4890e-04,\n",
      "          1.5964e-05, -5.9347e-05],\n",
      "        [ 4.2542e-04, -2.0558e-04, -3.0223e-04,  ..., -4.1685e-05,\n",
      "          1.2672e-04, -2.0986e-04]], device='cuda:0')), ('fc.1.bias', tensor([ 2.2627e-03, -2.2987e-03,  3.7876e-03,  3.3772e-03,  1.8774e-03,\n",
      "        -3.3033e-03, -7.5331e-04, -1.3713e-03, -2.9212e-03, -1.2193e-05],\n",
      "       device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "# FINE-TUNING DI UNA CNN CHE ADDESTRERO' IO --> LEVO GLI ULTIMI 2 LAYER E CI METTO LA CONVTRANSPOSE\n",
    "\n",
    "# Loading a model in order to use one of those already trained in the exercises before\n",
    "pt_model = torch.load(\".\\model\\CNN\\cnn-ep1-lr0.0005-bs2048-depth1.pt\")\n",
    "print(pt_model)\n",
    "\n",
    "classes = 10\n",
    "use_bn = True\n",
    "activation = \"ReLU\"\n",
    "# pt_model.head = nn.Sequential(ConvBlock(num_features, classes, 1, 1, 0, use_bn),\n",
    "#                                     getattr(nn, activation)(),\n",
    "#                                     nn.ConvTranspose2d(classes, classes, 5, 4, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243f811-8227-4c6f-b07f-56e8cd91643a",
   "metadata": {},
   "source": [
    "### Exercise 2.3: *Explain* the predictions of a CNN\n",
    "\n",
    "In order to predict the correct class of an image, a ConvNet exploits its \"hierarchical\" architecture to create feature maps at different layers of abstraction of information.\n",
    "\n",
    "The composition of every bit of information extracted determines the whole set of details and peculiarities of an image that links it to a specific class.\n",
    "\n",
    "A lot of work has been done in recent years to try to look inside the black box, and find a way to quantitatively *explain* how a prediction was made. One of these ways is to implement [*Class Activation Maps*](http://cnnlocalization.csail.mit.edu/#:~:text=A%20class%20activation%20map%20for,decision%20made%20by%20the%20CNN.):\n",
    "\n",
    "> B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR'16 (arXiv:1512.04150, 2015).\n",
    "\n",
    "Let's demonstrate how my trained CNN *attends* to specific image features to recognize *specific* classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1893c",
   "metadata": {},
   "source": [
    "For this task, I decided to borrow the code from this source (link), in order to try to apply CAMs to some CIFAR10 images.\n",
    "\n",
    "Moreover, as a passionate photographer, since we're talking about images, I *HAD* to try to create CAMs of some of my favourite photographs. Here are some visual results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# 10 classes of CIFAR10\n",
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "cifar = True  # xxx else i have an high quality truck image\n",
    "if cifar:\n",
    "    image_idx = 18                          # indice dell'immagine da testare\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "    test_set = torchvision.datasets.CIFAR10(root = './data', train = False, download = True, transform = transform)\n",
    "    image, label = test_set[image_idx]\n",
    "    pil_image = TF.to_pil_image(image)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(pil_image)\n",
    "    plt.show()\n",
    "\n",
    "    # Save the image\n",
    "    image_file = 'images/cifar_' + str(classes[label]) + '.jpg'\n",
    "    pil_image.save(image_file)\n",
    "    print(\"real label:\", classes[label])\n",
    "else:\n",
    "    image_file = 'images/hd_truck.jpg'\n",
    "    print(\"using hd image:\", image_file)\n",
    "\n",
    "# networks such as googlenet, resnet, densenet already use global average pooling at the end, so CAM could be used directly.\n",
    "\n",
    "finalconv_name = \"features\"\n",
    "# net = torch.load(\"./model/resnet_cnn-ep5-lr0.001-bs512-depth5-residual.pt\")\n",
    "net = torch.load(\"./model/resnet_to_convergence/cnn-ep5-lr0.004-bs64-depth25-residual.pt\")\n",
    "print(net)\n",
    "net.eval()\n",
    "\n",
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "net._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "# get the softmax weight\n",
    "params = list(net.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "# normalize = transforms.Normalize(\n",
    "#     mean=[0.485, 0.456, 0.406],\n",
    "#     std=[0.229, 0.224, 0.225]\n",
    "# )\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    # normalize\n",
    "])\n",
    "\n",
    "# load test image\n",
    "img_pil = Image.open(image_file)\n",
    "img_tensor = preprocess(img_pil)\n",
    "img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "logit = net(img_variable.to('cuda'))\n",
    "\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "probs, idx = h_x.sort(0, True)\n",
    "probs = probs.cpu().numpy()\n",
    "idx = idx.cpu().numpy()\n",
    "\n",
    "# output the prediction\n",
    "for i in range(0, 10):\n",
    "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "\n",
    "# generate class activation mapping for the top1 prediction\n",
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "# render the CAM and output\n",
    "print('output CAM.jpg for the top1 prediction: %s' % classes[idx[0]])\n",
    "img = cv2.imread(image_file)\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0], (width, height)), cv2.COLORMAP_JET)\n",
    "# result = heatmap * 0.3 + img * 0.5\n",
    "result = heatmap * 0.4 + img * 0.5\n",
    "if cifar:\n",
    "    cv2.imwrite('images/CAM_cifar_' + str(classes[label]) + '_idx' + str(image_idx) + '_probs' + str(probs[0]) + '.jpg',\n",
    "                result)\n",
    "else:\n",
    "    cv2.imwrite('images/CAM_hd_truck_probs' + str(probs[0]) + '.jpg', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "f208204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some functions for plotting the Class Activation Maps\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h * w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "\n",
    "def show_cam(CAMs, width, height, orig_image, class_idx, save_name):\n",
    "    for i, cam in enumerate(CAMs):\n",
    "        heatmap = cv2.applyColorMap(cv2.resize(cam,(width, height)), cv2.COLORMAP_JET)\n",
    "        result = heatmap * 0.5 + orig_image * 0.5\n",
    "        # put class label text on the result\n",
    "        cv2.putText(result, str(int(class_idx[i])), (20, 40), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.imshow('CAM', result/255.)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.imwrite(f\"outputs/CAM_{save_name}.jpg\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for all the images in the `input` folder\n",
    "for image_path in glob.glob('input/*'):\n",
    "    # read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    orig_image = image.copy()\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = np.expand_dims(image, axis=2)\n",
    "    height, width, _ = orig_image.shape\n",
    "    # apply the image transforms\n",
    "    image_tensor = transform(image)\n",
    "    # add batch dimension\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "    # forward pass through model\n",
    "    outputs = model(image_tensor)\n",
    "    # get the softmax probabilities\n",
    "    probs = F.softmax(outputs).data.squeeze()\n",
    "    # get the class indices of top k probabilities\n",
    "    class_idx = topk(probs, 1)[1].int()\n",
    "    \n",
    "    # generate class activation mapping for the top1 prediction\n",
    "    CAMs = returnCAM(features_blobs[0], weight_softmax, class_idx)\n",
    "    # file name to save the resulting CAM image with\n",
    "    save_name = f\"{image_path.split('/')[-1].split('.')[0]}\"\n",
    "    # show and save the results\n",
    "    show_cam(CAMs, width, height, orig_image, class_idx, save_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
