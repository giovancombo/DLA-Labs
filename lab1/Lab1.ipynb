{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d550d73",
   "metadata": {},
   "source": [
    "***Deep Learning Applications 2023** course, held by Professor **Andrew David Bagdanov** - University of Florence, Italy*\n",
    "\n",
    "*Notebook and code created by **Giovanni Colombo** - Mat. 7092745*\n",
    "\n",
    "Check the dedicated [Repository on GitHub](https://github.com/giovancombo/DLA-Labs/tree/main/lab1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f7c5d-46f3-4cbd-80ad-f1e50cd65096",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #1 - CNNs\n",
    "\n",
    "In this first laboratory we will work relatively simple architectures to get a feel for working with Deep Models. This notebook is designed to work with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed8906-bd19-4b4f-8b79-4feae355ffd6",
   "metadata": {},
   "source": [
    "## Exercise 1: Warming Up\n",
    "In this series of exercises I will duplicate (on a small scale) the results of the ResNet paper:\n",
    "\n",
    "> [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385), Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR 2016.\n",
    "\n",
    "I will do this in steps, firstly using a Multilayer Perceptron on MNIST.\n",
    "\n",
    "What's important to recall is that the main message of the ResNet paper is that **deeper networks do not guarantee** more reduction in training loss (or in validation accuracy).\n",
    "Below, I will incrementally build a sequence of experiments to verify this for different architectures, starting with an *MLP*.\n",
    "\n",
    "The Laboratory requires me to compare multiple training runs, so I took this as a great opportunity to learn to use [Weights and Biases](https://wandb.ai/site) for performance monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2cad13-ee2c-4e43-b5c7-31760da8c2df",
   "metadata": {},
   "source": [
    "### Exercise 1.1: A baseline MLP\n",
    "\n",
    "I will now implement a *simple Multilayer Perceptron* to classify the 10 digits of MNIST, and (hopefully) train it to convergence, monitoring Training and Validation losses and accuraces with W&B.\n",
    "\n",
    "The exercise wants me to think in an *abstract* way: I'll have to instantiate multiple models, with different hyperparameters configurations each, and train them on different datasets.\n",
    "It could be a good idea to try to generalize the most possible the instantiation of every object of the training workflow. That's why I decided to try to build a single file `config.yaml`, where I put almost every variable that can help me building any model I want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821224b8",
   "metadata": {},
   "source": [
    "I define then a `load` function, that passes the dictionary `config` (obtained from my `.yaml` file) as an argument, in order to load the dataset we want (between MNIST and CIFAR10), transformed accordingly, and splitted into *Train*, *Validation* and *Test* sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91032c6",
   "metadata": {},
   "source": [
    "The script file `models.py` contains all the model classes used for this Laboratory:\n",
    "+ **MLP**, for instantiating a *Multilayer Perceptron*\n",
    "+ **ResidualMLP**, for instantiating an MLP that implements *Residual Connections*\n",
    "+ **CNN**, for instantiating *Convolutional Network*, with the possibility of tuning almost every possible parameter\n",
    "+ **ResidualCNN**, for instantiating a ConvNet that implements *Residual Connections*\n",
    "+ **ResNet**, for instantiating an actual *ResNet* as defined in the [Paper](https://arxiv.org/abs/1512.03385), in its *[9, 18, 34, 50, 101, 152]* versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8165608",
   "metadata": {},
   "source": [
    "The `build_model` function instantiates Model, Loss Function and Optimizer chosen with the `config` file, and sends it to `device`, that can be `cuda` (in my case, a *Nvidia GeForce RTX 3060 Laptop*) or `cpu`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd52404",
   "metadata": {},
   "source": [
    "Functions for periodical log of Loss and Accuracy from Training and Evaluation phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a89546",
   "metadata": {},
   "source": [
    "The training loop lies in the `train` function, that takes all the objects instantiated in the previous steps and uses them to train the model.\n",
    "\n",
    "The *forward* and *backward* passes are performed batch-wise through the `train_batch` function, that implements a tweak to reshape the input images' sizes accordingly to the model used. Same thing is done in the `validation` and `test` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991571f",
   "metadata": {},
   "source": [
    "The `load`, `build_model`, `train` and `test` functions are all contained in a single function, `model_pipeline`, that allows me to wrap all my workflow into a *Weights & Biases* run more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f38cf7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Weights & Biases run...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\giova\\__UNI\\Deep Learning Applications\\DLA-Labs\\lab1\\wandb\\run-20240214_010303-tws06eg2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/giovancombo/DLA_Lab1_CNN/runs/tws06eg2' target=\"_blank\">gorgeous-etchings-591</a></strong> to <a href='https://wandb.ai/giovancombo/DLA_Lab1_CNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/giovancombo/DLA_Lab1_CNN' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab1_CNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/giovancombo/DLA_Lab1_CNN/runs/tws06eg2' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab1_CNN/runs/tws06eg2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10 loaded with 40000 Train samples, 10000 Validation samples, 10000 Test samples.\n",
      "\n",
      "Model instantiated: CNN\n",
      "Number of parameters: 1176074\n",
      "\n",
      "CNN(\n",
      "  (act): ReLU()\n",
      "  (convlayers): Sequential(\n",
      "    (0): ConvBlock(\n",
      "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ReLU()\n",
      "    (4): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): ReLU()\n",
      "    (6): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): ReLU()\n",
      "    (8): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (9): ReLU()\n",
      "    (10): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (11): ReLU()\n",
      "    (12): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (13): ReLU()\n",
      "    (14): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (15): ReLU()\n",
      "    (16): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (17): ReLU()\n",
      "    (18): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (19): ReLU()\n",
      "    (20): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (21): ReLU()\n",
      "    (22): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (23): ReLU()\n",
      "    (24): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (25): ReLU()\n",
      "    (26): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (27): ReLU()\n",
      "    (28): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (29): ReLU()\n",
      "    (30): Identity()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=65536, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Optimizer: Adam\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "Loss: CrossEntropyLoss()\n",
      "Device: cuda\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|                                                       | 0/20 [00:23<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\giova\\AppData\\Local\\Temp\\ipykernel_37896\\2648403833.py\", line 31, in main\n",
      "    train(model, train_loader, val_loader, criterion, optimizer, device, config)\n",
      "  File \"c:\\Users\\giova\\__UNI\\Deep Learning Applications\\DLA-Labs\\lab1\\pipeline.py\", line 118, in train\n",
      "    loss.backward()\n",
      "  File \"c:\\Users\\giova\\anaconda3\\envs\\DLA\\lib\\site-packages\\torch\\_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"c:\\Users\\giova\\anaconda3\\envs\\DLA\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gorgeous-etchings-591</strong> at: <a href='https://wandb.ai/giovancombo/DLA_Lab1_CNN/runs/tws06eg2' target=\"_blank\">https://wandb.ai/giovancombo/DLA_Lab1_CNN/runs/tws06eg2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240214_010303-tws06eg2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 47\u001b[0m\n\u001b[0;32m     43\u001b[0m             utils\u001b[38;5;241m.\u001b[39msave_model(config, model)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 47\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn [2], line 31\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 3. Training the model\u001b[39;00m\n\u001b[0;32m     30\u001b[0m wandb\u001b[38;5;241m.\u001b[39mwatch(model, criterion, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_freq\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlog_interval)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 4. Evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m     34\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m test(model, test_loader, device, config)\n",
      "File \u001b[1;32mc:\\Users\\giova\\__UNI\\Deep Learning Applications\\DLA-Labs\\lab1\\pipeline.py:118\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, device, config)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m    117\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 118\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    119\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Calculating training accuracy\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\DLA\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giova\\anaconda3\\envs\\DLA\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import wandb\n",
    "\n",
    "from pipeline import *\n",
    "import utils\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    wandb.login()\n",
    "    print(\"Initializing Weights & Biases run...\")\n",
    "\n",
    "    # Loading the configuration file\n",
    "    with open(\"config.yaml\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    # Initializing a wandb run for logging losses, accuracies and gradients\n",
    "    with wandb.init(project = config['project_name'], config = config):\n",
    "        config = wandb.config\n",
    "\n",
    "        # 1. Loading the data\n",
    "        train_loader, val_loader, test_loader = load(config.dataset, config.batch_size)\n",
    "\n",
    "        # 2. Building the model\n",
    "        model, criterion, optimizer = build_model(device, config)\n",
    "\n",
    "        # 3. Training the model\n",
    "        wandb.watch(model, criterion, log=\"all\", log_freq=config.log_interval)\n",
    "        train(model, train_loader, val_loader, criterion, optimizer, device, config)\n",
    "\n",
    "        # 4. Evaluate the model on the test set\n",
    "        test_loss, test_accuracy = test(model, test_loader, device, config)\n",
    "\n",
    "        print(f\"Testing completed! | Test Loss: {test_loss:.4f}; Test Accuracy = {test_accuracy:.2f}%\")\n",
    "        wandb.log({\"Test Loss\": test_loss,\n",
    "                \"Test Accuracy\": test_accuracy})\n",
    "        wandb.unwatch(model)\n",
    "        \n",
    "        # 5. Saving the model, assigning it a name based on the hyperparameters used\n",
    "        if config['save_model']:\n",
    "            utils.save_model(config, model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eb75ad",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Rinse and Repeat\n",
    "\n",
    "I will now repeat the verification I did above, but with **Convolutional** Neural Networks.\n",
    "This specific part of the exercise focuses on revealing that **deeper** CNNs *without* residual connections do not always work better, and **even deeper** ones *with* residual connections.\n",
    "\n",
    "**Note**: MNIST is *very* easy to work on (at least up to about 99% accuracy), so I will work on **CIFAR10** from now on.\n",
    "\n",
    "Launching the `model_pipeline` function with its proper configuration allows me to observe the performance of multiple kinds of Convolutional architectures.\n",
    "\n",
    "The focus, here, is on playing with the total **depth** (i.e. the number of layers) of the network, while maintaining the general architecture untouched, in order to show that a **deeper** ConvNet provides better performances, **up to a certain depth (!)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d39e9b",
   "metadata": {},
   "source": [
    "All logs and trackings of my runs are available on Weights & Biases, at [this link](https://wandb.ai/giovancombo/DLA_Lab1_CNN?workspace=user-giovancombo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c879eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dataset and model settings in config.yaml\n",
    "# config.dataset = [\"MNIST\", \"CIFAR10\"]; config.convnet = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69b4c9",
   "metadata": {},
   "source": [
    "...Well, as previously said, reaching a very high Validation Accuracy on **MNIST** is *very* easy.\n",
    "Let's try then to train some ConvNets on the **CIFAR10** dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4de2f2-abc5-4f98-9eaf-3497f734a022",
   "metadata": {},
   "source": [
    "-----\n",
    "## Exercise 2: Choose at Least One\n",
    "\n",
    "Let's now deepen our understanding of Deep Networks for visual recognition.\n",
    "\n",
    "+ Firstly, I will find a quantitative answer about *how* and *why* Redidual Networks learn more efficiently than their Convolutional counterparts.\n",
    "+ Secondly, I will become a *network surgeon*, trying to fully-convolutionalize a network by acting on its final layers.\n",
    "+ Thirdly, I will try to implement *Class Activation Maps*, in order to see which parts of an image were the most decisive for its classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07978e8e-9f2e-4949-9699-495af6cb6349",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Explain why Residual Connections are so effective\n",
    "\n",
    "The question *\"Why Residual Networks learn more efficiently than Convolutional Networks?\"* can find an answer by looking at the gradient magnitudes passing through the networks, during backpropagation.\n",
    "\n",
    "`wandb.watch(log = \"all\")` tells *Weights & Biases* to log *gradients* and *parameters*' evolution in all the layers of the network. This functionality is useful to graphically visualize the concept of **Vanishing Gradients**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9edb64",
   "metadata": {},
   "source": [
    "For this exercise, I firstly tried to run a basic *MLP*, and then an *MLP with Residual Connections*. Honestly, at the time, I didn't think that this could be a very clever idea, since I've always seen Residuals been added only on Convolutional Networks, but... I decided to give it a try anyway.\n",
    "\n",
    "As mentioned before, I compared these two architectures by challenging them on their performance over their **depth** (i.t. their number of layers).\n",
    "\n",
    "A basic **10-layer MLP** is seen suffering from Vanishing Gradients, with its accuracy dropping all the way down to 10%, that means picking a class **by chance**.\n",
    "\n",
    "As mentioned in the original [ResNet paper](https://arxiv.org/abs/1512.03385), a higher number of layers leads to not only higher validation loss, but also a *higher training loss*: this means that we are not facing overfitting, but in the \"weird\" behavior that a deeper model shows itself.\n",
    "\n",
    "On the contrary, the **10-layer Residual MLP** performed well, confirming the explanation of ResNet authors: Residual Connections allow a network to go **a lot** deeper (with the only limitation of reaching overfitting).\n",
    "\n",
    "The results can be quantitatively checked by observing the *W&B* logs about gradient magnitudes. The basic **MLP** shows gradients that are very close to zero, meaning that the model is not making any real progress.\n",
    "\n",
    "Conversely, the **Residual MLP** showed gradients that did not vanish nor explode, and progressively diminishing their magnitude during training, meaning that the model is proceeding towards convergence on a (local, hopefully global) optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a4d0fb",
   "metadata": {},
   "source": [
    "The same behaviour can be detected while working on ConvNets and their Residual versions (check gradients on *W&B*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a3a7b-2ed6-4f58-a1b7-5ab1fc432893",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Fully-convolutionalize a network.\n",
    "\n",
    "I decided to save the best model trained so far, the **ResidualCNN** with (..config), and **fully-convolutionalize** it. That is, turn it into a network that can predict classification outputs at *all* pixels in an input image.\n",
    "\n",
    "One goal of this eercise is trying to turn this into a **detector** of handwritten digits.\n",
    "\n",
    "**Hint**: To test my fully-convolutionalized network, I might need to write some functions to take random MNIST samples and embed them into a larger image (i.e. in a regular grid or at random positions), in order to create examples on which train the network at *detecting* digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb332bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dataset and model settings in config.yaml\n",
    "# config.dataset = \"CIFAR10\"; config.fullycnn = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83455667",
   "metadata": {},
   "source": [
    "(Mostrare Plots di 3/4 immagini con detection effettuata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7955b",
   "metadata": {},
   "source": [
    "The ConvNets built in the previous exercise have a global Average Pooling layer and a Fully Connected Layer at the end, in order to merge all infro from the convolutions in a single prediction for all the image, on the 10 MNIST/CIFAR10 classes.\n",
    "\n",
    "In a Fully Convolutional Network, we need instead to produce a prediction for every single one of the 28x28 (32x32) pixels of an image. I then proceed to do a \"network surgery\", removing the two layers mentioned above and rearranging the net to have the dimension of the input image as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243f811-8227-4c6f-b07f-56e8cd91643a",
   "metadata": {},
   "source": [
    "### Exercise 2.3: *Explain* the predictions of a CNN\n",
    "\n",
    "In order to predict the correct class of an image, a ConvNet exploits its \"hierarchical\" architecture to create feature maps at different layers of abstraction of information.\n",
    "\n",
    "The composition of every bit of information extracted determines the whole set of details and peculiarities of an image that links it to a specific class.\n",
    "\n",
    "A lot of work has been done in recent years to try to look inside the black box, and find a way to quantitatively *explain* how a prediction was made. One of these ways is to implement [*Class Activation Maps*](http://cnnlocalization.csail.mit.edu/#:~:text=A%20class%20activation%20map%20for,decision%20made%20by%20the%20CNN.):\n",
    "\n",
    "> B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR'16 (arXiv:1512.04150, 2015).\n",
    "\n",
    "Let's demonstrate how my trained CNN *attends* to specific image features to recognize *specific* classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1893c",
   "metadata": {},
   "source": [
    "For this task, I decided to borrow the code from this source (link), in order to try to apply CAMs to some CIFAR10 images.\n",
    "\n",
    "Moreover, as a passionate photographer, since we're talking about images, I *HAD* to try to create CAMs of some of my favourite photographs. Here are some visual results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# model = torch.load(RESIDUALCNN)\n",
    "\n",
    "def cam_test(model, test_loader, epoch):\n",
    "    \n",
    "    classes = ['airplane','bird','car','cat','deer','dog','horse','monkey','ship','truck']\n",
    "\n",
    "    params = [param for param in model.parameters()]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in test_loader:\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            oututs, b_gap, a_gap = model(images)\n",
    "            \n",
    "            _, predicated = torch.max(oututs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicated == labels).sum().item()\n",
    "            \n",
    "            image_labels, image_paths = [], []\n",
    "\n",
    "            for i in range(5):\n",
    "\n",
    "                k = i\n",
    "                vutils.save_image(images[k], f\"Lab1/img/image{i}.jpg\")\n",
    "\n",
    "                # for cam using only the weights of the class predicted\n",
    "                 \n",
    "                #weights = params[-2][predicated[i].item()].detach()\n",
    "                #c = torch.sum(b_gap[k]*weights[:,None, None], dim = 0)\n",
    "\n",
    "                #using global average pooling parameters\n",
    "\n",
    "                c = torch.sum(b_gap[k]*a_gap[k][:,None, None], dim = 0)\n",
    "                \n",
    "                c = (c-torch.min(c))/(torch.max(c)-torch.min(c))\n",
    "            \n",
    "                cam_img = np.uint8(255 * c.cpu().numpy())\n",
    "\n",
    "                hm = cv2.applyColorMap(cv2.resize(cam_img, (96, 96)), cv2.COLORMAP_JET)\n",
    "            \n",
    "                re = hm*0.3+(images[k].permute(1,2,0).cpu().numpy()*255 )*0.4\n",
    "\n",
    "                cv2.imwrite(f\"Lab1/img/CAM{i}.jpg\", re)\n",
    "\n",
    "                image_labels.append(classes[labels[k]]+\"-\"+classes[predicated[k]])\n",
    "                image_paths.append(f\"Lab1/img/CAM{i}.jpg\")\n",
    "\n",
    "            utils.plot_images(image_paths, image_labels, epoch)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c1d0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d0b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# same 10 classes of cifar\n",
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "cifar = True  # xxx else i have an high quality truck image\n",
    "if cifar:\n",
    "    # 1 ship\n",
    "    # 7 frog\n",
    "    # 27 airplane\n",
    "    # 42 dog\n",
    "    image_idx = 18  # indice dell'immagine da testare\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor()])\n",
    "    test_set = CIFAR10(root='./data', train=False,\n",
    "                       download=True, transform=transform)\n",
    "    image, label = test_set[image_idx]\n",
    "    pil_image = TF.to_pil_image(image)\n",
    "    # Display the image\n",
    "    plt.imshow(pil_image)\n",
    "    plt.show()\n",
    "    # Save the image\n",
    "    image_file = 'images/cifar_' + str(classes[label]) + '.jpg'\n",
    "    pil_image.save(image_file)\n",
    "    print(\"real label:\", classes[label])\n",
    "else:\n",
    "    image_file = 'images/hd_truck.jpg'\n",
    "    print(\"using hd image:\", image_file)\n",
    "\n",
    "\n",
    "finalconv_name = \"features\"\n",
    "# net = torch.load(\"./model/resnet_cnn-ep5-lr0.001-bs512-depth5-residual.pt\")\n",
    "net = torch.load(\"./model/resnet_to_convergence/cnn-ep5-lr0.004-bs64-depth25-residual.pt\")\n",
    "print(net)\n",
    "net.eval()\n",
    "\n",
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "\n",
    "\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "\n",
    "net._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "# get the softmax weight\n",
    "params = list(net.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.cpu().numpy())\n",
    "\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h * w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "\n",
    "# normalize = transforms.Normalize(\n",
    "#     mean=[0.485, 0.456, 0.406],\n",
    "#     std=[0.229, 0.224, 0.225]\n",
    "# )\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    # normalize\n",
    "])\n",
    "\n",
    "# load test image\n",
    "img_pil = Image.open(image_file)\n",
    "img_tensor = preprocess(img_pil)\n",
    "img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "logit = net(img_variable.to('cuda'))\n",
    "\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "probs, idx = h_x.sort(0, True)\n",
    "probs = probs.cpu().numpy()\n",
    "idx = idx.cpu().numpy()\n",
    "\n",
    "# output the prediction\n",
    "for i in range(0, 10):\n",
    "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "\n",
    "# generate class activation mapping for the top1 prediction\n",
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "# render the CAM and output\n",
    "print('output CAM.jpg for the top1 prediction: %s' % classes[idx[0]])\n",
    "img = cv2.imread(image_file)\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0], (width, height)), cv2.COLORMAP_JET)\n",
    "# result = heatmap * 0.3 + img * 0.5\n",
    "result = heatmap * 0.4 + img * 0.5\n",
    "if cifar:\n",
    "    cv2.imwrite('images/CAM_cifar_' + str(classes[label]) + '_idx' + str(image_idx) + '_probs' + str(probs[0]) + '.jpg',\n",
    "                result)\n",
    "else:\n",
    "    cv2.imwrite('images/CAM_hd_truck_probs' + str(probs[0]) + '.jpg', result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
