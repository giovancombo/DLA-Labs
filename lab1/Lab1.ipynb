{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d550d73",
   "metadata": {},
   "source": [
    "***Deep Learning Applications 2023** course, held by Professor **Andrew David Bagdanov** - University of Florence, Italy*\n",
    "\n",
    "*Notebook and code created by **Giovanni Colombo** - Mat. 7092745*\n",
    "\n",
    "Check the dedicated [Repository on GitHub](https://github.com/giovancombo/DLA-Labs/tree/main/lab1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f7c5d-46f3-4cbd-80ad-f1e50cd65096",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #1 - CNNs\n",
    "\n",
    "In this first laboratory we will work relatively simple architectures to get a feel for working with Deep Models. This notebook is designed to work with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed8906-bd19-4b4f-8b79-4feae355ffd6",
   "metadata": {},
   "source": [
    "## Exercise 1: Warming Up\n",
    "In this series of exercises I will duplicate (on a small scale) the results of the ResNet paper:\n",
    "\n",
    "> [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385), Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR 2016.\n",
    "\n",
    "I will do this in steps, firstly using a Multilayer Perceptron on MNIST.\n",
    "\n",
    "What's important to recall is that the main message of the ResNet paper is that **deeper networks do not guarantee** more reduction in training loss (or in validation accuracy).\n",
    "Below, I will incrementally build a sequence of experiments to verify this for different architectures, starting with an *MLP*.\n",
    "\n",
    "The Laboratory requires me to compare multiple training runs, so I took this as a great opportunity to learn to use [Weights and Biases](https://wandb.ai/site) for performance monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2cad13-ee2c-4e43-b5c7-31760da8c2df",
   "metadata": {},
   "source": [
    "### Exercise 1.1: A baseline MLP\n",
    "\n",
    "I will now implement a *simple Multilayer Perceptron* to classify the 10 digits of MNIST, and (hopefully) train it to convergence, monitoring Training and Validation losses and accuraces with W&B.\n",
    "\n",
    "The exercise wants me to think in an *abstract* way: I'll have to instantiate multiple models, with different hyperparameters configurations each, and train them on different datasets.\n",
    "It could be a good idea to try to generalize the most possible the instantiation of every object of the training workflow. That's why I decided to try to build a single file `config.yaml`, where I put almost every variable that can help me building any model I want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821224b8",
   "metadata": {},
   "source": [
    "I define then a `load` function, that passes the dictionary `config` (obtained from my `.yaml` file) as an argument, in order to load the dataset we want (between MNIST and CIFAR10), transformed accordingly, and splitted into *Train*, *Validation* and *Test* sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91032c6",
   "metadata": {},
   "source": [
    "The script file `models.py` contains all the model classes used for this Laboratory:\n",
    "+ **MLP**, for instantiating a *Multilayer Perceptron*\n",
    "+ **ResidualMLP**, for instantiating an MLP that implements *Residual Connections*\n",
    "+ **CNN**, for instantiating *Convolutional Network*, with the possibility of tuning almost every possible parameter\n",
    "+ **ResidualCNN**, for instantiating a ConvNet that implements *Residual Connections*\n",
    "+ **ResNet**, for instantiating an actual *ResNet* as defined in the [Paper](https://arxiv.org/abs/1512.03385), in its *[9, 18, 34, 50, 101, 152]* versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8165608",
   "metadata": {},
   "source": [
    "The `build_model` function instantiates Model, Loss Function and Optimizer chosen with the `config` file, and sends it to `device`, that can be `cuda` (in my case, a *Nvidia GeForce RTX 3060 Laptop*) or `cpu`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd52404",
   "metadata": {},
   "source": [
    "Functions for periodical log of Loss and Accuracy from Training and Evaluation phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a89546",
   "metadata": {},
   "source": [
    "The training loop lies in the `train` function, that takes all the objects instantiated in the previous steps and uses them to train the model.\n",
    "\n",
    "The *forward* and *backward* passes are performed batch-wise through the `train_batch` function, that implements a tweak to reshape the input images' sizes accordingly to the model used. Same thing is done in the `validation` and `test` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991571f",
   "metadata": {},
   "source": [
    "The `load`, `build_model`, `train` and `test` functions are all contained in a single function, `model_pipeline`, that allows me to wrap all my workflow into a *Weights & Biases* run more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38cf7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import wandb\n",
    "\n",
    "from pipeline import *\n",
    "import utils\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    wandb.login()\n",
    "    print(\"Initializing Weights & Biases run...\")\n",
    "\n",
    "    # Loading the configuration file\n",
    "    with open(\"config.yaml\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    # Initializing a wandb run for logging losses, accuracies and gradients\n",
    "    with wandb.init(project = config['project_name'], config = config):\n",
    "        config = wandb.config\n",
    "\n",
    "        # 1. Loading the data\n",
    "        train_loader, val_loader, test_loader = load(config.dataset, config.batch_size)\n",
    "\n",
    "        # 2. Building the model\n",
    "        model, criterion, optimizer = build_model(device, config)\n",
    "\n",
    "        # 3. Training the model\n",
    "        train(model, train_loader, val_loader, criterion, optimizer, device, config)\n",
    "\n",
    "        # 4. Evaluate the model on the test set\n",
    "        test_loss, test_accuracy = test(model, test_loader, device, config)\n",
    "\n",
    "        print(f\"Testing completed! | Test Loss: {test_loss:.4f}; Test Accuracy = {test_accuracy:.2f}%\")\n",
    "        wandb.log({\"Test Loss\": test_loss,\n",
    "                \"Test Accuracy\": test_accuracy})\n",
    "        wandb.unwatch(model)\n",
    "        \n",
    "        # 5. Saving the model, assigning it a name based on the hyperparameters used\n",
    "        if config['save_model']:\n",
    "            utils.save_model(config, model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eb75ad",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Rinse and Repeat\n",
    "\n",
    "I will now repeat the verification I did above, but with **Convolutional** Neural Networks.\n",
    "This specific part of the exercise focuses on revealing that **deeper** CNNs *without* residual connections do not always work better, and **even deeper** ones *with* residual connections.\n",
    "\n",
    "**Note**: MNIST is *very* easy to work on (at least up to about 99% accuracy), so I will work on **CIFAR10** from now on.\n",
    "\n",
    "Launching the `model_pipeline` function with its proper configuration allows me to observe the performance of multiple kinds of Convolutional architectures.\n",
    "\n",
    "The focus, here, is on playing with the total **depth** (i.e. the number of layers) of the network, while maintaining the general architecture untouched, in order to show that a **deeper** ConvNet provides better performances, **up to a certain depth (!)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d39e9b",
   "metadata": {},
   "source": [
    "All logs and trackings of my runs are available on Weights & Biases, at [this link](https://wandb.ai/giovancombo/DLA_Lab1_CNN?workspace=user-giovancombo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c879eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dataset and model settings in config.yaml\n",
    "# config.dataset = [\"MNIST\", \"CIFAR10\"]; config.convnet = True\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69b4c9",
   "metadata": {},
   "source": [
    "...Well, as previously said, reaching a very high Validation Accuracy on **MNIST** is *very* easy.\n",
    "Let's try then to train some ConvNets on the **CIFAR10** dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4de2f2-abc5-4f98-9eaf-3497f734a022",
   "metadata": {},
   "source": [
    "-----\n",
    "## Exercise 2: Choose at Least One\n",
    "\n",
    "Let's now deepen our understanding of Deep Networks for visual recognition.\n",
    "\n",
    "+ Firstly, I will find a quantitative answer about *how* and *why* Redidual Networks learn more efficiently than their Convolutional counterparts.\n",
    "+ Secondly, I will become a *network surgeon*, trying to fully-convolutionalize a network by acting on its final layers.\n",
    "+ Thirdly, I will try to implement *Class Activation Maps*, in order to see which parts of an image were the most decisive for its classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07978e8e-9f2e-4949-9699-495af6cb6349",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Explain why Residual Connections are so effective\n",
    "\n",
    "The question *\"Why Residual Networks learn more efficiently than Convolutional Networks?\"* can find an answer by looking at the gradient magnitudes passing through the networks, during backpropagation.\n",
    "\n",
    "`wandb.watch(log = \"all\")` tells *Weights & Biases* to log *gradients* and *parameters*' evolution in all the layers of the network. This functionality is useful to graphically visualize the concept of **Vanishing Gradients**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9edb64",
   "metadata": {},
   "source": [
    "For this exercise, I firstly tried to run a basic *MLP*, and then an *MLP with Residual Connections*. Honestly, at the time, I didn't think that this could be a very clever idea, since I've always seen Residuals been added only on Convolutional Networks, but... I decided to give it a try anyway.\n",
    "\n",
    "As mentioned before, I compared these two architectures by challenging them on their performance over their **depth** (i.t. their number of layers).\n",
    "\n",
    "A basic **10-layer MLP** is seen suffering from Vanishing Gradients, with its accuracy dropping all the way down to 10%, that means picking a class **by chance**.\n",
    "\n",
    "As mentioned in the original [ResNet paper](https://arxiv.org/abs/1512.03385), a higher number of layers leads to not only higher validation loss, but also a *higher training loss*: this means that we are not facing overfitting, but in the \"weird\" behavior that a deeper model shows itself.\n",
    "\n",
    "On the contrary, the **10-layer Residual MLP** performed well, confirming the explanation of ResNet authors: Residual Connections allow a network to go **a lot** deeper (with the only limitation of reaching overfitting).\n",
    "\n",
    "The results can be quantitatively checked by observing the *W&B* logs about gradient magnitudes. The basic **MLP** shows gradients that are very close to zero, meaning that the model is not making any real progress.\n",
    "\n",
    "Conversely, the **Residual MLP** showed gradients that did not vanish nor explode, and progressively diminishing their magnitude during training, meaning that the model is proceeding towards convergence on a (local, hopefully global) optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a4d0fb",
   "metadata": {},
   "source": [
    "The same behaviour can be detected while working on ConvNets and their Residual versions (check gradients on *W&B*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a3a7b-2ed6-4f58-a1b7-5ab1fc432893",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Fully-convolutionalize a network.\n",
    "\n",
    "I decided to save the best model trained so far, the **ResidualCNN** with (..config), and **fully-convolutionalize** it. That is, turn it into a network that can predict classification outputs at *all* pixels in an input image.\n",
    "\n",
    "One goal of this eercise is trying to turn this into a **detector** of handwritten digits.\n",
    "\n",
    "**Hint**: To test my fully-convolutionalized network, I might need to write some functions to take random MNIST samples and embed them into a larger image (i.e. in a regular grid or at random positions), in order to create examples on which train the network at *detecting* digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb332bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dataset and model settings in config.yaml\n",
    "# config.dataset = \"CIFAR10\"; config.fullycnn = True\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83455667",
   "metadata": {},
   "source": [
    "(Mostrare Plots di 3/4 immagini con detection effettuata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7955b",
   "metadata": {},
   "source": [
    "The ConvNets built in the previous exercise have a global Average Pooling layer and a Fully Connected Layer at the end, in order to merge all infro from the convolutions in a single prediction for all the image, on the 10 MNIST/CIFAR10 classes.\n",
    "\n",
    "In a Fully Convolutional Network, we need instead to produce a prediction for every single one of the 28x28 (32x32) pixels of an image. I then proceed to do a \"network surgery\", removing the two layers mentioned above and rearranging the net to have the dimension of the input image as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243f811-8227-4c6f-b07f-56e8cd91643a",
   "metadata": {},
   "source": [
    "### Exercise 2.3: *Explain* the predictions of a CNN\n",
    "\n",
    "In order to predict the correct class of an image, a ConvNet exploits its \"hierarchical\" architecture to create feature maps at different layers of abstraction of information.\n",
    "\n",
    "The composition of every bit of information extracted determines the whole set of details and peculiarities of an image that links it to a specific class.\n",
    "\n",
    "A lot of work has been done in recent years to try to look inside the black box, and find a way to quantitatively *explain* how a prediction was made. One of these ways is to implement [*Class Activation Maps*](http://cnnlocalization.csail.mit.edu/#:~:text=A%20class%20activation%20map%20for,decision%20made%20by%20the%20CNN.):\n",
    "\n",
    "> B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR'16 (arXiv:1512.04150, 2015).\n",
    "\n",
    "Let's demonstrate how my trained CNN *attends* to specific image features to recognize *specific* classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1893c",
   "metadata": {},
   "source": [
    "For this task, I decided to borrow the code from this source (link), in order to try to apply CAMs to some CIFAR10 images.\n",
    "\n",
    "Moreover, as a passionate photographer, since we're talking about images, I *HAD* to try to create CAMs of some of my favourite photographs. Here are some visual results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# model = torch.load(RESIDUALCNN)\n",
    "\n",
    "def cam_test(model, test_loader, epoch):\n",
    "    \n",
    "    classes = ['airplane','bird','car','cat','deer','dog','horse','monkey','ship','truck']\n",
    "\n",
    "    params = [param for param in model.parameters()]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in test_loader:\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            oututs, b_gap, a_gap = model(images)\n",
    "            \n",
    "            _, predicated = torch.max(oututs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicated == labels).sum().item()\n",
    "            \n",
    "            image_labels, image_paths = [], []\n",
    "\n",
    "            for i in range(5):\n",
    "\n",
    "                k = i\n",
    "                vutils.save_image(images[k], f\"Lab1/img/image{i}.jpg\")\n",
    "\n",
    "                # for cam using only the weights of the class predicted\n",
    "                 \n",
    "                #weights = params[-2][predicated[i].item()].detach()\n",
    "                #c = torch.sum(b_gap[k]*weights[:,None, None], dim = 0)\n",
    "\n",
    "                #using global average pooling parameters\n",
    "\n",
    "                c = torch.sum(b_gap[k]*a_gap[k][:,None, None], dim = 0)\n",
    "                \n",
    "                c = (c-torch.min(c))/(torch.max(c)-torch.min(c))\n",
    "            \n",
    "                cam_img = np.uint8(255 * c.cpu().numpy())\n",
    "\n",
    "                hm = cv2.applyColorMap(cv2.resize(cam_img, (96, 96)), cv2.COLORMAP_JET)\n",
    "            \n",
    "                re = hm*0.3+(images[k].permute(1,2,0).cpu().numpy()*255 )*0.4\n",
    "\n",
    "                cv2.imwrite(f\"Lab1/img/CAM{i}.jpg\", re)\n",
    "\n",
    "                image_labels.append(classes[labels[k]]+\"-\"+classes[predicated[k]])\n",
    "                image_paths.append(f\"Lab1/img/CAM{i}.jpg\")\n",
    "\n",
    "            utils.plot_images(image_paths, image_labels, epoch)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c1d0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d0b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# same 10 classes of cifar\n",
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "cifar = True  # xxx else i have an high quality truck image\n",
    "if cifar:\n",
    "    # 1 ship\n",
    "    # 7 frog\n",
    "    # 27 airplane\n",
    "    # 42 dog\n",
    "    image_idx = 18  # indice dell'immagine da testare\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor()])\n",
    "    test_set = CIFAR10(root='./data', train=False,\n",
    "                       download=True, transform=transform)\n",
    "    image, label = test_set[image_idx]\n",
    "    pil_image = TF.to_pil_image(image)\n",
    "    # Display the image\n",
    "    plt.imshow(pil_image)\n",
    "    plt.show()\n",
    "    # Save the image\n",
    "    image_file = 'images/cifar_' + str(classes[label]) + '.jpg'\n",
    "    pil_image.save(image_file)\n",
    "    print(\"real label:\", classes[label])\n",
    "else:\n",
    "    image_file = 'images/hd_truck.jpg'\n",
    "    print(\"using hd image:\", image_file)\n",
    "\n",
    "\n",
    "finalconv_name = \"features\"\n",
    "# net = torch.load(\"./model/resnet_cnn-ep5-lr0.001-bs512-depth5-residual.pt\")\n",
    "net = torch.load(\"./model/resnet_to_convergence/cnn-ep5-lr0.004-bs64-depth25-residual.pt\")\n",
    "print(net)\n",
    "net.eval()\n",
    "\n",
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "\n",
    "\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "\n",
    "net._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "# get the softmax weight\n",
    "params = list(net.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.cpu().numpy())\n",
    "\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h * w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "\n",
    "# normalize = transforms.Normalize(\n",
    "#     mean=[0.485, 0.456, 0.406],\n",
    "#     std=[0.229, 0.224, 0.225]\n",
    "# )\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    # normalize\n",
    "])\n",
    "\n",
    "# load test image\n",
    "img_pil = Image.open(image_file)\n",
    "img_tensor = preprocess(img_pil)\n",
    "img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "logit = net(img_variable.to('cuda'))\n",
    "\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "probs, idx = h_x.sort(0, True)\n",
    "probs = probs.cpu().numpy()\n",
    "idx = idx.cpu().numpy()\n",
    "\n",
    "# output the prediction\n",
    "for i in range(0, 10):\n",
    "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "\n",
    "# generate class activation mapping for the top1 prediction\n",
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "# render the CAM and output\n",
    "print('output CAM.jpg for the top1 prediction: %s' % classes[idx[0]])\n",
    "img = cv2.imread(image_file)\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0], (width, height)), cv2.COLORMAP_JET)\n",
    "# result = heatmap * 0.3 + img * 0.5\n",
    "result = heatmap * 0.4 + img * 0.5\n",
    "if cifar:\n",
    "    cv2.imwrite('images/CAM_cifar_' + str(classes[label]) + '_idx' + str(image_idx) + '_probs' + str(probs[0]) + '.jpg',\n",
    "                result)\n",
    "else:\n",
    "    cv2.imwrite('images/CAM_hd_truck_probs' + str(probs[0]) + '.jpg', result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
